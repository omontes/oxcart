{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch PDF Processing - Philatelic RAG (Optimized v4)\n",
    "\n",
    "Ultra-fast processing of all PDFs in the `pdfs/` folder with minimal overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from demo_page import *\n",
    "from philatelic_patterns import *\n",
    "from dolphin_transformer import transform_dolphin_to_oxcart_preserving_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup - Optimized configuration\n",
    "config_path = \"./config/Dolphin.yaml\"\n",
    "pdfs_dir = Path(\"./pdfs\")\n",
    "save_dir = \"./results\"\n",
    "parsed_jsons_dir = Path(\"./results/parsed_jsons\")\n",
    "max_batch_size = 2  # Increased for better GPU utilization\n",
    "\n",
    "# Pre-clean GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "print(\"üöÄ Initializing Dolphin model...\")\n",
    "config = OmegaConf.load(config_path)\n",
    "model = DOLPHIN(config)\n",
    "\n",
    "# Apply FP16 optimization\n",
    "orig_chat = model.chat\n",
    "def chat_fp16(*args, **kwargs):\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        return orig_chat(*args, **kwargs)\n",
    "model.chat = chat_fp16\n",
    "\n",
    "setup_output_dirs(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model loaded. GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF files - optimized file discovery with recursive search\n",
    "pdf_files = sorted(pdfs_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "# Quick check for existing files\n",
    "existing_processed = set()\n",
    "if parsed_jsons_dir.exists():\n",
    "    existing_processed = {\n",
    "        json_file.stem.replace(\"_philatelic\", \"\") \n",
    "        for json_file in parsed_jsons_dir.glob(\"*_philatelic.json\")\n",
    "    }\n",
    "\n",
    "to_process = [f for f in pdf_files if f.stem not in existing_processed]\n",
    "\n",
    "print(f\"üìÅ {len(pdf_files)} total | {len(existing_processed)} done | {len(to_process)} to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultra-fast batch processing\n",
    "failed_pdfs = []\n",
    "processed_count = 0\n",
    "memory_cleanup_interval = 5  # More frequent cleanup\n",
    "\n",
    "# Minimal progress bar with reduced update frequency\n",
    "progress_bar = tqdm(to_process, desc=\"Processing\", mininterval=2.0, maxinterval=10.0)\n",
    "\n",
    "for pdf_file in progress_bar:\n",
    "    pdf_name = pdf_file.stem\n",
    "    \n",
    "    try:\n",
    "        # Process PDF with Dolphin\n",
    "        json_path, recognition_results = process_document(\n",
    "            document_path=str(pdf_file),\n",
    "            model=model,\n",
    "            save_dir=save_dir,\n",
    "            max_batch_size=max_batch_size\n",
    "        )\n",
    "        \n",
    "        # Transform to OXCART format with philatelic enrichment\n",
    "        ox = transform_dolphin_to_oxcart_preserving_labels(\n",
    "            recognition_results,\n",
    "            doc_id=pdf_name,\n",
    "            page_dims_provider=lambda p: Image.open(f\"./results/pages/page_{p:03d}.png\").size,\n",
    "            para_max_chars=1500,\n",
    "            target_avg_length=300,\n",
    "            max_chunk_length=1200,\n",
    "            table_row_block_size=None,\n",
    "            optimize_for_rag=True\n",
    "        )\n",
    "        \n",
    "        # Enrich with philatelic metadata\n",
    "        ox = enrich_all_chunks_advanced_philatelic(ox)\n",
    "        \n",
    "        # Save philatelic JSON\n",
    "        output_path = parsed_jsons_dir / f\"{pdf_name}_philatelic.json\"\n",
    "        save_json(ox, str(output_path))\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "        # Update progress bar description periodically\n",
    "        if processed_count % 10 == 0:\n",
    "            progress_bar.set_description(f\"Processed {processed_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        failed_pdfs.append({\"pdf\": pdf_name, \"error\": str(e)})\n",
    "    \n",
    "    # Optimized memory cleanup\n",
    "    if torch.cuda.is_available() and (processed_count % memory_cleanup_interval == 0):\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"üéâ Complete! Processed: {processed_count} | Failed: {len(failed_pdfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup and results\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save failed PDFs if any\n",
    "if failed_pdfs:\n",
    "    with open(\"failed_pdfs.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(failed_pdfs, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"‚ùå {len(failed_pdfs)} failed - saved to failed_pdfs.json\")\n",
    "\n",
    "# Final summary\n",
    "total_processed = len(list(parsed_jsons_dir.glob(\"*_philatelic.json\"))) if parsed_jsons_dir.exists() else 0\n",
    "completion_rate = total_processed/len(pdf_files)*100 if pdf_files else 0\n",
    "\n",
    "print(f\"üìä {total_processed}/{len(pdf_files)} PDFs ({completion_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x31v0tmhqm",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Leer el notebook\n",
    "with open(r\"C:\\Users\\VM-SERVER\\Desktop\\Oxcart RAG\\combine_chunks.ipynb\", 'r', encoding='utf-8') as f:\n",
    "    notebook = json.load(f)\n",
    "\n",
    "# Buscar y corregir las llamadas incorrectas a should_combine_chunks\n",
    "corrections_made = 0\n",
    "\n",
    "for cell in notebook['cells']:\n",
    "    if cell['cell_type'] == 'code':\n",
    "        for i, line in enumerate(cell['source']):\n",
    "            # Buscar las llamadas con 3 argumentos y corregir a 2\n",
    "            if 'should_combine_chunks(current_chunk, test_processed_chunks, test_used_headers)' in line:\n",
    "                cell['source'][i] = line.replace(\n",
    "                    'should_combine_chunks(current_chunk, test_processed_chunks, test_used_headers)',\n",
    "                    'should_combine_chunks(current_chunk, test_processed_chunks)'\n",
    "                )\n",
    "                corrections_made += 1\n",
    "                print(f\"Corregido en l√≠nea: {line.strip()}\")\n",
    "\n",
    "# Guardar el notebook corregido\n",
    "with open(r\"C:\\Users\\VM-SERVER\\Desktop\\Oxcart RAG\\combine_chunks.ipynb\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(notebook, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "print(f\"\\nTotal de correcciones realizadas: {corrections_made}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
