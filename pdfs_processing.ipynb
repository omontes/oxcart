{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch PDF Processing - Philatelic RAG\n",
    "\n",
    "Efficient processing of all PDFs in the `pdfs/` folder to generate philatelic-enriched JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from demo_page import *\n",
    "from philatelic_patterns import *\n",
    "from dolphin_transformer import transform_dolphin_to_oxcart_preserving_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "config_path = \"./config/Dolphin.yaml\"\n",
    "pdfs_dir = Path(\"./pdfs\")\n",
    "save_dir = \"./results\"\n",
    "parsed_jsons_dir = Path(\"./results/parsed_jsons\")\n",
    "max_batch_size = 8  # Increased from 4 for better throughput\n",
    "\n",
    "# Initialize model once\n",
    "print(\"ðŸš€ Initializing Dolphin model...\")\n",
    "config = OmegaConf.load(config_path)\n",
    "model = DOLPHIN(config)\n",
    "\n",
    "# Apply FP16 optimization like in dolphin_parser\n",
    "orig_chat = model.chat\n",
    "def chat_fp16(*args, **kwargs):\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "        return orig_chat(*args, **kwargs)\n",
    "model.chat = chat_fp16  # Apply FP16 optimization\n",
    "\n",
    "setup_output_dirs(save_dir)\n",
    "\n",
    "print(f\"âœ… Model loaded with FP16 optimization. GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PDF files and check existing processed files (optimized)\n",
    "pdf_files = sorted([f for f in pdfs_dir.glob(\"*.pdf\")])\n",
    "existing_processed = set()\n",
    "\n",
    "# Optimize file checking - only check if directory exists\n",
    "if parsed_jsons_dir.exists():\n",
    "    existing_processed = {\n",
    "        json_file.stem.replace(\"_philatelic\", \"\") \n",
    "        for json_file in parsed_jsons_dir.glob(\"*_philatelic.json\")\n",
    "    }\n",
    "\n",
    "to_process = [f for f in pdf_files if f.stem not in existing_processed]\n",
    "\n",
    "print(f\"ðŸ“ Total PDFs: {len(pdf_files)} | Already processed: {len(existing_processed)} | To process: {len(to_process)}\")\n",
    "\n",
    "# Only show detailed info if there are many existing files\n",
    "if len(existing_processed) > 10:\n",
    "    print(f\"Sample already processed: {sorted(list(existing_processed))[:5]}...\")\n",
    "elif existing_processed:\n",
    "    print(f\"Already processed: {sorted(existing_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing with error handling\n",
    "failed_pdfs = []\n",
    "processed_count = 0\n",
    "memory_cleanup_interval = 10  # Clean memory every N PDFs instead of every PDF\n",
    "\n",
    "for pdf_file in tqdm(to_process, desc=\"Processing PDFs\"):\n",
    "    pdf_name = pdf_file.stem\n",
    "    \n",
    "    try:\n",
    "        if processed_count % 5 == 0:  # Reduce logging frequency\n",
    "            print(f\"\\nðŸ”„ Processing: {pdf_name} ({processed_count+1}/{len(to_process)})\")\n",
    "        \n",
    "        # Process PDF with Dolphin\n",
    "        json_path, recognition_results = process_document(\n",
    "            document_path=str(pdf_file),\n",
    "            model=model,\n",
    "            save_dir=save_dir,\n",
    "            max_batch_size=max_batch_size\n",
    "        )\n",
    "        \n",
    "        # Transform to OXCART format with philatelic enrichment\n",
    "        ox = transform_dolphin_to_oxcart_preserving_labels(\n",
    "            recognition_results,\n",
    "            doc_id=pdf_name,\n",
    "            page_dims_provider=lambda p: Image.open(f\"./results/pages/page_{p:03d}.png\").size,\n",
    "            para_max_chars=1500,\n",
    "            target_avg_length=300,\n",
    "            max_chunk_length=1200,\n",
    "            table_row_block_size=None,\n",
    "            optimize_for_rag=True\n",
    "        )\n",
    "        \n",
    "        # Enrich with philatelic metadata\n",
    "        ox = enrich_all_chunks_advanced_philatelic(ox)\n",
    "        \n",
    "        # Save philatelic JSON\n",
    "        output_path = parsed_jsons_dir / f\"{pdf_name}_philatelic.json\"\n",
    "        save_json(ox, str(output_path))\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error processing {pdf_name}: {str(e)}\"\n",
    "        print(f\"âŒ {error_msg}\")\n",
    "        failed_pdfs.append({\"pdf\": pdf_name, \"error\": str(e)})\n",
    "    \n",
    "    finally:\n",
    "        # Clean GPU memory periodically instead of every PDF\n",
    "        if torch.cuda.is_available() and (processed_count % memory_cleanup_interval == 0):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Processing complete!\")\n",
    "print(f\"âœ… Successfully processed: {processed_count}\")\n",
    "print(f\"âŒ Failed: {len(failed_pdfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup and summary\n",
    "# Clean GPU memory one final time\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Save failed PDFs list for retry\n",
    "if failed_pdfs:\n",
    "    failed_file = Path(\"failed_pdfs.json\")\n",
    "    with open(failed_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(failed_pdfs, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Failed PDFs saved to: {failed_file}\")\n",
    "    for failed in failed_pdfs[:3]:  # Show only first 3 to reduce output\n",
    "        print(f\"  - {failed['pdf']}: {failed['error']}\")\n",
    "    if len(failed_pdfs) > 3:\n",
    "        print(f\"  ... and {len(failed_pdfs) - 3} more\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ¯ All PDFs processed successfully!\")\n",
    "\n",
    "# Optimized final summary\n",
    "if parsed_jsons_dir.exists():\n",
    "    total_processed = len(list(parsed_jsons_dir.glob(\"*_philatelic.json\")))\n",
    "else:\n",
    "    total_processed = 0\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Status:\")\n",
    "print(f\"Total philatelic JSONs: {total_processed}\")\n",
    "print(f\"Total PDFs: {len(pdf_files)}\")\n",
    "print(f\"Completion rate: {total_processed/len(pdf_files)*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
