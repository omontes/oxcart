{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# OXCART Philatelic RAG System\n",
    "\n",
    "Sistema completo de indexaciÃ³n y bÃºsqueda semÃ¡ntica para documentos filatÃ©licos.\n",
    "\n",
    "**Funcionalidades:**\n",
    "- ðŸ“„ IndexaciÃ³n automÃ¡tica de todos los JSONs philatelic\n",
    "- ðŸ” BÃºsqueda semÃ¡ntica avanzada con filtros filatÃ©licos\n",
    "- ðŸ¤– RAG bÃ¡sico con LLM para responder preguntas\n",
    "- ðŸ“Š Dashboard de estadÃ­sticas y validaciÃ³n\n",
    "- ðŸŒ Interfaz Gradio para consultas interactivas\n",
    "\n",
    "**Requisitos:**\n",
    "- Weaviate corriendo en Docker: `docker-compose up -d`\n",
    "- OpenAI API key configurada en `.env`\n",
    "- JSONs philatelic en `results/final_jsons/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_section",
   "metadata": {},
   "source": [
    "## 1. Setup y ConfiguraciÃ³n\n",
    "\n",
    "ConfiguraciÃ³n inicial del entorno y carga de librerÃ­as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30693326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# Cargar variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Imports de terceros\n",
    "import pandas as pd\n",
    "\n",
    "print(\"âœ… Imports bÃ¡sicos completados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar variables de entorno\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WEAVIATE_URL = os.getenv('WEAVIATE_URL', 'http://localhost:8083')\n",
    "PHILATELIC_JSONS_DIR = os.getenv('PHILATELIC_JSONS_DIR', './results/final_jsons')\n",
    "COLLECTION_NAME = os.getenv('WEAVIATE_COLLECTION_NAME', 'Oxcart')\n",
    "\n",
    "print(f\"ðŸ”§ ConfiguraciÃ³n:\")\n",
    "print(f\"   â€¢ Weaviate URL: {WEAVIATE_URL}\")\n",
    "print(f\"   â€¢ JSONs Directory: {PHILATELIC_JSONS_DIR}\")\n",
    "print(f\"   â€¢ Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"   â€¢ OpenAI API Key: {'âœ… Configurada' if OPENAI_API_KEY else 'âŒ Falta configurar'}\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"\\\\nâš ï¸  IMPORTANTE: Configura tu OPENAI_API_KEY en el archivo .env\")\n",
    "    print(\"   Copia .env.example a .env y agrega tu API key\")\n",
    "\n",
    "# Verificar que el directorio de JSONs existe\n",
    "if not os.path.exists(PHILATELIC_JSONS_DIR):\n",
    "    print(f\"\\\\nâš ï¸  Directorio {PHILATELIC_JSONS_DIR} no encontrado\")\n",
    "    print(\"   AsegÃºrate de haber procesado documentos con el Dolphin parser\")\n",
    "else:\n",
    "    json_files = glob.glob(os.path.join(PHILATELIC_JSONS_DIR, '*_final.json'))\n",
    "    print(f\"\\\\nðŸ“ Encontrados {len(json_files)} archivos JSON filatÃ©licos\")\n",
    "    if json_files:\n",
    "        print(\"   Ejemplos:\")\n",
    "        for file in json_files[:3]:\n",
    "            print(f\"   â€¢ {os.path.basename(file)}\")\n",
    "        if len(json_files) > 3:\n",
    "            print(f\"   â€¢ ... y {len(json_files) - 3} mÃ¡s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import_modules",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recargar mÃ³dulos del sistema OXCART para obtener las Ãºltimas mejoras\n",
    "import importlib\n",
    "\n",
    "# Recargar el mÃ³dulo philatelic_weaviate para obtener las funciones actualizadas\n",
    "try:\n",
    "    import philatelic_weaviate\n",
    "    importlib.reload(philatelic_weaviate)\n",
    "    print(\"ðŸ”„ MÃ³dulo philatelic_weaviate recargado\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from philatelic_weaviate import (\n",
    "    create_weaviate_client,\n",
    "    create_oxcart_collection,\n",
    "    index_philatelic_document,\n",
    "    search_chunks_semantic,\n",
    "    get_collection_stats,\n",
    "    transform_chunk_to_weaviate\n",
    ")\n",
    "\n",
    "from philatelic_chunk_schema import (\n",
    "    PhilatelicDocument,\n",
    "    PhilatelicChunk,\n",
    "    validate_chunk_structure,\n",
    "    get_chunk_summary\n",
    ")\n",
    "\n",
    "print(\"âœ… MÃ³dulos OXCART cargados exitosamente con las mejoras mÃ¡s recientes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discovery_section",
   "metadata": {},
   "source": [
    "## 2. Descubrimiento de Documentos\n",
    "\n",
    "Escanear automÃ¡ticamente todos los archivos JSON philatelic disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1ap3dktty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_philatelic_jsons(directory: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Descubrir todos los archivos JSON philatelic en el directorio.\n",
    "    Cuenta chunks ya indexados vs pendientes vs truncados vs no truncados.\n",
    "    \n",
    "    Returns:\n",
    "        Lista de diccionarios con informaciÃ³n de cada archivo\n",
    "    \"\"\"\n",
    "    # Importar tqdm para progress bar\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    json_files = []\n",
    "    \n",
    "    # Buscar archivos *_final.json\n",
    "    pattern = os.path.join(directory, \"*_final.json\")\n",
    "    philatelic_files = glob.glob(pattern)\n",
    "    \n",
    "    print(f\"ðŸ” Buscando archivos en: {directory}\")\n",
    "    print(f\"ðŸ“‹ PatrÃ³n de bÃºsqueda: *_final.json\")\n",
    "    print(f\"ðŸ“„ Archivos encontrados: {len(philatelic_files)}\")\n",
    "    \n",
    "    # Progress bar para descubrimiento\n",
    "    for file_path in tqdm(philatelic_files, desc=\"ðŸ“„ Analizando documentos\", unit=\"doc\"):\n",
    "        try:\n",
    "            # Obtener informaciÃ³n del archivo\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            file_name = os.path.basename(file_path)\n",
    "            doc_id = file_name.replace(\"_final.json\", \"\")\n",
    "            \n",
    "            # Cargar archivo para obtener estadÃ­sticas bÃ¡sicas\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            chunks = data.get(\"chunks\", [])\n",
    "            page_count = data.get(\"page_count\", len(chunks))  # Estimado si no estÃ¡ disponible\n",
    "            \n",
    "            # Calcular estadÃ­sticas bÃ¡sicas y contar chunks por estado\n",
    "            total_text_length = 0\n",
    "            chunk_types = {}\n",
    "            chunks_indexed = 0\n",
    "            chunks_pending = 0\n",
    "            chunks_truncated = 0\n",
    "            chunks_not_truncated = 0\n",
    "            chunks_truncated_unknown = 0\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                chunk_text = chunk.get(\"text\", \"\") or chunk.get(\"content\", \"\")\n",
    "                total_text_length += len(chunk_text)\n",
    "                \n",
    "                chunk_type = chunk.get(\"chunk_type\", \"text\")\n",
    "                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1\n",
    "                \n",
    "                # Verificar estado de indexaciÃ³n\n",
    "                if chunk.get(\"indexed\", False):\n",
    "                    chunks_indexed += 1\n",
    "                else:\n",
    "                    chunks_pending += 1\n",
    "                \n",
    "                # Verificar estado de truncado (trazabilidad completa)\n",
    "                truncated_flag = chunk.get(\"truncated\", None)\n",
    "                if truncated_flag is True:\n",
    "                    chunks_truncated += 1\n",
    "                elif truncated_flag is False:\n",
    "                    chunks_not_truncated += 1\n",
    "                else:\n",
    "                    # Chunk sin marcar (sin procesar aÃºn)\n",
    "                    chunks_truncated_unknown += 1\n",
    "            \n",
    "            avg_chunk_length = total_text_length / len(chunks) if chunks else 0\n",
    "            \n",
    "            json_files.append({\n",
    "                \"file_path\": file_path,\n",
    "                \"file_name\": file_name,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"file_size_mb\": round(file_size, 2),\n",
    "                \"chunks_count\": len(chunks),\n",
    "                \"chunks_indexed\": chunks_indexed,\n",
    "                \"chunks_pending\": chunks_pending,\n",
    "                \"chunks_truncated\": chunks_truncated,\n",
    "                \"chunks_not_truncated\": chunks_not_truncated,\n",
    "                \"chunks_truncated_unknown\": chunks_truncated_unknown,\n",
    "                \"page_count\": page_count,\n",
    "                \"total_text_length\": total_text_length,\n",
    "                \"avg_chunk_length\": round(avg_chunk_length, 1),\n",
    "                \"chunk_types\": chunk_types,\n",
    "                \"data\": data  # Guardar datos para indexaciÃ³n\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error procesando {file_path}: {e}\")\n",
    "    \n",
    "    # Mostrar resumen de indexaciÃ³n y trazabilidad\n",
    "    if json_files:\n",
    "        total_chunks = sum(f[\"chunks_count\"] for f in json_files)\n",
    "        total_indexed = sum(f[\"chunks_indexed\"] for f in json_files)\n",
    "        total_pending = sum(f[\"chunks_pending\"] for f in json_files)\n",
    "        total_truncated = sum(f[\"chunks_truncated\"] for f in json_files)\n",
    "        total_not_truncated = sum(f[\"chunks_not_truncated\"] for f in json_files)\n",
    "        total_truncated_unknown = sum(f[\"chunks_truncated_unknown\"] for f in json_files)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š ESTADO DE INDEXACIÃ“N:\")\n",
    "        print(f\"   ðŸ“¦ Total chunks: {total_chunks:,}\")\n",
    "        print(f\"   âœ… Ya indexados: {total_indexed:,} ({(total_indexed/total_chunks)*100:.1f}%)\")\n",
    "        print(f\"   â³ Pendientes: {total_pending:,} ({(total_pending/total_chunks)*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nðŸ” TRAZABILIDAD DE TRUNCADO:\")\n",
    "        print(f\"   âœ‚ï¸ Truncados: {total_truncated:,} ({(total_truncated/total_chunks)*100:.1f}%)\")\n",
    "        print(f\"   ðŸ“ No truncados: {total_not_truncated:,} ({(total_not_truncated/total_chunks)*100:.1f}%)\")\n",
    "        print(f\"   â“ Sin procesar: {total_truncated_unknown:,} ({(total_truncated_unknown/total_chunks)*100:.1f}%)\")\n",
    "        \n",
    "        if total_pending == 0:\n",
    "            print(f\"   ðŸŽ‰ Â¡Todos los chunks estÃ¡n indexados!\")\n",
    "        elif total_indexed > 0:\n",
    "            print(f\"   ðŸ”„ Se continuarÃ¡ desde donde se quedÃ³\")\n",
    "            \n",
    "        # InformaciÃ³n sobre trazabilidad\n",
    "        if total_truncated > 0 or total_not_truncated > 0:\n",
    "            total_processed = total_truncated + total_not_truncated\n",
    "            print(f\"   ðŸ’¡ {total_processed:,} chunks con trazabilidad completa de truncado\")\n",
    "            if total_truncated > 0:\n",
    "                print(f\"   ðŸ“„ Los chunks truncados mantienen texto original en 'text_original'\")\n",
    "    \n",
    "    return json_files\n",
    "\n",
    "print(\"âœ… FunciÃ³n de descubrimiento mejorada con trazabilidad completa de truncado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discover_documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descubrir archivos\n",
    "discovered_files = discover_philatelic_jsons(PHILATELIC_JSONS_DIR)\n",
    "\n",
    "print(f\"\\\\nðŸ“Š RESUMEN DE DESCUBRIMIENTO:\")\n",
    "print(f\"   ðŸ“„ Archivos encontrados: {len(discovered_files)}\")\n",
    "\n",
    "if discovered_files:\n",
    "    # === ESTADÃSTICAS BÃSICAS ===\n",
    "    total_chunks = sum(f[\"chunks_count\"] for f in discovered_files)\n",
    "    total_indexed = sum(f[\"chunks_indexed\"] for f in discovered_files)\n",
    "    total_pending = sum(f[\"chunks_pending\"] for f in discovered_files)\n",
    "    total_pages = sum(f[\"page_count\"] for f in discovered_files)\n",
    "    total_size = sum(f[\"file_size_mb\"] for f in discovered_files)\n",
    "    total_text_length = sum(f[\"total_text_length\"] for f in discovered_files)\n",
    "    \n",
    "    print(f\"   ðŸ“¦ Total chunks: {total_chunks:,}\")\n",
    "    print(f\"   âœ… Ya indexados: {total_indexed:,} ({(total_indexed/total_chunks)*100:.1f}%)\")\n",
    "    print(f\"   â³ Pendientes: {total_pending:,} ({(total_pending/total_chunks)*100:.1f}%)\")\n",
    "    print(f\"   ðŸ“„ Total pÃ¡ginas: {total_pages:,}\")\n",
    "    print(f\"   ðŸ’¾ TamaÃ±o total: {total_size:.1f} MB\")\n",
    "    \n",
    "    # === ESTADÃSTICAS AVANZADAS DE CHUNKS ===\n",
    "    if total_chunks > 0:\n",
    "        # Promedio global de tamaÃ±o de chunks\n",
    "        avg_chunk_size_global = total_text_length / total_chunks\n",
    "        avg_chunks_per_doc = total_chunks / len(discovered_files)\n",
    "        \n",
    "        # DistribuciÃ³n de tipos de chunks\n",
    "        all_chunk_types = {}\n",
    "        chunk_sizes = []\n",
    "        \n",
    "        for f in discovered_files:\n",
    "            chunk_sizes.extend([f[\"avg_chunk_length\"]] * f[\"chunks_count\"])\n",
    "            for chunk_type, count in f[\"chunk_types\"].items():\n",
    "                all_chunk_types[chunk_type] = all_chunk_types.get(chunk_type, 0) + count\n",
    "        \n",
    "        # EstadÃ­sticas de tamaÃ±o\n",
    "        min_chunk_size = min(chunk_sizes) if chunk_sizes else 0\n",
    "        max_chunk_size = max(chunk_sizes) if chunk_sizes else 0\n",
    "        \n",
    "        print(f\"\\\\nðŸ“Š ESTADÃSTICAS DE CHUNKS:\")\n",
    "        print(f\"   ðŸ“ TamaÃ±o promedio global: {avg_chunk_size_global:.0f} caracteres\")\n",
    "        print(f\"   ðŸ“ˆ Promedio chunks/documento: {avg_chunks_per_doc:.1f}\")\n",
    "        print(f\"   ðŸ“‰ Rango de tamaÃ±os: {min_chunk_size:.0f} - {max_chunk_size:.0f} chars\")\n",
    "        \n",
    "        # Top tipos de chunks\n",
    "        print(f\"   ðŸ·ï¸ Tipos mÃ¡s comunes:\")\n",
    "        sorted_types = sorted(all_chunk_types.items(), key=lambda x: x[1], reverse=True)\n",
    "        for chunk_type, count in sorted_types[:5]:\n",
    "            percentage = (count / total_chunks) * 100\n",
    "            print(f\"      â€¢ {chunk_type}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # === ESTIMACIÃ“N DE COSTOS OPENAI ===\n",
    "    if total_pending > 0:\n",
    "        print(f\"\\\\nðŸ’° ESTIMACIÃ“N DE COSTOS OPENAI (SOLO CHUNKS PENDIENTES):\")\n",
    "        \n",
    "        # ConfiguraciÃ³n del modelo de embeddings\n",
    "        # text-embedding-3-large: $0.00013 per 1K tokens (mÃ¡s reciente y eficiente)\n",
    "        EMBEDDING_MODEL = \"text-embedding-3-large\"\n",
    "        COST_PER_1K_TOKENS = 0.00013  # USD\n",
    "        CHARS_PER_TOKEN = 4  # AproximaciÃ³n para texto en espaÃ±ol\n",
    "        \n",
    "        # Calcular tokens estimados solo para chunks pendientes\n",
    "        pending_text_length = 0\n",
    "        for f in discovered_files:\n",
    "            if f[\"chunks_pending\"] > 0:\n",
    "                # Estimar texto de chunks pendientes (proporcionalmente)\n",
    "                pending_ratio = f[\"chunks_pending\"] / f[\"chunks_count\"]\n",
    "                pending_text_length += f[\"total_text_length\"] * pending_ratio\n",
    "        \n",
    "        estimated_tokens = pending_text_length / CHARS_PER_TOKEN\n",
    "        estimated_cost = (estimated_tokens / 1000) * COST_PER_1K_TOKENS\n",
    "        \n",
    "        print(f\"   ðŸ¤– Modelo: {EMBEDDING_MODEL}\")\n",
    "        print(f\"   ðŸ“ Caracteres pendientes: {pending_text_length:,.0f}\")\n",
    "        print(f\"   ðŸŽ¯ Tokens estimados: {estimated_tokens:,.0f}\")\n",
    "        print(f\"   ðŸ’µ Costo estimado: ${estimated_cost:.4f} USD\")\n",
    "        \n",
    "        # Estimaciones adicionales Ãºtiles\n",
    "        if estimated_cost > 0:\n",
    "            cost_per_chunk = estimated_cost / total_pending\n",
    "            docs_with_pending = sum(1 for f in discovered_files if f[\"chunks_pending\"] > 0)\n",
    "            cost_per_document = estimated_cost / docs_with_pending if docs_with_pending > 0 else 0\n",
    "            \n",
    "            print(f\"   ðŸ“Š Costo por chunk pendiente: ${cost_per_chunk:.6f} USD\")\n",
    "            print(f\"   ðŸ“„ Costo por documento con pendientes: ${cost_per_document:.4f} USD\")\n",
    "            \n",
    "            # Rangos de referencia\n",
    "            if estimated_cost < 0.01:\n",
    "                cost_range = \"ðŸ’š Muy bajo\"\n",
    "            elif estimated_cost < 0.10:\n",
    "                cost_range = \"ðŸ’™ Bajo\"\n",
    "            elif estimated_cost < 1.00:\n",
    "                cost_range = \"ðŸ’› Moderado\"\n",
    "            elif estimated_cost < 5.00:\n",
    "                cost_range = \"ðŸ§¡ Alto\"\n",
    "            else:\n",
    "                cost_range = \"â¤ï¸ Muy alto\"\n",
    "            \n",
    "            print(f\"   ðŸ“ˆ Rango de costo: {cost_range}\")\n",
    "    else:\n",
    "        print(f\"\\\\nðŸŽ‰ Â¡No hay chunks pendientes para indexar!\")\n",
    "        print(f\"   ðŸ’° Costo estimado: $0.00 USD\")\n",
    "    \n",
    "    # === ADVERTENCIAS Y NOTAS ===\n",
    "    print(f\"\\\\nâš ï¸ NOTAS IMPORTANTES:\")\n",
    "    print(f\"   â€¢ Solo se procesarÃ¡n chunks pendientes (sin flag 'indexed': true)\")\n",
    "    print(f\"   â€¢ Los chunks exitosos se marcarÃ¡n automÃ¡ticamente como indexados\")\n",
    "    print(f\"   â€¢ Los archivos JSON se actualizarÃ¡n automÃ¡ticamente\")\n",
    "    print(f\"   â€¢ Las futuras ejecuciones continuarÃ¡n donde se quedÃ³\")\n",
    "    print(f\"   â€¢ Los costos son estimaciones basadas en {CHARS_PER_TOKEN} chars/token\")\n",
    "    \n",
    "else:\n",
    "    print(f\"   âš ï¸ No se encontraron archivos *_final.json en {PHILATELIC_JSONS_DIR}\")\n",
    "    print(f\"   ðŸ’¡ AsegÃºrate de haber procesado documentos con el Dolphin parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_files_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar tabla resumen de archivos\n",
    "if discovered_files:\n",
    "    files_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Documento\": f[\"doc_id\"],\n",
    "            \"Chunks\": f[\"chunks_count\"],\n",
    "            \"PÃ¡ginas\": f[\"page_count\"],\n",
    "            \"TamaÃ±o (MB)\": f[\"file_size_mb\"],\n",
    "            \"Promedio chunk\": f[\"avg_chunk_length\"],\n",
    "            \"Tipos principales\": \", \".join([f\"{k}: {v}\" for k, v in list(f[\"chunk_types\"].items())[:3]])\n",
    "        }\n",
    "        for f in discovered_files\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nðŸ“‹ DOCUMENTOS ENCONTRADOS:\")\n",
    "    print(files_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nâŒ No hay documentos para mostrar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weaviate_section",
   "metadata": {},
   "source": [
    "## 3. ConfiguraciÃ³n de Weaviate\n",
    "\n",
    "Conectar a Weaviate y crear la colecciÃ³n con esquema optimizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f854f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "weaviate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connect_weaviate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar a Weaviate\n",
    "print(\"ðŸ”Œ Conectando a Weaviate...\")\n",
    "\n",
    "try:\n",
    "    client = create_weaviate_client(WEAVIATE_URL, OPENAI_API_KEY)\n",
    "    print(\"âœ… ConexiÃ³n exitosa\")\n",
    "    \n",
    "    # Verificar que Weaviate estÃ© funcionando\n",
    "    meta = client.get_meta()\n",
    "    print(f\"ðŸ“Š Weaviate versiÃ³n: {meta.get('version', 'unknown')}\")\n",
    "    \n",
    "    # Verificar si la colecciÃ³n existe\n",
    "    try:\n",
    "        collections = client.collections.list_all()\n",
    "        collection_names = [col.name for col in collections]\n",
    "        \n",
    "        if COLLECTION_NAME in collection_names:\n",
    "            collection = client.collections.get(COLLECTION_NAME)\n",
    "            total_objects = collection.aggregate.over_all(total_count=True).total_count\n",
    "            print(f\"ðŸ“Š ColecciÃ³n '{COLLECTION_NAME}' existe con {total_objects} documentos\")\n",
    "        else:\n",
    "            print(f\"ðŸ“ ColecciÃ³n '{COLLECTION_NAME}' no existe (se crearÃ¡ durante la indexaciÃ³n)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ No se pudo verificar colecciones: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error conectando a Weaviate: {e}\")\n",
    "    print(\"ðŸ’¡ AsegÃºrate de que Weaviate estÃ© corriendo:\")\n",
    "    print(\"   docker-compose up -d\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.collections.delete(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear colecciÃ³n Oxcart\n",
    "if client:\n",
    "    print(\"\\nðŸ—ï¸ Configurando colecciÃ³n Oxcart...\")\n",
    "    \n",
    "    collection_created = create_oxcart_collection(client, COLLECTION_NAME)\n",
    "    \n",
    "    if collection_created:\n",
    "        print(\"âœ… ColecciÃ³n lista para indexaciÃ³n\")\n",
    "        \n",
    "        # Mostrar estadÃ­sticas de la colecciÃ³n\n",
    "        stats = get_collection_stats(client, COLLECTION_NAME)\n",
    "        if stats:\n",
    "            print(f\"ðŸ“Š Chunks actuales en Weaviate: {stats.get('total_chunks', 0)}\")\n",
    "            if stats.get('documents'):\n",
    "                print(f\"ðŸ“„ Documentos indexados: {list(stats['documents'].keys())}\")\n",
    "    else:\n",
    "        print(\"âŒ Error configurando colecciÃ³n\")\n",
    "        client = None\n",
    "else:\n",
    "    print(\"âš ï¸ Saltando configuraciÃ³n de colecciÃ³n (sin conexiÃ³n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indexing_section",
   "metadata": {},
   "source": [
    "## 4. IndexaciÃ³n AutomÃ¡tica\n",
    "\n",
    "Indexar automÃ¡ticamente todos los documentos philatelic en Weaviate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indexing_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_all_documents(client, discovered_files: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Indexar todos los documentos descubiertos en Weaviate con progress bars, persistencia\n",
    "    y manejo inteligente de chunks largos.\n",
    "    \n",
    "    Returns:\n",
    "        Dict con resultados de indexaciÃ³n\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    \n",
    "    if not client:\n",
    "        return {\"error\": \"No hay conexiÃ³n a Weaviate\"}\n",
    "    \n",
    "    if not discovered_files:\n",
    "        return {\"error\": \"No hay documentos para indexar\"}\n",
    "    \n",
    "    # Filtrar documentos con chunks pendientes\n",
    "    documents_to_process = []\n",
    "    total_pending_chunks = 0\n",
    "    \n",
    "    for file_info in discovered_files:\n",
    "        chunks_pending = file_info.get(\"chunks_pending\", 0)\n",
    "        if chunks_pending > 0:\n",
    "            documents_to_process.append(file_info)\n",
    "            total_pending_chunks += chunks_pending\n",
    "    \n",
    "    if not documents_to_process:\n",
    "        print(\"ðŸŽ‰ Â¡Todos los documentos ya estÃ¡n completamente indexados!\")\n",
    "        return {\n",
    "            \"total_documents\": len(discovered_files),\n",
    "            \"successful_documents\": len(discovered_files),\n",
    "            \"failed_documents\": 0,\n",
    "            \"total_chunks_indexed\": 0,\n",
    "            \"total_chunks_pending\": 0,\n",
    "            \"all_indexed\": True\n",
    "        }\n",
    "    \n",
    "    print(f\"ðŸš€ INICIANDO INDEXACIÃ“N MASIVA ROBUSTA CON MANEJO DE CHUNKS LARGOS\")\n",
    "    print(f\"ðŸ“„ Documentos con chunks pendientes: {len(documents_to_process)}\")\n",
    "    print(f\"ðŸ“¦ Total chunks pendientes: {total_pending_chunks:,}\")\n",
    "    print(f\"âœ‚ï¸ Chunks largos serÃ¡n truncados automÃ¡ticamente a 12,000 caracteres\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    indexing_results = []\n",
    "    total_chunks_indexed = 0\n",
    "    total_chunks_failed = 0\n",
    "    total_chunks_truncated = 0\n",
    "    total_chars_saved = 0\n",
    "    documents_updated = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Progress bar principal para documentos\n",
    "    doc_pbar = tqdm(\n",
    "        documents_to_process, \n",
    "        desc=\"ðŸ“„ Procesando documentos\", \n",
    "        unit=\"doc\",\n",
    "        position=0\n",
    "    )\n",
    "    \n",
    "    # Progress bar secundaria para chunks del documento actual\n",
    "    chunk_pbar = None\n",
    "    \n",
    "    for i, file_info in enumerate(doc_pbar):\n",
    "        doc_id = file_info[\"doc_id\"]\n",
    "        document = file_info[\"data\"]\n",
    "        chunks_count = file_info[\"chunks_count\"]\n",
    "        chunks_pending = file_info[\"chunks_pending\"]\n",
    "        file_path = file_info[\"file_path\"]\n",
    "        \n",
    "        doc_pbar.set_description(f\"ðŸ“„ Procesando {doc_id}\")\n",
    "        \n",
    "        # Progress bar para chunks de este documento\n",
    "        if chunks_pending > 0:\n",
    "            chunk_pbar = tqdm(\n",
    "                total=chunks_pending,\n",
    "                desc=f\"  ðŸ“¦ Indexando chunks\",\n",
    "                unit=\"chunk\",\n",
    "                position=1,\n",
    "                leave=False\n",
    "            )\n",
    "            \n",
    "            # Callback para actualizar progress bar de chunks\n",
    "            def update_chunk_progress(successful_count):\n",
    "                if chunk_pbar:\n",
    "                    chunk_pbar.update(successful_count)\n",
    "        else:\n",
    "            update_chunk_progress = None\n",
    "        \n",
    "        try:\n",
    "            # Indexar documento usando la funciÃ³n mejorada\n",
    "            result = index_philatelic_document(\n",
    "                client, \n",
    "                document, \n",
    "                COLLECTION_NAME,\n",
    "                progress_callback=update_chunk_progress\n",
    "            )\n",
    "            \n",
    "            # Guardar resultado\n",
    "            chunks_indexed = result.get(\"successful\", 0)\n",
    "            chunks_failed = len(result.get(\"errors\", []))\n",
    "            chunks_marked = result.get(\"chunks_marked_as_indexed\", 0)\n",
    "            \n",
    "            # EstadÃ­sticas de validaciÃ³n (chunks truncados)\n",
    "            validation_stats = result.get(\"validation_stats\", {})\n",
    "            chunks_truncated_doc = validation_stats.get(\"truncated_chunks\", 0)\n",
    "            chars_saved_doc = validation_stats.get(\"total_chars_saved\", 0)\n",
    "            \n",
    "            indexing_results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"success\": chunks_indexed > 0 or result.get(\"already_indexed\", False),\n",
    "                \"chunks_indexed\": chunks_indexed,\n",
    "                \"chunks_failed\": chunks_failed,\n",
    "                \"chunks_marked\": chunks_marked,\n",
    "                \"chunks_truncated\": chunks_truncated_doc,\n",
    "                \"chars_saved\": chars_saved_doc,\n",
    "                \"already_indexed\": result.get(\"already_indexed\", False),\n",
    "                \"errors\": result.get(\"errors\", []),\n",
    "                \"validation_stats\": validation_stats\n",
    "            })\n",
    "            \n",
    "            total_chunks_indexed += chunks_indexed\n",
    "            total_chunks_failed += chunks_failed\n",
    "            total_chunks_truncated += chunks_truncated_doc\n",
    "            total_chars_saved += chars_saved_doc\n",
    "            \n",
    "            # Actualizar descripciÃ³n del progress bar\n",
    "            status_parts = []\n",
    "            if chunks_marked > 0:\n",
    "                status_parts.append(f\"{chunks_marked} marcados\")\n",
    "            if chunks_truncated_doc > 0:\n",
    "                status_parts.append(f\"{chunks_truncated_doc} truncados\")\n",
    "            \n",
    "            if status_parts:\n",
    "                doc_pbar.set_postfix_str(f\"âœ… {', '.join(status_parts)}\")\n",
    "            \n",
    "            # Guardar archivo JSON actualizado si hay chunks marcados\n",
    "            if chunks_marked > 0:\n",
    "                try:\n",
    "                    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(document, f, ensure_ascii=False, indent=2)\n",
    "                    documents_updated += 1\n",
    "                except Exception as save_error:\n",
    "                    print(f\"âš ï¸ Error guardando {file_path}: {save_error}\")\n",
    "            elif result.get(\"already_indexed\"):\n",
    "                doc_pbar.set_postfix_str(\"âœ… Ya indexado\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error indexando {doc_id}: {e}\")\n",
    "            indexing_results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "        \n",
    "        finally:\n",
    "            # Cerrar progress bar de chunks\n",
    "            if chunk_pbar:\n",
    "                chunk_pbar.close()\n",
    "    \n",
    "    doc_pbar.close()\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Resumen final\n",
    "    successful_docs = sum(1 for r in indexing_results if r.get(\"success\", False))\n",
    "    \n",
    "    summary = {\n",
    "        \"total_documents\": len(discovered_files),\n",
    "        \"documents_processed\": len(documents_to_process),\n",
    "        \"successful_documents\": successful_docs,\n",
    "        \"failed_documents\": len(documents_to_process) - successful_docs,\n",
    "        \"documents_updated\": documents_updated,\n",
    "        \"total_chunks_indexed\": total_chunks_indexed,\n",
    "        \"total_chunks_failed\": total_chunks_failed,\n",
    "        \"total_chunks_truncated\": total_chunks_truncated,\n",
    "        \"total_chars_saved\": total_chars_saved,\n",
    "        \"total_time_seconds\": total_time,\n",
    "        \"avg_time_per_document\": total_time / len(documents_to_process) if documents_to_process else 0,\n",
    "        \"chunks_per_second\": total_chunks_indexed / total_time if total_time > 0 else 0,\n",
    "        \"results\": indexing_results\n",
    "    }\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š RESUMEN FINAL DE INDEXACIÃ“N:\")\n",
    "    print(f\"   âœ… Documentos procesados: {len(documents_to_process)}\")\n",
    "    print(f\"   âœ… Documentos exitosos: {successful_docs}\")\n",
    "    print(f\"   ðŸ’¾ Archivos JSON actualizados: {documents_updated}\")\n",
    "    print(f\"   ðŸ“¦ Chunks indexados: {total_chunks_indexed:,}\")\n",
    "    print(f\"   âŒ Chunks fallidos: {total_chunks_failed:,}\")\n",
    "    \n",
    "    # Mostrar estadÃ­sticas de truncado si hay chunks truncados\n",
    "    if total_chunks_truncated > 0:\n",
    "        print(f\"   âœ‚ï¸ Chunks truncados exitosamente: {total_chunks_truncated:,}\")\n",
    "        print(f\"   ðŸ’¾ Caracteres removidos: {total_chars_saved:,}\")\n",
    "        truncation_rate = (total_chunks_truncated / (total_chunks_indexed + total_chunks_failed)) * 100\n",
    "        print(f\"   ðŸ“Š Tasa de truncado: {truncation_rate:.1f}%\")\n",
    "        print(\"   ðŸ’¡ Los chunks truncados mantienen informaciÃ³n clave del inicio\")\n",
    "    \n",
    "    print(f\"   â±ï¸ Tiempo total: {total_time:.1f} segundos\")\n",
    "    print(f\"   ðŸš€ Velocidad: {summary['chunks_per_second']:.1f} chunks/segundo\")\n",
    "    \n",
    "    success_rate = (total_chunks_indexed / (total_chunks_indexed + total_chunks_failed)) * 100 if (total_chunks_indexed + total_chunks_failed) > 0 else 0\n",
    "    print(f\"   ðŸ“ˆ Tasa de Ã©xito: {success_rate:.1f}%\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"âœ… FunciÃ³n de indexaciÃ³n mejorada con manejo inteligente de chunks largos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRUEBA CON MANEJO DE CHUNKS LARGOS ===\n",
    "# Cambiar test_mode = True para probar la soluciÃ³n de chunks largos\n",
    "test_mode = False  # âœ… ACTIVADO para probar soluciÃ³n de chunks largos\n",
    "if test_mode and discovered_files:\n",
    "    print(\"ðŸ§ª MODO PRUEBA: Probando manejo inteligente de chunks largos\")\n",
    "    print(\"âœ‚ï¸ Esta prueba validarÃ¡ que chunks > 30,000 caracteres sean truncados automÃ¡ticamente\")\n",
    "    test_files = [discovered_files[2]]  # Solo el primer documento\n",
    "    indexing_summary = index_all_documents(client, test_files)\n",
    "    \n",
    "    print(\"\\\\nðŸ§ª ANÃLISIS DE PRUEBA:\")\n",
    "    if indexing_summary and \"results\" in indexing_summary:\n",
    "        result = indexing_summary[\"results\"][0]\n",
    "        if result.get(\"chunks_truncated\", 0) > 0:\n",
    "            print(f\"âœ… Ã‰XITO: {result['chunks_truncated']} chunks fueron truncados automÃ¡ticamente\")\n",
    "            print(f\"ðŸ’¾ Caracteres removidos: {result['chars_saved']:,}\")\n",
    "            print(\"âœ… No deberÃ­a haber errores de 'maximum context length'\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸ No se encontraron chunks que requirieran truncado en este documento\")\n",
    "        \n",
    "        if result.get(\"chunks_indexed\", 0) > 0:\n",
    "            print(f\"âœ… Ã‰XITO: {result['chunks_indexed']} chunks indexados exitosamente\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No se indexÃ³ ningÃºn chunk - revisar errores\")\n",
    "            \n",
    "    print(\"\\\\nðŸ§ª Prueba completada. Si no hay errores de 'maximum context length', la soluciÃ³n funciona.\")\n",
    "elif test_mode and not discovered_files:\n",
    "    print(\"âš ï¸ No hay documentos para probar\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Modo prueba desactivado (test_mode = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6556d098",
   "metadata": {},
   "source": [
    "# === INDEXACIÃ“N COMPLETA ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_indexing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar indexaciÃ³n\n",
    "if client and discovered_files:\n",
    "    print(\"ðŸŽ¯ Â¿Proceder con la indexaciÃ³n robusta?\")\n",
    "    \n",
    "    # Calcular solo chunks pendientes\n",
    "    total_pending = sum(f.get(\"chunks_pending\", 0) for f in discovered_files)\n",
    "    docs_with_pending = sum(1 for f in discovered_files if f.get(\"chunks_pending\", 0) > 0)\n",
    "    \n",
    "    if total_pending == 0:\n",
    "        print(\"ðŸŽ‰ Â¡Todos los chunks ya estÃ¡n indexados!\")\n",
    "        print(\"   No hay nada que procesar.\")\n",
    "        indexing_summary = {\n",
    "            \"all_indexed\": True,\n",
    "            \"message\": \"Todos los chunks ya estÃ¡n indexados\"\n",
    "        }\n",
    "    else:\n",
    "        print(f\"   ðŸ“„ Documentos con chunks pendientes: {docs_with_pending}\")\n",
    "        print(f\"   ðŸ“¦ Total chunks pendientes: {total_pending:,}\")\n",
    "        \n",
    "        # Estimar tiempo mejorado (con rate limiting y reintentos)\n",
    "        estimated_minutes = total_pending / 75  # MÃ¡s conservador: ~75 chunks por minuto\n",
    "        print(f\"   â±ï¸ Tiempo estimado: {estimated_minutes:.1f} minutos\")\n",
    "        print(f\"   ðŸ”„ Incluye reintentos automÃ¡ticos y manejo de rate limits\")\n",
    "        print(f\"   ðŸ’¾ Los archivos JSON se actualizarÃ¡n automÃ¡ticamente\")\n",
    "        \n",
    "        # Ejecutar indexaciÃ³n robusta\n",
    "        indexing_summary = index_all_documents(client, discovered_files)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results_file = \"indexing_results_robust.json\"\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(indexing_summary, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\\\nðŸ’¾ Resultados guardados en: {results_file}\")\n",
    "        \n",
    "        # Mostrar resumen de archivos actualizados\n",
    "        if indexing_summary.get(\"documents_updated\", 0) > 0:\n",
    "            print(f\"\\\\nðŸ“ ARCHIVOS JSON ACTUALIZADOS:\")\n",
    "            print(f\"   â€¢ {indexing_summary['documents_updated']} archivos con chunks marcados como indexados\")\n",
    "            print(f\"   â€¢ Las futuras ejecuciones saltarÃ¡n automÃ¡ticamente estos chunks\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No se puede proceder con la indexaciÃ³n:\")\n",
    "    if not client:\n",
    "        print(\"   - Sin conexiÃ³n a Weaviate\")\n",
    "    if not discovered_files:\n",
    "        print(\"   - No hay documentos para indexar\")\n",
    "    indexing_summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation_section",
   "metadata": {},
   "source": [
    "## 5. ValidaciÃ³n y EstadÃ­sticas\n",
    "\n",
    "Verificar que la indexaciÃ³n fue exitosa y mostrar estadÃ­sticas detalladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate_indexing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar indexaciÃ³n\n",
    "if client:\n",
    "    print(\"ðŸ” VALIDANDO INDEXACIÃ“N...\")\n",
    "    \n",
    "    # Obtener estadÃ­sticas actuales\n",
    "    current_stats = get_collection_stats(client, COLLECTION_NAME, 2000)\n",
    "    \n",
    "    if current_stats:\n",
    "        print(f\"\\nðŸ“Š ESTADÃSTICAS DE WEAVIATE:\")\n",
    "        print(f\"   ðŸ“¦ Total chunks indexados: {current_stats.get('total_chunks', 0):,}\")\n",
    "        print(f\"   ðŸ“„ Documentos Ãºnicos: {current_stats.get('total_documents', 0)}\")\n",
    "        \n",
    "        # Mostrar documentos indexados\n",
    "        if current_stats.get('documents'):\n",
    "            print(f\"\\nðŸ“‹ DOCUMENTOS EN WEAVIATE:\")\n",
    "            for doc_id, chunk_count in current_stats['documents'].items():\n",
    "                print(f\"   â€¢ {doc_id}: {chunk_count:,} chunks\")\n",
    "        \n",
    "        # Mostrar tipos de chunks\n",
    "        if current_stats.get('chunk_types'):\n",
    "            print(f\"\\nðŸ·ï¸ TIPOS DE CHUNKS:\")\n",
    "            for chunk_type, count in current_stats['chunk_types'].items():\n",
    "                print(f\"   â€¢ {chunk_type}: {count:,}\")\n",
    "        \n",
    "        # Comparar con archivos originales\n",
    "        if 'discovered_files' in locals() and discovered_files:\n",
    "            expected_chunks = sum(f[\"chunks_count\"] for f in discovered_files)\n",
    "            indexed_chunks = current_stats.get('total_chunks', 0)\n",
    "            \n",
    "            print(f\"\\nðŸ”„ COMPARACIÃ“N:\")\n",
    "            print(f\"   ðŸ“¥ Chunks esperados: {expected_chunks:,}\")\n",
    "            print(f\"   ðŸ“¤ Chunks indexados: {indexed_chunks:,}\")\n",
    "            \n",
    "            if indexed_chunks == expected_chunks:\n",
    "                print(f\"   âœ… Â¡IndexaciÃ³n completa al 100%!\")\n",
    "            elif indexed_chunks > 0:\n",
    "                coverage = (indexed_chunks / expected_chunks) * 100\n",
    "                print(f\"   ðŸ“Š Cobertura: {coverage:.1f}%\")\n",
    "                if coverage < 100:\n",
    "                    missing = expected_chunks - indexed_chunks\n",
    "                    print(f\"   âš ï¸ Faltan {missing:,} chunks\")\n",
    "            else:\n",
    "                print(f\"   âŒ No hay chunks indexados\")\n",
    "    else:\n",
    "        print(\"âŒ No se pudieron obtener estadÃ­sticas de Weaviate\")\n",
    "else:\n",
    "    print(\"âš ï¸ Sin conexiÃ³n a Weaviate para validaciÃ³n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search_testing_section",
   "metadata": {},
   "source": [
    "## 6. Pruebas de BÃºsqueda SemÃ¡ntica\n",
    "\n",
    "Probar el sistema de bÃºsqueda semÃ¡ntica con consultas filatÃ©licas especÃ­ficas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ef132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Thresholded + boosted + diversified Weaviate retrieval\n",
    "- Works with Weaviate Python client v4 (collections.query.*)\n",
    "- Backward-compatible name: search_chunks_semantic(...)\n",
    "- Adds: min_score gating, domain boosts (Scott/year/quality), dedup, MMR, multi-mode fallback\n",
    "\"\"\"\n",
    "\n",
    "import re, time, math, hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---- Weaviate v4 query helpers\n",
    "try:\n",
    "    from weaviate.classes import query as wv_query\n",
    "    from weaviate.classes.query import Filter as WvFilter\n",
    "except Exception:\n",
    "    # If you import differently in your project, adjust these two imports accordingly.\n",
    "    wv_query = None\n",
    "    WvFilter = None\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Utility helpers\n",
    "# =========================\n",
    "\n",
    "def _distance_to_similarity(distance: Optional[float], metric: str = \"cosine\") -> Optional[float]:\n",
    "    if distance is None:\n",
    "        return None\n",
    "    d = float(distance)\n",
    "    if metric == \"cosine\":\n",
    "        return max(0.0, min(1.0, 1.0 - d))  # cosine distance -> similarity in [0..1]\n",
    "    elif metric in (\"l2\", \"euclidean\"):\n",
    "        return 1.0 / (1.0 + d)\n",
    "    elif metric == \"dot\":  # heuristic\n",
    "        return 1.0 - (d / 2.0)\n",
    "    return None\n",
    "\n",
    "def _norm_score(raw: Optional[float]) -> float:\n",
    "    \"\"\"Normalize to [0,1]. Weaviate hybrid/bm25 'score' is usually [0..1]; vector similarity from our converter is also [0..1].\"\"\"\n",
    "    if raw is None:\n",
    "        return 0.0\n",
    "    return max(0.0, min(1.0, float(raw)))\n",
    "\n",
    "def _text_hash(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def _extract_query_years(query: str) -> List[int]:\n",
    "    years = []\n",
    "    for y in re.findall(r\"\\b(18\\d{2}|19\\d{2}|20\\d{2})\\b\", query):\n",
    "        try:\n",
    "            years.append(int(y))\n",
    "        except:\n",
    "            pass\n",
    "    return years\n",
    "\n",
    "def _year_overlap(query_years: List[int], hit_years: List[int]) -> bool:\n",
    "    if not query_years or not hit_years:\n",
    "        return False\n",
    "    qs = set(int(y) for y in query_years if str(y).isdigit())\n",
    "    hs = set(int(y) for y in hit_years if str(y).isdigit())\n",
    "    return len(qs.intersection(hs)) > 0\n",
    "\n",
    "def _boosts(hit: Dict[str, Any], query: str, requested_scotts: Optional[List[str]], query_years: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    Domain-aware boosts capped to 0.30 total.\n",
    "    - Scott exact match: +0.15\n",
    "    - Year overlap     : +0.08\n",
    "    - Chunk quality    : up to +0.07\n",
    "    \"\"\"\n",
    "    boost = 0.0\n",
    "\n",
    "    # Scott boost\n",
    "    if requested_scotts:\n",
    "        scotts = {str(s).strip().lower() for s in (hit.get(\"scott_numbers\") or [])}\n",
    "        want   = {str(s).strip().lower() for s in requested_scotts}\n",
    "        if scotts & want:\n",
    "            boost += 0.15\n",
    "\n",
    "    # Year overlap boost\n",
    "    if _year_overlap(query_years, hit.get(\"years\") or []):\n",
    "        boost += 0.08\n",
    "\n",
    "    # Quality boost\n",
    "    q = hit.get(\"quality_score\", 0.0)\n",
    "    try:\n",
    "        qn = max(0.0, min(1.0, float(q)))\n",
    "        boost += 0.07 * qn\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return min(boost, 0.30)\n",
    "\n",
    "def _passes_content_gates(hit: Dict[str, Any], min_chars: int) -> Tuple[bool, str]:\n",
    "    t = (hit.get(\"text\") or \"\").strip()\n",
    "    if len(t) < min_chars:\n",
    "        return False, f\"too_short<{min_chars}\"\n",
    "    # Gentle philately guard to prevent off-topic noise\n",
    "    if not re.search(r\"\\bstamp\\b|\\bperforat|\\bwatermark|\\bscott\\b|\\bsurcharge|\\bissue\\b\", t, flags=re.I):\n",
    "        return False, \"weak_domain_signal\"\n",
    "    return True, \"ok\"\n",
    "\n",
    "def _dedup(hits: List[Dict[str, Any]], max_per_doc: int = 2) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Cap to N per (doc_id, page_number) and deduplicate by text hash.\"\"\"\n",
    "    by_doc = defaultdict(int)\n",
    "    seen_hash = set()\n",
    "    out = []\n",
    "    for h in hits:\n",
    "        key = (h.get(\"doc_id\"), h.get(\"page_number\"))\n",
    "        hsh = _text_hash(h.get(\"text\") or \"\")\n",
    "        if hsh in seen_hash:\n",
    "            h[\"_reject_reason\"] = \"dup_text\"\n",
    "            continue\n",
    "        if by_doc[key] >= max_per_doc:\n",
    "            h[\"_reject_reason\"] = \"doc_cap\"\n",
    "            continue\n",
    "        by_doc[key] += 1\n",
    "        seen_hash.add(hsh)\n",
    "        out.append(h)\n",
    "    return out\n",
    "\n",
    "def _mmr_select(candidates: List[Dict[str, Any]], k: int, lambda_diversity: float = 0.7) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Lightweight MMR using Jaccard similarity on token sets.\n",
    "    Assumes 'final_score' exists.\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "    chosen, rest = [], candidates[:]\n",
    "    for c in rest:\n",
    "        toks = set(re.findall(r\"[a-z0-9]+\", (c.get(\"text\") or \"\").lower()))\n",
    "        c[\"_tokset\"] = toks\n",
    "    while rest and len(chosen) < k:\n",
    "        best, best_val = None, -1e9\n",
    "        for c in rest:\n",
    "            relevance = float(c.get(\"final_score\", 0.0))\n",
    "            diversity_bonus = 0.0\n",
    "            if chosen:\n",
    "                max_sim = 0.0\n",
    "                for p in chosen:\n",
    "                    inter = len(c[\"_tokset\"].intersection(p[\"_tokset\"]))\n",
    "                    union = len(c[\"_tokset\"].union(p[\"_tokset\"])) or 1\n",
    "                    jacc = inter / union\n",
    "                    max_sim = max(max_sim, jacc)\n",
    "                diversity_bonus = (1 - max_sim)  # prefer lower similarity\n",
    "            val = lambda_diversity * relevance + (1 - lambda_diversity) * diversity_bonus\n",
    "            if val > best_val:\n",
    "                best, best_val = c, val\n",
    "        chosen.append(best)\n",
    "        rest.remove(best)\n",
    "    for c in chosen:\n",
    "        c.pop(\"_tokset\", None)\n",
    "    return chosen\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Filters (Weaviate v4)\n",
    "# =========================\n",
    "\n",
    "def _build_filters(filters: Optional[Dict[str, Any]]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Build Weaviate v4 Filter (where clause) from your light dict.\n",
    "    Supported keys in `filters`:\n",
    "      - \"year_range\": (start:int, end:int)\n",
    "      - \"scott_numbers\": List[str]   (TEXT_ARRAY containsAny)\n",
    "      - \"catalog_system\": \"Scott\"    (TEXT eq)\n",
    "      - you can extend as needed\n",
    "    \"\"\"\n",
    "    if not filters:\n",
    "        return None\n",
    "    clauses = []\n",
    "\n",
    "    if \"year_range\" in filters and isinstance(filters[\"year_range\"], (tuple, list)) and len(filters[\"year_range\"]) == 2:\n",
    "        ys, ye = filters[\"year_range\"]\n",
    "        # Prefer range-capable fields if present in your schema; if not, this will still work if you added year_start/year_end.\n",
    "        try:\n",
    "            c1 = WvFilter.by_property(\"year_start\").greater_than_equal(ys)\n",
    "            c2 = WvFilter.by_property(\"year_end\").less_than_equal(ye)\n",
    "            clauses.append(c1)\n",
    "            clauses.append(c2)\n",
    "        except Exception:\n",
    "            # Fallback: if you only have INT_ARRAY 'years', we approximate with containsAny of all years in range (coarse)\n",
    "            year_list = list(range(int(ys), int(ye) + 1))\n",
    "            clauses.append(WvFilter.by_property(\"years\").contains_any(year_list))\n",
    "\n",
    "    if filters.get(\"catalog_system\"):\n",
    "        clauses.append(WvFilter.by_property(\"catalog_systems\").contains_any([filters[\"catalog_system\"]]))\n",
    "\n",
    "    if filters.get(\"scott_numbers\"):\n",
    "        clauses.append(WvFilter.by_property(\"scott_numbers\").contains_any(list(filters[\"scott_numbers\"])))\n",
    "\n",
    "    if not clauses:\n",
    "        return None\n",
    "\n",
    "    # AND all clauses\n",
    "    where = clauses[0]\n",
    "    for c in clauses[1:]:\n",
    "        where = where & c\n",
    "    return where\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main: thresholded + diversified retrieval\n",
    "# =========================\n",
    "\n",
    "def search_chunks_semantic(\n",
    "    client,\n",
    "    query: str,\n",
    "    collection_name: str = \"Oxcart\",\n",
    "    limit: int = 5,\n",
    "    filters: Optional[Dict[str, Any]] = None,\n",
    "    mode: str = \"vector\",          # kept for backward-compat; now we may try multi-stage if needed\n",
    "    alpha: float = 0.35,           # for hybrid\n",
    "    distance_metric: str = \"cosine\",\n",
    "    # --- New safety/quality knobs (tunable) ---\n",
    "    min_score: float = 0.55,       # threshold AFTER boosts (0..1)\n",
    "    min_chars: int = 280,          # tiny snippet filter\n",
    "    mmr_lambda: float = 0.7,       # 0.5..0.8 usually fine\n",
    "    overfetch_factor: int = 3,     # fetch NÃ—limit then gate\n",
    "    k_min: int = 3,                # minimum contexts needed\n",
    "    requested_scotts: Optional[List[str]] = None,  # domain boost\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Advanced retrieval with thresholding, boosts, dedup, MMR and multi-stage fallback.\n",
    "    Returns a list of results with 'final_score' and 'stage' annotations.\n",
    "    \"\"\"\n",
    "    # Prepare\n",
    "    coll = client.collections.get(collection_name)\n",
    "    f = _build_filters(filters)\n",
    "    query_years = _extract_query_years(query)\n",
    "    rejected_reasons = defaultdict(int)\n",
    "\n",
    "    def _run(mode_local: str, label: str, hard_limit: int) -> List[Dict[str, Any]]:\n",
    "        # Query Weaviate\n",
    "        if mode_local == \"hybrid\":\n",
    "            resp = coll.query.hybrid(\n",
    "                query=query,\n",
    "                alpha=alpha,\n",
    "                limit=hard_limit,\n",
    "                filters=f,\n",
    "                return_properties=[\n",
    "                    \"chunk_id\",\"chunk_type\",\"text\",\"text_original\",\"doc_id\",\"page_number\",\n",
    "                    \"catalog_systems\",\"catalog_numbers\",\"scott_numbers\",\"years\",\"colors\",\n",
    "                    \"topics_primary\",\"variety_classes\",\"has_catalog\",\"has_prices\",\"has_varieties\",\n",
    "                    \"is_guanacaste\",\"quality_score\"\n",
    "                ],\n",
    "                return_metadata=wv_query.MetadataQuery(score=True, distance=True),\n",
    "            )\n",
    "        elif mode_local == \"bm25\":\n",
    "            resp = coll.query.bm25(\n",
    "                query=query,\n",
    "                limit=hard_limit,\n",
    "                filters=f,\n",
    "                return_properties=[\n",
    "                    \"chunk_id\",\"chunk_type\",\"text\",\"text_original\",\"doc_id\",\"page_number\",\n",
    "                    \"catalog_systems\",\"catalog_numbers\",\"scott_numbers\",\"years\",\"colors\",\n",
    "                    \"topics_primary\",\"variety_classes\",\"has_catalog\",\"has_prices\",\"has_varieties\",\n",
    "                    \"is_guanacaste\",\"quality_score\"\n",
    "                ],\n",
    "                return_metadata=wv_query.MetadataQuery(score=True),\n",
    "            )\n",
    "        else:\n",
    "            resp = coll.query.near_text(\n",
    "                query=query,\n",
    "                limit=hard_limit,\n",
    "                filters=f,\n",
    "                return_properties=[\n",
    "                    \"chunk_id\",\"chunk_type\",\"text\",\"text_original\",\"doc_id\",\"page_number\",\n",
    "                    \"catalog_systems\",\"catalog_numbers\",\"scott_numbers\",\"years\",\"colors\",\n",
    "                    \"topics_primary\",\"variety_classes\",\"has_catalog\",\"has_prices\",\"has_varieties\",\n",
    "                    \"is_guanacaste\",\"quality_score\"\n",
    "                ],\n",
    "                return_metadata=wv_query.MetadataQuery(distance=True),\n",
    "            )\n",
    "\n",
    "        raw_out = []\n",
    "        for obj in (resp.objects or []):\n",
    "            props = obj.properties or {}\n",
    "            meta = getattr(obj, \"metadata\", None)\n",
    "            distance = getattr(meta, \"distance\", None) if meta else None\n",
    "            hybrid_score = getattr(meta, \"score\", None) if meta else None\n",
    "\n",
    "            similarity = _distance_to_similarity(distance, metric=distance_metric)\n",
    "            base_score = hybrid_score if hybrid_score is not None else (similarity if similarity is not None else 0.0)\n",
    "\n",
    "            # De-duplicate figure markdown inside text_original (your original logic, preserved)\n",
    "            figure_pattern = r'(!\\[([^\\]]*)\\]\\([^)]+\\))'\n",
    "            original_content = props.get(\"text_original\", \"\") or props.get(\"text\", \"\") or \"\"\n",
    "\n",
    "            figures = re.findall(figure_pattern, original_content)\n",
    "            seen_figures = set()\n",
    "            unique_figures = []\n",
    "            for fig in figures:\n",
    "                img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                if img_path:\n",
    "                    img_identifier = img_path.group(1)\n",
    "                    if img_identifier not in seen_figures:\n",
    "                        seen_figures.add(img_identifier)\n",
    "                        unique_figures.append(fig)\n",
    "\n",
    "            existing_figures = set()\n",
    "            for fig in unique_figures:\n",
    "                if fig[0] in original_content:\n",
    "                    img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                    if img_path:\n",
    "                        existing_figures.add(img_path.group(1))\n",
    "\n",
    "            missing_figures = []\n",
    "            for fig in unique_figures:\n",
    "                img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                if img_path and img_path.group(1) not in existing_figures:\n",
    "                    missing_figures.append(fig[0])\n",
    "\n",
    "            if missing_figures:\n",
    "                figures_text = \"\\n\\n\" + \"\\n\".join(missing_figures)\n",
    "                original_content = original_content + figures_text\n",
    "\n",
    "            raw_out.append({\n",
    "                \"uuid\": str(obj.uuid),\n",
    "                \"score\": base_score,             # original score (hybrid/bm25) or similarity (vector)\n",
    "                \"similarity\": similarity,\n",
    "                \"distance\": distance,\n",
    "                \"chunk_id\": props.get(\"chunk_id\", \"\"),\n",
    "                \"chunk_type\": props.get(\"chunk_type\", \"\"),\n",
    "                \"text\": original_content,\n",
    "                \"doc_id\": props.get(\"doc_id\", \"\"),\n",
    "                \"page_number\": props.get(\"page_number\", 0),\n",
    "                \"catalog_systems\": props.get(\"catalog_systems\", []),\n",
    "                \"catalog_numbers\": props.get(\"catalog_numbers\", []),\n",
    "                \"scott_numbers\": props.get(\"scott_numbers\", []),\n",
    "                \"years\": props.get(\"years\", []),\n",
    "                \"colors\": props.get(\"colors\", []),\n",
    "                \"topics_primary\": props.get(\"topics_primary\", \"\"),\n",
    "                \"variety_classes\": props.get(\"variety_classes\", []),\n",
    "                \"has_catalog\": props.get(\"has_catalog\", False),\n",
    "                \"has_prices\": props.get(\"has_prices\", False),\n",
    "                \"has_varieties\": props.get(\"has_varieties\", False),\n",
    "                \"is_guanacaste\": props.get(\"is_guanacaste\", False),\n",
    "                \"quality_score\": props.get(\"quality_score\", 0.0),\n",
    "                \"mode\": mode_local,\n",
    "                \"stage\": label,\n",
    "            })\n",
    "        return raw_out\n",
    "\n",
    "    def _gate_and_rank(raw_hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        gated = []\n",
    "        for h in raw_hits:\n",
    "            s_norm = _norm_score(h.get(\"score\"))\n",
    "            b = _boosts(h, query, requested_scotts, query_years)\n",
    "            final = max(0.0, min(1.0, s_norm + b))\n",
    "            ok, reason = _passes_content_gates(h, min_chars=min_chars)\n",
    "            if not ok:\n",
    "                h[\"_reject_reason\"] = reason\n",
    "                rejected_reasons[reason] += 1\n",
    "                continue\n",
    "            if final < min_score:\n",
    "                h[\"_reject_reason\"] = f\"below_threshold<{min_score:.2f}>\"\n",
    "                rejected_reasons[h[\"_reject_reason\"]] += 1\n",
    "                continue\n",
    "\n",
    "            h[\"norm_score\"] = s_norm\n",
    "            h[\"boost\"] = b\n",
    "            h[\"final_score\"] = final\n",
    "            gated.append(h)\n",
    "\n",
    "        # dedup per doc & by text hash\n",
    "        deduped = _dedup(gated, max_per_doc=2)\n",
    "        # diversify with MMR\n",
    "        diversified = _mmr_select(\n",
    "            sorted(deduped, key=lambda x: x[\"final_score\"], reverse=True),\n",
    "            k=min(limit, len(deduped)),\n",
    "            lambda_diversity=mmr_lambda\n",
    "        )\n",
    "        return diversified\n",
    "\n",
    "    # -------------------------\n",
    "    # Multi-stage retrieval\n",
    "    # -------------------------\n",
    "    # 1) Try the user-requested mode first (keeps backward compatibility)\n",
    "    modes_order = [mode]\n",
    "    for m in (\"hybrid\", \"bm25\", \"vector\"):\n",
    "        if m not in modes_order:\n",
    "            modes_order.append(m)\n",
    "\n",
    "    gathered: List[Dict[str, Any]] = []\n",
    "    for mi, m in enumerate(modes_order):\n",
    "        raw = _run(m, f\"S{mi+1}:{m}+filters\", hard_limit=limit * overfetch_factor)\n",
    "        gated = _gate_and_rank(raw)\n",
    "\n",
    "        # Merge by uuid, keep best final_score\n",
    "        heap = {g[\"uuid\"]: g for g in gathered}\n",
    "        for g in gated:\n",
    "            if g[\"uuid\"] not in heap or g[\"final_score\"] > heap[g[\"uuid\"]][\"final_score\"]:\n",
    "                heap[g[\"uuid\"]] = g\n",
    "        gathered = list(heap.values())\n",
    "        gathered.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "        gathered = gathered[:limit]\n",
    "        if len(gathered) >= k_min:\n",
    "            break\n",
    "\n",
    "    # 2) Relax filters if still not enough (drop Scott, then year)\n",
    "    if len(gathered) < k_min and filters:\n",
    "        relaxed = dict(filters)\n",
    "        if \"scott_numbers\" in relaxed:\n",
    "            relaxed.pop(\"scott_numbers\")\n",
    "        elif \"year_range\" in relaxed:\n",
    "            relaxed.pop(\"year_range\")\n",
    "\n",
    "        f_relaxed = _build_filters(relaxed)\n",
    "        if f_relaxed is not None:\n",
    "            for mi, m in enumerate(modes_order):\n",
    "                # temporarily replace f for this pass\n",
    "                _old_f = f\n",
    "                try:\n",
    "                    f = f_relaxed\n",
    "                    raw = _run(m, f\"Rx{mi+1}:{m}+relaxed\", hard_limit=limit * overfetch_factor)\n",
    "                    gated = _gate_and_rank(raw)\n",
    "                    heap = {g[\"uuid\"]: g for g in gathered}\n",
    "                    for g in gated:\n",
    "                        if g[\"uuid\"] not in heap or g[\"final_score\"] > heap[g[\"uuid\"]][\"final_score\"]:\n",
    "                            heap[g[\"uuid\"]] = g\n",
    "                    gathered = list(heap.values())\n",
    "                    gathered.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "                    gathered = gathered[:limit]\n",
    "                    if len(gathered) >= k_min:\n",
    "                        break\n",
    "                finally:\n",
    "                    f = _old_f  # restore\n",
    "\n",
    "    # You can inspect `rejected_reasons` here for debugging if needed.\n",
    "    return gathered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770ed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Example usage (commented)\n",
    "# =========================\n",
    "results = search_chunks_semantic(\n",
    "    client=client,\n",
    "    query=\"Costa Rica 1881-82 surcharges Scott 55\",\n",
    "    collection_name=\"Oxcart\",\n",
    "    limit=60,\n",
    "    filters={},\n",
    "    mode=\"hybrid\",\n",
    "    alpha=0.35,\n",
    "    min_score=0.55,\n",
    "    min_chars=280,\n",
    "    k_min=3,\n",
    "    requested_scotts=[],\n",
    ")\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['final_score']:.3f} | {r['stage']} | {r['doc_id']} p.{r['page_number']} | Scotts={r.get('scott_numbers')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_searches",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_philatelic_searches(client) -> None:\n",
    "    \"\"\"\n",
    "    Ejecutar bÃºsquedas de prueba para validar el sistema.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        print(\"âŒ Sin conexiÃ³n a Weaviate\")\n",
    "        return\n",
    "    \n",
    "    # Consultas de prueba filatÃ©licas\n",
    "    test_queries = [\n",
    "        {\n",
    "            \"name\": \"BÃºsqueda general de sellos\",\n",
    "            \"query\": \"stamps Scott catalog Costa Rica\",\n",
    "            \"filters\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CatÃ¡logo Scott especÃ­fico\",\n",
    "            \"query\": \"Scott catalog numbers\",\n",
    "            \"filters\": {\"catalog_system\": \"Scott\"}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Sobrecargas y variedades\",\n",
    "            \"query\": \"overprint surcharge variety error\",\n",
    "            \"filters\": {\"has_varieties\": True}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Periodo Guanacaste\",\n",
    "            \"query\": \"Guanacaste overprint historical Costa Rica\",\n",
    "            \"filters\": {\"is_guanacaste\": True}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Especificaciones tÃ©cnicas\",\n",
    "            \"query\": \"perforation paper printing watermark\",\n",
    "            \"filters\": {\"has_technical_specs\": True}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Tablas con datos\",\n",
    "            \"query\": \"catalog table prices values\",\n",
    "            \"filters\": {\"chunk_type\": \"table\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ” EJECUTANDO BÃšSQUEDAS DE PRUEBA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, test in enumerate(test_queries, 1):\n",
    "        print(f\"\\nðŸ”Ž [{i}/{len(test_queries)}] {test['name']}\")\n",
    "        print(f\"   Query: \\\"{test['query']}\\\"\")\n",
    "        if test['filters']:\n",
    "            print(f\"   Filtros: {test['filters']}\")\n",
    "        \n",
    "        try:\n",
    "            results = search_chunks_semantic(\n",
    "                client, \n",
    "                test[\"query\"], \n",
    "                \"Oxcart\", \n",
    "                limit=3,\n",
    "                filters=test[\"filters\"]\n",
    "            )\n",
    "            \n",
    "            print(f\"   ðŸ“Š Resultados: {len(results)}\")\n",
    "            \n",
    "            for j, result in enumerate(results, 1):\n",
    "                print(f\"\\n      ðŸ·ï¸ #{j} (Score: {result['score']:.3f})\")\n",
    "                print(f\"         ðŸ“„ Documento: {result['doc_id']}\")\n",
    "                print(f\"         ðŸ“‹ Tipo: {result['chunk_type']}\")\n",
    "                print(f\"         ðŸ“„ PÃ¡gina: {result['page_number']}\")\n",
    "                \n",
    "                # Mostrar metadatos relevantes\n",
    "                if result.get('catalog_systems'):\n",
    "                    print(f\"         ðŸ“– CatÃ¡logos: {result['catalog_systems']}\")\n",
    "                if result.get('scott_numbers'):\n",
    "                    print(f\"         ðŸ”¢ Scott: {result['scott_numbers']}\")\n",
    "                if result.get('years'):\n",
    "                    print(f\"         ðŸ“… AÃ±os: {result['years']}\")\n",
    "                if result.get('colors'):\n",
    "                    print(f\"         ðŸŽ¨ Colores: {result['colors']}\")\n",
    "                if result.get('variety_classes'):\n",
    "                    print(f\"         ðŸ”€ Variedades: {result['variety_classes']}\")\n",
    "                \n",
    "                # Texto truncado\n",
    "                text = result.get('text', '')\n",
    "                if len(text) > 200:\n",
    "                    text = text[:200] + \"...\"\n",
    "                print(f\"         ðŸ“ Texto: {text}\")\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"   âš ï¸ No se encontraron resultados\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error en bÃºsqueda: {e}\")\n",
    "        \n",
    "        print(\"   \" + \"-\" * 50)\n",
    "\n",
    "# # Ejecutar pruebas de bÃºsqueda\n",
    "# if client:\n",
    "#     test_philatelic_searches(client)\n",
    "# else:\n",
    "#     print(\"âš ï¸ No se pueden ejecutar bÃºsquedas sin conexiÃ³n a Weaviate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79867c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_chunks_semantic(\n",
    "                client, \n",
    "                \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\", \n",
    "                \"Oxcart\", \n",
    "                limit=100,\n",
    "                filters=[],\n",
    "                mode = \"hybrid\",\n",
    "                alpha= 0.45\n",
    "                \n",
    "            )\n",
    "            \n",
    "print(f\"   ðŸ“Š Resultados: {len(results)}\")\n",
    "\n",
    "for j, result in enumerate(results, 1):\n",
    "    print(f\"\\n      ðŸ·ï¸ #{j} (Score: {result['score']:.3f})\")\n",
    "    print(f\"         ðŸ“„ Documento: {result['doc_id']}\")\n",
    "    print(f\"         ðŸ“‹ Tipo: {result['chunk_type']}\")\n",
    "    print(f\"         ðŸ“„ PÃ¡gina: {result['page_number']}\")\n",
    "    \n",
    "    # Mostrar metadatos relevantes\n",
    "    if result.get('catalog_systems'):\n",
    "        print(f\"         ðŸ“– CatÃ¡logos: {result['catalog_systems']}\")\n",
    "    if result.get('scott_numbers'):\n",
    "        print(f\"         ðŸ”¢ Scott: {result['scott_numbers']}\")\n",
    "    if result.get('years'):\n",
    "        print(f\"         ðŸ“… AÃ±os: {result['years']}\")\n",
    "    if result.get('colors'):\n",
    "        print(f\"         ðŸŽ¨ Colores: {result['colors']}\")\n",
    "    if result.get('variety_classes'):\n",
    "        print(f\"         ðŸ”€ Variedades: {result['variety_classes']}\")\n",
    "    \n",
    "    # Texto truncado\n",
    "    text = result.get('text', '')\n",
    "    # if len(text) > 200:\n",
    "    #     text = text[:200] + \"...\"\n",
    "    print(f\"         ðŸ“ Texto: {text}\")\n",
    "    print(\"**********************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradio_section",
   "metadata": {},
   "source": [
    "## 7. Interfaz Gradio para RAG\n",
    "\n",
    "Interfaz web interactiva para bÃºsquedas semÃ¡nticas y consultas RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradio_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     import gradio as gr\n",
    "#     import openai\n",
    "#     gradio_available = True\n",
    "#     print(\"âœ… Gradio disponible\")\n",
    "# except ImportError:\n",
    "#     print(\"âš ï¸ Gradio no estÃ¡ instalado\")\n",
    "#     print(\"ðŸ’¡ Para instalar: pip install gradio\")\n",
    "#     gradio_available = False\n",
    "\n",
    "# # Configurar OpenAI para RAG\n",
    "# if OPENAI_API_KEY:\n",
    "#     openai.api_key = OPENAI_API_KEY\n",
    "#     openai_available = True\n",
    "# else:\n",
    "#     openai_available = False\n",
    "#     print(\"âš ï¸ OpenAI API key no configurada para RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# def search_and_answer(\n",
    "#     query: str,\n",
    "#     rag_system: Dict[str, Any],\n",
    "#     use_filters: bool = False,\n",
    "#     catalog_system: str = \"\",\n",
    "#     chunk_type: str = \"\",\n",
    "#     has_varieties: bool = False,\n",
    "#     max_results: int = 10,\n",
    "# ) -> Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:\n",
    "#     \"\"\"\n",
    "#     BÃºsqueda semÃ¡ntica + RAG (OpenAI >= 1.0, modelo gpt-4o-mini).\n",
    "#     Devuelve: (respuesta_rag, resultados(lista de dicts), metadatos(dict))\n",
    "#     \"\"\"\n",
    "#     # ValidaciÃ³n de conexiÃ³n\n",
    "#     if not rag_system or not rag_system.get(\"client\"):\n",
    "#         meta = {\"query\": query, \"total_results\": 0, \"max_results\": max_results, \"filters_used\": {}, \"context_length\": 0}\n",
    "#         return \"âŒ Error: Sin conexiÃ³n a Weaviate\", [], meta\n",
    "\n",
    "#     client_wv = rag_system[\"client\"]\n",
    "#     collection_name = rag_system.get(\"collection_name\", \"Oxcart\")\n",
    "\n",
    "#     # Construir filtros\n",
    "#     filt = None\n",
    "#     if use_filters:\n",
    "#         filt = {}\n",
    "#         if catalog_system:\n",
    "#             filt[\"catalog_system\"] = catalog_system\n",
    "#         if chunk_type:\n",
    "#             filt[\"chunk_type\"] = chunk_type\n",
    "#         if has_varieties:\n",
    "#             filt[\"has_varieties\"] = True\n",
    "\n",
    "#     # BÃºsqueda semÃ¡ntica (usa tu funciÃ³n ya definida)\n",
    "#     results = search_chunks_semantic(\n",
    "#         client=client_wv,\n",
    "#         query=query,\n",
    "#         collection_name=collection_name,\n",
    "#         limit=int(max_results),\n",
    "#         filters=filt,\n",
    "#         mode = \"hybrid\",\n",
    "#         alpha= 0.35\n",
    "#     )\n",
    "\n",
    "#     # Preparar contexto para RAG (top 3)\n",
    "#     top = results[:3]\n",
    "#     context = \"\\n\\n\".join(\n",
    "#         f\"Documento {r.get('doc_id', 'N/A')} (PÃ¡gina {r.get('page_number', 'Â¿?')}): {r.get('text','')}\"\n",
    "#         for r in top\n",
    "#     )\n",
    "#     context_len = len(context)\n",
    "\n",
    "#     # Generar respuesta RAG (OpenAI >= 1.0.0)\n",
    "#     rag_answer = \"âš ï¸ No se encontraron resultados para generar respuesta\"\n",
    "#     openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#     if not results:\n",
    "#         rag_answer = \"âš ï¸ No se encontraron resultados para generar respuesta\"\n",
    "#     elif not openai_key:\n",
    "#         rag_answer = \"âš ï¸ RAG no disponible: OpenAI API key no configurada\"\n",
    "#     else:\n",
    "#         try:\n",
    "#             from openai import OpenAI\n",
    "#             oa_client = OpenAI(api_key=openai_key)\n",
    "\n",
    "#             system_prompt = (\n",
    "#                 \"You are an expert in costa rica philately (stamps, covers, etc). \"\n",
    "#                 \"Only answer based with the information provided. If there is not enough info for answer please, \"\n",
    "#                 \"answer with: 'I dont have information'. You must include any references about philatelic like scott catalogue references, dates, etc.\"\n",
    "#             )\n",
    "\n",
    "#             model = os.getenv(\"RAG_MODEL\", \"gpt-4o-mini\")\n",
    "#             resp = oa_client.chat.completions.create(\n",
    "#                 model=model,\n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#                     {\"role\": \"user\", \"content\": f\"Here is the information for your answers:\\n{context}\\n\\nAnswer this only with the information provided: {query}\"}\n",
    "#                 ],\n",
    "#                 temperature=0.3,\n",
    "#                 max_tokens=1000,\n",
    "#             )\n",
    "\n",
    "#             rag_text = resp.choices[0].message.content if resp.choices else \"\"\n",
    "#             if not rag_text:\n",
    "#                 rag_text = \"No se obtuvo texto de respuesta del modelo.\"\n",
    "\n",
    "#             rag_answer = (\n",
    "#                 \"ðŸ¤– **Respuesta RAG:**\\n\\n\"\n",
    "#                 + rag_text\n",
    "#                 + f\"\\n\\nðŸ“Š *Basado en {len(results)} resultados de bÃºsqueda*\"\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             rag_answer = f\"âŒ Error generando respuesta RAG: {e}\"\n",
    "\n",
    "#     metadata = {\n",
    "#         \"query\": query,\n",
    "#         \"total_results\": len(results),\n",
    "#         \"max_results\": int(max_results),\n",
    "#         \"filters_used\": filt or {},\n",
    "#         \"context_length\": context_len,\n",
    "#     }\n",
    "#     return rag_answer, results, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_collection_info() -> str:\n",
    "#     \"\"\"\n",
    "#     Obtener informaciÃ³n de la colecciÃ³n para mostrar en la interfaz.\n",
    "#     \"\"\"\n",
    "#     if not client:\n",
    "#         return \"âŒ Sin conexiÃ³n a Weaviate\"\n",
    "    \n",
    "#     try:\n",
    "#         stats = get_collection_stats(client, \"Oxcart\")\n",
    "#         if stats:\n",
    "#             info = f\"ðŸ“Š **EstadÃ­sticas de la ColecciÃ³n Oxcart:**\\n\\n\"\n",
    "#             info += f\"ðŸ“¦ **Total chunks:** {stats['total_chunks']:,}\\n\"\n",
    "#             info += f\"ðŸ“„ **Documentos:** {stats['total_documents']}\\n\\n\"\n",
    "            \n",
    "#             if stats.get('documents'):\n",
    "#                 info += \"**Documentos indexados:**\\n\"\n",
    "#                 for doc_id, count in stats['documents'].items():\n",
    "#                     info += f\"â€¢ {doc_id}: {count:,} chunks\\n\"\n",
    "            \n",
    "#             return info\n",
    "#         else:\n",
    "#             return \"âŒ No se pudieron obtener estadÃ­sticas\"\n",
    "#     except Exception as e:\n",
    "#         return f\"âŒ Error: {e}\"\n",
    "\n",
    "# print(\"âœ… Funciones RAG definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36640970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = get_collection_stats(client, \"Oxcart\")\n",
    "# stats['total_documents']\n",
    "# stats['total_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Estructura que usan tus funciones de bÃºsqueda/respuesta\n",
    "# rag_system = {\n",
    "#     \"success\": True,\n",
    "#     \"client\": client,                    # para que search_and_answer pueda consultar\n",
    "#     \"collection_name\": COLLECTION_NAME,  # nombre de la colecciÃ³n\n",
    "#     \"weaviate_url\": WEAVIATE_URL,        # info para la UI\n",
    "#     \"total_documents\": stats['total_documents'],       # para mostrar estado\n",
    "#     \"total_chunks\": stats['total_chunks'],        # opcional en la UI\n",
    "#     # puedes aÃ±adir mÃ¡s campos que tu search_and_answer necesite\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "launch_gradio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gradio as gr\n",
    "# from typing import Dict, Any\n",
    "# import threading\n",
    "# import time\n",
    "\n",
    "# def create_gradio_interface(rag_system: Dict[str, Any]) -> gr.Blocks:\n",
    "#     \"\"\"\n",
    "#     Crea la interfaz Gradio para consultas RAG.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def gradio_search_and_answer(query, use_filters, catalog_system, chunk_type, has_varieties, max_results):\n",
    "#         \"\"\"\n",
    "#         Wrapper para Gradio: llama a search_and_answer y formatea salidas.\n",
    "#         \"\"\"\n",
    "#         if not rag_system:\n",
    "#             return \"âŒ Sistema RAG no estÃ¡ configurado\", \"No hay resultados\", \"No hay metadatos\"\n",
    "\n",
    "#         # Llamada a tu funciÃ³n (se asume definida en tu entorno)\n",
    "#         answer, results, metadata = search_and_answer(\n",
    "#             query=query,\n",
    "#             rag_system=rag_system,\n",
    "#             use_filters=use_filters,\n",
    "#             catalog_system=catalog_system,\n",
    "#             chunk_type=chunk_type,\n",
    "#             has_varieties=has_varieties,\n",
    "#             max_results=int(max_results),\n",
    "#         )\n",
    "\n",
    "#         # --- Formatear resultados de bÃºsqueda ---\n",
    "#         lines = []\n",
    "#         if results:\n",
    "#             for i, r in enumerate(results):\n",
    "#                 doc_id = r.get(\"doc_id\") or r.get(\"document_id\", \"N/A\")\n",
    "#                 chunk_type_val = r.get(\"chunk_type\", \"N/A\")\n",
    "#                 page_number = r.get(\"page_number\", \"N/A\")\n",
    "#                 catalogs = r.get(\"catalog_systems\") or []\n",
    "#                 scotts = r.get(\"scott_numbers\") or []\n",
    "#                 years = r.get(\"years\") or []\n",
    "\n",
    "#                 # Vista previa: usa content_preview si existe; si no, toma 'text'\n",
    "#                 preview = r.get(\"content_preview\")\n",
    "#                 if not preview:\n",
    "#                     text = r.get(\"text\", \"\")\n",
    "#                     preview = (text[:300] + \"...\") if len(text) > 300 else text\n",
    "\n",
    "#                 block = []\n",
    "#                 block.append(f\"**Resultado {i+1}**\")\n",
    "#                 block.append(f\"â€¢ Documento: {doc_id}\")\n",
    "#                 block.append(f\"â€¢ Tipo: {chunk_type_val} | PÃ¡gina: {page_number}\")\n",
    "#                 if catalogs:\n",
    "#                     block.append(f\"â€¢ CatÃ¡logos: {', '.join(catalogs)}\")\n",
    "#                 if scotts:\n",
    "#                     block.append(f\"â€¢ Scott: {', '.join(scotts)}\")\n",
    "#                 if years:\n",
    "#                     block.append(f\"â€¢ AÃ±os: {', '.join(str(y) for y in years)}\")\n",
    "#                 block.append(f\"â€¢ Vista previa: {preview}\")\n",
    "#                 block.append(\"-\" * 50)\n",
    "#                 lines.append(\"\\n\".join(block))\n",
    "#             search_output = \"\\n\".join(lines)\n",
    "#         else:\n",
    "#             search_output = \"No se encontraron resultados\"\n",
    "\n",
    "#         # --- Formatear metadatos ---\n",
    "#         metadata = metadata or {}\n",
    "#         metadata_output = (\n",
    "#             \"**Metadatos de la consulta:**\\n\"\n",
    "#             f\"â€¢ Consulta: {metadata.get('query', 'N/A')}\\n\"\n",
    "#             f\"â€¢ Resultados encontrados: {metadata.get('total_results', 0)}\\n\"\n",
    "#             f\"â€¢ MÃ¡ximo solicitado: {metadata.get('max_results', 'N/A')}\\n\"\n",
    "#             f\"â€¢ Filtros usados: {metadata.get('filters_used', {})}\\n\"\n",
    "#             f\"â€¢ Longitud del contexto: {metadata.get('context_length', 'N/A')} caracteres\\n\"\n",
    "#         )\n",
    "\n",
    "#         return answer, search_output, metadata_output\n",
    "\n",
    "#     # Valores informativos del sistema\n",
    "#     collection_name = rag_system.get(\"collection_name\", \"Oxcart\")\n",
    "#     total_docs = rag_system.get(\"total_documents\", 0)\n",
    "#     weaviate_url = rag_system.get(\"weaviate_url\") or os.getenv(\"WEAVIATE_URL\", \"http://localhost:8080\")\n",
    "\n",
    "#     # --- UI ---\n",
    "#     with gr.Blocks(title=\"OXCART RAG - Consultas FilatÃ©licas\") as interface:\n",
    "#         gr.Markdown(\n",
    "#             \"# ðŸ” OXCART RAG - Sistema de Consultas FilatÃ©licas\\n\\n\"\n",
    "#             \"Realiza consultas inteligentes sobre tu colecciÃ³n de documentos filatÃ©licos \"\n",
    "#             \"usando bÃºsqueda semÃ¡ntica y respuestas generadas por IA.\"\n",
    "#         )\n",
    "\n",
    "#         with gr.Row():\n",
    "#             with gr.Column(scale=2):\n",
    "#                 # Input principal\n",
    "#                 query_input = gr.Textbox(\n",
    "#                     label=\"ðŸ’­ Tu consulta filatÃ©lica\",\n",
    "#                     placeholder=\"Ej: Â¿QuÃ© sellos de EspaÃ±a de 1950 estÃ¡n catalogados como Scott?\",\n",
    "#                     lines=2,\n",
    "#                 )\n",
    "\n",
    "#                 # BotÃ³n de bÃºsqueda\n",
    "#                 search_btn = gr.Button(\"ðŸ” Buscar y Responder\", variant=\"primary\")\n",
    "\n",
    "#                 # Consultas de ejemplo\n",
    "#                 gr.Markdown(\"**ðŸ’¡ Consultas de ejemplo:**\")\n",
    "#                 example_queries = [\n",
    "#                     \"Â¿QuÃ© sellos conmemorativos de EspaÃ±a estÃ¡n en la colecciÃ³n?\",\n",
    "#                     \"MuÃ©strame informaciÃ³n sobre sellos con errores de perforaciÃ³n\",\n",
    "#                     \"Â¿CuÃ¡les son los sellos mÃ¡s valiosos segÃºn el catÃ¡logo Michel?\",\n",
    "#                     \"InformaciÃ³n sobre sellos de MÃ©xico de la dÃ©cada de 1960\",\n",
    "#                     \"Â¿QuÃ© variedades filatÃ©licas estÃ¡n documentadas?\",\n",
    "#                 ]\n",
    "#                 # Botones que rellenan el textbox\n",
    "#                 for example in example_queries:\n",
    "#                     gr.Button(example, variant=\"secondary\").click(\n",
    "#                         fn=(lambda ex=example: ex),\n",
    "#                         inputs=None,\n",
    "#                         outputs=query_input,\n",
    "#                     )\n",
    "\n",
    "#             with gr.Column(scale=1):\n",
    "#                 # Filtros avanzados\n",
    "#                 gr.Markdown(\"**ðŸŽ¯ Filtros Avanzados**\")\n",
    "\n",
    "#                 use_filters = gr.Checkbox(label=\"Usar filtros especÃ­ficos\", value=False)\n",
    "\n",
    "#                 catalog_system = gr.Dropdown(\n",
    "#                     choices=[\"\", \"Scott\", \"Michel\", \"Yvert\", \"Stanley Gibbons\", \"Edifil\"],\n",
    "#                     label=\"Sistema de catÃ¡logo\",\n",
    "#                     value=\"\",\n",
    "#                 )\n",
    "\n",
    "#                 chunk_type = gr.Dropdown(\n",
    "#                     choices=[\"\", \"text\", \"table\", \"figure\", \"title\", \"header\"],\n",
    "#                     label=\"Tipo de contenido\",\n",
    "#                     value=\"\",\n",
    "#                 )\n",
    "\n",
    "#                 has_varieties = gr.Checkbox(label=\"Solo documentos con variedades\", value=False)\n",
    "\n",
    "#                 max_results = gr.Slider(\n",
    "#                     minimum=1,\n",
    "#                     maximum=100,\n",
    "#                     value=5,\n",
    "#                     step=1,\n",
    "#                     label=\"MÃ¡ximo resultados\",\n",
    "#                 )\n",
    "\n",
    "#         # Outputs\n",
    "#         with gr.Row():\n",
    "#             with gr.Column():\n",
    "#                 gr.Markdown(\"## ðŸ¤– Respuesta IA\")\n",
    "#                 answer_output = gr.Textbox(label=\"Respuesta generada\", lines=8, interactive=False)\n",
    "\n",
    "#         with gr.Row():\n",
    "#             with gr.Column():\n",
    "#                 gr.Markdown(\"## ðŸ“„ Documentos Encontrados\")\n",
    "#                 search_output = gr.Textbox(label=\"Resultados de bÃºsqueda\", lines=12, interactive=False)\n",
    "\n",
    "#             with gr.Column():\n",
    "#                 gr.Markdown(\"## ðŸ“Š Metadatos\")\n",
    "#                 metadata_output = gr.Textbox(label=\"InformaciÃ³n de la consulta\", lines=10, interactive=False)\n",
    "\n",
    "#         # Eventos\n",
    "#         search_btn.click(\n",
    "#             fn=gradio_search_and_answer,\n",
    "#             inputs=[query_input, use_filters, catalog_system, chunk_type, has_varieties, max_results],\n",
    "#             outputs=[answer_output, search_output, metadata_output],\n",
    "#         )\n",
    "\n",
    "#         query_input.submit(\n",
    "#             fn=gradio_search_and_answer,\n",
    "#             inputs=[query_input, use_filters, catalog_system, chunk_type, has_varieties, max_results],\n",
    "#             outputs=[answer_output, search_output, metadata_output],\n",
    "#         )\n",
    "\n",
    "#         # InformaciÃ³n del sistema\n",
    "#         gr.Markdown(\n",
    "#             \"---\\n\"\n",
    "#             f\"**ðŸ“Š Estado del Sistema:**\\n\"\n",
    "#             f\"â€¢ ColecciÃ³n: {collection_name}\\n\"\n",
    "#             f\"â€¢ Documentos indexados: {total_docs:,}\\n\"\n",
    "#             f\"â€¢ Weaviate URL: {weaviate_url}\\n\"\n",
    "#             \"â€¢ Estado: âœ… Operativo\\n\"\n",
    "#         )\n",
    "\n",
    "#     return interface\n",
    "\n",
    "\n",
    "# # ---- Lanzador robusto con manejo de errores de tÃºnel pÃºblico ----\n",
    "# if rag_system and rag_system.get(\"success\", False):\n",
    "#     print(\"\\n\" + \"=\" * 60)\n",
    "#     print(\"ðŸš€ LANZANDO INTERFAZ GRADIO (CON MANEJO DE ERRORES)\")\n",
    "#     print(\"=\" * 60)\n",
    "\n",
    "#     gradio_app = create_gradio_interface(rag_system)\n",
    "\n",
    "#     GRADIO_PORT = int(os.getenv(\"GRADIO_PORT\", 7860))\n",
    "#     GRADIO_SHARE = os.getenv(\"GRADIO_SHARE\", \"false\").lower() == \"true\"  # Por defecto False por problemas de conectividad\n",
    "\n",
    "#     print(f\"âš™ï¸ Puerto: {GRADIO_PORT}\")\n",
    "#     print(f\"ðŸŒ URL PÃºblica: {'âš ï¸ Intentando...' if GRADIO_SHARE else 'âŒ Deshabilitada (mÃ¡s seguro)'}\")\n",
    "    \n",
    "#     try:\n",
    "#         print(\"ðŸ”„ Iniciando servidor Gradio...\")\n",
    "        \n",
    "#         # Intentar con tÃºnel pÃºblico primero si estÃ¡ habilitado\n",
    "#         if GRADIO_SHARE:\n",
    "#             print(\"â³ Intentando crear tÃºnel pÃºblico...\")\n",
    "#             try:\n",
    "#                 demo = gradio_app.launch(\n",
    "#                     server_port=GRADIO_PORT,\n",
    "#                     share=True,\n",
    "#                     inbrowser=False,\n",
    "#                     show_error=True,\n",
    "#                     prevent_thread_lock=False,\n",
    "#                     quiet=False\n",
    "#                 )\n",
    "                \n",
    "#                 print(\"\\nðŸŽ‰ Â¡Ã‰XITO! TÃºnel pÃºblico creado\")\n",
    "#                 print(f\"ðŸŒ URLs DISPONIBLES:\")\n",
    "#                 print(f\"   ðŸ“± Local: http://localhost:{GRADIO_PORT}\")\n",
    "                \n",
    "#                 if hasattr(demo, 'share_url') and demo.share_url:\n",
    "#                     print(f\"   ðŸŒ PÃºblica: {demo.share_url}\")\n",
    "#                     print(f\"\\nðŸ”— **URL PÃšBLICA:** {demo.share_url}\")\n",
    "#                 else:\n",
    "#                     print(f\"   ðŸŒ PÃºblica: Revisa la salida de Gradio arriba â˜ï¸\")\n",
    "                \n",
    "#             except Exception as share_error:\n",
    "#                 print(f\"âš ï¸ Error creando tÃºnel pÃºblico: {share_error}\")\n",
    "#                 print(\"ðŸ”„ Cambiando a modo local solamente...\")\n",
    "                \n",
    "#                 # Fallback: solo local\n",
    "#                 demo = gradio_app.launch(\n",
    "#                     server_port=GRADIO_PORT,\n",
    "#                     share=False,\n",
    "#                     inbrowser=True,\n",
    "#                     show_error=True,\n",
    "#                     prevent_thread_lock=False\n",
    "#                 )\n",
    "                \n",
    "#                 print(f\"\\nâœ… SERVIDOR LOCAL OPERATIVO:\")\n",
    "#                 print(f\"   ðŸ“± URL Local: http://localhost:{GRADIO_PORT}\")\n",
    "#                 print(f\"   âš ï¸ URL PÃºblica: No disponible (error en tÃºnel)\")\n",
    "                \n",
    "#         else:\n",
    "#             # Solo modo local\n",
    "#             demo = gradio_app.launch(\n",
    "#                 server_port=GRADIO_PORT,\n",
    "#                 share=False,\n",
    "#                 inbrowser=True,\n",
    "#                 show_error=True,\n",
    "#                 prevent_thread_lock=False\n",
    "#             )\n",
    "            \n",
    "#             print(f\"\\nâœ… SERVIDOR LOCAL OPERATIVO:\")\n",
    "#             print(f\"   ðŸ“± URL Local: http://localhost:{GRADIO_PORT}\")\n",
    "#             print(f\"   ðŸ’¡ Para URL pÃºblica, cambia GRADIO_SHARE=true en .env\")\n",
    "        \n",
    "#         print(f\"\\nðŸ“‹ INSTRUCCIONES:\")\n",
    "#         print(f\"   â€¢ La interfaz estÃ¡ operativa y lista para consultas\")\n",
    "#         print(f\"   â€¢ Para detenerla: gr.close_all()\")\n",
    "#         print(f\"   â€¢ Comparte la URL local en tu red si necesitas acceso remoto\")\n",
    "        \n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"ðŸŽ¯ INTERFAZ RAG LISTA - Â¡Comienza a hacer consultas!\")\n",
    "#         print(f\"{'='*60}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error crÃ­tico lanzando Gradio: {e}\")\n",
    "#         print(\"\\nðŸ”§ SOLUCIONES SUGERIDAS:\")\n",
    "#         print(\"   1. Ejecuta: gr.close_all()\")\n",
    "#         print(\"   2. Cambia el puerto: GRADIO_PORT=7861 en .env\")\n",
    "#         print(\"   3. Verifica que no hay otros servicios en el puerto\")\n",
    "#         print(\"   4. Reinicia el notebook\")\n",
    "        \n",
    "# else:\n",
    "#     print(\"\\nâš ï¸  No se puede crear la interfaz Gradio:\")\n",
    "#     if not rag_system:\n",
    "#         print(\"   â€¢ Sistema RAG no estÃ¡ configurado\")\n",
    "#     else:\n",
    "#         print(f\"   â€¢ Error en RAG: {rag_system.get('error', 'Error desconocido')}\")\n",
    "#     print(\"\\nðŸ”§ Para solucionar:\")\n",
    "#     print(\"   1. Verifica que Weaviate estÃ© corriendo\")\n",
    "#     print(\"   2. Configura OPENAI_API_KEY en .env\") \n",
    "#     print(\"   3. Ejecuta la indexaciÃ³n de documentos\")\n",
    "#     print(\"   4. Reinicia este notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cerrar instancias anteriores de Gradio si existen\n",
    "# import gradio as gr\n",
    "# gr.close_all()\n",
    "# print(\"ðŸ”„ Cerrando instancias anteriores de Gradio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
