"""
Philatelic Chunk Logic - Advanced Bilingual Processing Module

This module provides comprehensive bilingual (English/Spanish) philatelic pattern
matching and chunk enrichment for OXCART RAG system. Based on extensive research
of international catalog systems, EFO terminology, and Costa Rican philatelic context.

VERSION: 2.0 - Bilingual Advanced Philatelic Logic
AUTHOR: Generated by Claude Code for OXCART RAG Project  
LAST UPDATE: 2025-08-26

Key Features:
- Bilingual pattern matching (English/Spanish)
- International catalog systems (Scott, Michel, Yvert, Gibbons, Zumstein)
- Costa Rica specific patterns (Guanacaste overprints, GR/CR numbers)
- EFO classification (Errors, Freaks, Oddities) in both languages
- Technical specifications (perforation, paper, printing, gum)
- Condition assessment with standardized terminology
- Semantic enrichment for improved RAG performance
"""

import re
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
from datetime import datetime


# ============================================================================
# SECURE PHILATELIC PATTERNS - ENHANCED WITH CONTEXTUAL ANCHORS
# ============================================================================

class SecurePhilatelicPatterns:
    """
    Enhanced philatelic patterns with contextual anchors to prevent false positives.
    
    This class implements a security-focused approach using:
    - Mandatory philatelic context anchors
    - Negative context detection (bio-informatics, software, etc.)
    - Confidence scoring based on context validation
    - Extended context windows for better accuracy
    """
    
    # Global philatelic anchor - must appear within context window
    PHILATELIC_ANCHOR = re.compile(
        r'\b(?:'
        r'stamp|stamps|philatel|postage|postal|mail|envelope|cover|'
        r'perforation|perforat|overprint|watermark|gum|mint|used|'
        r'catalog|catalogue|scott|michel|yvert|gibbons|zumstein|'
        r'variety|varieties|error|freak|oddity|E\.?F\.?O\.?|'
        r'cent(?:avo)?s?|colon(?:es)?|peso|pesos|real|reales|'
        r'denomination|face\s+value|printing|lithograph|engrav|'
        r'sello|sellos|estampilla|filatelia|correo|postal|sobre|'
        r'dentado|sobrecarga|marca\s+de\s+agua|goma|nuevo|usado|'
        r'catálogo|variedad|variedades|error|rareza|'
        r'céntimos?|centavos?|colones?|denominación|valor\s+facial|'
        r'impresión|litografía|grabado|plancha|pliego|hoja'
        r')\b',
        re.IGNORECASE | re.VERBOSE
    )
    
    # Negative context patterns - exclude these domains
    NEGATIVE_CONTEXT = re.compile(
        r'\b(?:'
        r'software|database|ontology|bioinformatic|arrayexpress|ensembl|'
        r'experimental\s+factor\s+ontology|gene\s+ontology|'
        r'computer|digital|electronic|technical\s+manual|'
        r'specification|standard|protocol|API|JSON|XML|'
        r'programming|algorithm|function|method|class|'
        r'servidor|base\s+de\s+datos|ontología|bioinformática|'
        r'computadora|digital|electrónico|manual\s+técnico|'
        r'especificación|estándar|protocolo|programación'
        r')\b',
        re.IGNORECASE | re.VERBOSE
    )
    
    # Secure catalog patterns with mandatory anchors - Only 4 Major International Systems
    SECURE_CATALOG_PATTERNS = {
        'Scott': re.compile(
            r'\b(?:Scott|Sc\.?)(?:\'s)?\s*(?:Nos?\.?|No\.?|#|Cat\.?|Catalog)?\s*([A-Z]?\d{1,4}[A-Z]?[a-z]?(?:[-–—]\d{1,4}[A-Z]?[a-z]?)?(?:(?:\s*[,;&]\s*)?[A-Z]?\d{1,4}[A-Z]?[a-z]?(?:[-–—]\d{1,4}[A-Z]?[a-z]?)?)*)',
            re.IGNORECASE
        ),
        
        'Michel': re.compile(
            r'\b(?:Michel|Mi\.?)\s*(?:Nos?\.?|No\.?|Nr\.?|#|Cat\.?|Katalog)?\s*([A-Z]?\d{1,4}[A-Z]?[a-z]?(?:(?:[-–—]|bis)\d{1,4}[A-Z]?[a-z]?)?(?:(?:\s*[,;&]\s*)?[A-Z]?\d{1,4}[A-Z]?[a-z]?(?:(?:[-–—]|bis)\d{1,4}[A-Z]?[a-z]?)?)*)',
            re.IGNORECASE
        ),
        
        'Yvert': re.compile(
            r'\b(?:Yvert(?:\s+et\s+Tellier)?|Y&T|YT)\b\s*(?:Nos?\.?|No\.?|#)?\s*([A-Z]?\d{1,4}[A-Z]?[a-z]?(?:(?:[-–—]|à)\d{1,4}[A-Z]?[a-z]?)?(?:(?:\s*[,;&]\s*)?[A-Z]?\d{1,4}[A-Z]?[a-z]?(?:(?:[-–—]|à)\d{1,4}[A-Z]?[a-z]?)?)*)',
            re.IGNORECASE
        ),
        
        'Stanley_Gibbons': re.compile(
            r'\b(?:Stanley\s+Gibbons|SG)\b\s*(?:Nos?\.?|No\.?|#)?\s*([A-Z]?\d{1,4}[A-Z]?[a-z]?(?:[-–—]\d{1,4}[A-Z]?[a-z]?)?(?:(?:\s*[,;&]\s*)?[A-Z]?\d{1,4}[A-Z]?[a-z]?(?:[-–—]\d{1,4}[A-Z]?[a-z]?)?)*)',
            re.IGNORECASE
        )
    }
    
    # Secure EFO patterns with required context validation
    SECURE_EFO_PATTERNS = {
        'explicit_efo': re.compile(
            r'\b(?:'
            r'E\.?F\.?O\.?s?|'
            r'errors?,?\s*freaks?\s*(?:and|&|y)\s*oddities?|'
            r'errores?,?\s*freaks?\s*(?:y|o)\s*(?:rareza|rarezas|oddities?)'
            r')\b',
            re.IGNORECASE | re.VERBOSE
        ),
        
        'error_patterns': [
            (re.compile(r'\b(?:centro\s+invertido|inverted\s+cent(?:er|re))\b', re.IGNORECASE),
             'inverted_center', 'centro invertido', 0.95),
            (re.compile(r'\b(?:sobrecarga\s+invertida|inverted\s+overprint)\b', re.IGNORECASE),
             'inverted_overprint', 'sobrecarga invertida', 0.90),
            (re.compile(r'\b(?:color\s+(?:omitido|faltante)|missing\s+colou?r)\b', re.IGNORECASE),
             'missing_color', 'color omitido', 0.85),
            (re.compile(r'\b(?:doble\s+(?:impresión|sobrecarga)|double\s+(?:impression|overprint))\b', re.IGNORECASE),
             'double_print', 'doble impresión', 0.80),
            (re.compile(r'\b(?:sin\s+dentar|imperforate|imperf)\b', re.IGNORECASE),
             'imperforate', 'sin dentar', 0.85)
        ],
        
        'freak_patterns': [
            (re.compile(r'\b(?:perforación\s+desplazada|misperforat|mis[-\s]?perf)\b', re.IGNORECASE),
             'misperforation', 'perforación desplazada', 0.80),
            (re.compile(r'\b(?:pliegue\s+de\s+papel|paper\s+fold)\b', re.IGNORECASE),
             'paper_fold', 'pliegue de papel', 0.75),
            (re.compile(r'\b(?:objeto\s+extra[nñ]o|foreign\s+object)\b', re.IGNORECASE),
             'foreign_object', 'objeto extraño', 0.70)
        ],
        
        'oddity_patterns': [
            (re.compile(r'\b(?:oddity|oddities|rareza|rarezas|curiosidad(?:es)?)\b', re.IGNORECASE),
             'general_oddity', 'rareza general', 0.60)
        ]
    }
    
    def get_catalog_patterns(self):
        """Return list of (name, pattern) tuples for catalog systems."""
        return list(self.SECURE_CATALOG_PATTERNS.items())
    
    def get_efo_patterns(self):
        """Return dictionary of EFO pattern categories."""
        return self.SECURE_EFO_PATTERNS


class ContextValidator:
    """
    Validates pattern matches using contextual analysis to prevent false positives.
    
    Uses a window-based approach to check for:
    - Required philatelic context anchors
    - Exclusionary negative contexts  
    - Confidence scoring based on context strength
    """
    
    def __init__(self, window_size: int = 150):
        self.window_size = window_size
        self.patterns = SecurePhilatelicPatterns()
    
    def validate_pattern_match(self, text: str, match_span: Tuple[int, int], 
                              pattern_type: str) -> Dict[str, Any]:
        """
        Validate a pattern match using contextual analysis.
        
        Args:
            text: Full text being analyzed
            match_span: (start, end) positions of the match
            pattern_type: Type of pattern ('catalog', 'efo', 'technical', etc.)
            
        Returns:
            Dictionary with validation results and confidence score
        """
        start, end = match_span
        context = self._extract_context(text, start, end)
        
        # Check for required philatelic anchor
        has_anchor = bool(self.patterns.PHILATELIC_ANCHOR.search(context))
        
        # Check for exclusionary negative context
        has_negative = bool(self.patterns.NEGATIVE_CONTEXT.search(context))
        
        # Count philatelic indicators for strength assessment
        anchor_matches = len(self.patterns.PHILATELIC_ANCHOR.findall(context))
        
        # Calculate confidence score
        confidence_score = self._calculate_confidence_score(
            context, pattern_type, has_anchor, has_negative, anchor_matches
        )
        
        return {
            'confidence_score': confidence_score,
            'context_valid': has_anchor and not has_negative,
            'has_philatelic_anchor': has_anchor,
            'has_negative_context': has_negative,
            'anchor_strength': anchor_matches,
            'pattern_type': pattern_type,
            'context_snippet': self._truncate_context(context, 100),
            'match_text': text[start:end]
        }
    
    def _extract_context(self, text: str, start: int, end: int) -> str:
        """Extract context window around the match."""
        context_start = max(0, start - self.window_size)
        context_end = min(len(text), end + self.window_size)
        return text[context_start:context_end]
    
    def _calculate_confidence_score(self, context: str, pattern_type: str, 
                                   has_anchor: bool, has_negative: bool, 
                                   anchor_count: int) -> float:
        """
        Calculate confidence score based on contextual factors.
        
        Scoring logic:
        - Base score: 0.4
        - Philatelic anchor present: +0.3
        - Multiple anchors: +0.1 per additional anchor (max +0.2)
        - Negative context: -0.5
        - Pattern-specific bonuses: +0.1 to +0.2
        """
        base_score = 0.4
        
        # Anchor presence bonus
        if has_anchor:
            base_score += 0.3
            
            # Multiple anchor bonus (diminishing returns)
            extra_anchors = min(anchor_count - 1, 2)
            base_score += extra_anchors * 0.1
        
        # Negative context penalty
        if has_negative:
            base_score -= 0.5
        
        # Pattern-specific bonuses
        if pattern_type == 'catalog':
            if re.search(r'\b(?:Scott|Michel|Yvert|Gibbons)\b.*\d+', context, re.IGNORECASE):
                base_score += 0.2
            elif re.search(r'\bcatalog|catalogue|catálogo\b', context, re.IGNORECASE):
                base_score += 0.1
        
        elif pattern_type == 'efo':
            if re.search(r'\b(?:variety|varieties|error|freak|oddity|variedad|rareza)\b', context, re.IGNORECASE):
                base_score += 0.2
            elif re.search(r'\b(?:inverted|missing|double|invertido|omitido|doble)\b', context, re.IGNORECASE):
                base_score += 0.1
        
        elif pattern_type == 'technical':
            if re.search(r'\b(?:perforation|watermark|gum|paper|printing|dentado|goma|papel)\b', context, re.IGNORECASE):
                base_score += 0.15
        
        # Clamp score to valid range
        return min(max(base_score, 0.0), 1.0)
    
    def _truncate_context(self, context: str, max_length: int) -> str:
        """Truncate context for display purposes."""
        if len(context) <= max_length:
            return context
        return context[:max_length-3] + '...'


# ============================================================================
# BILINGUAL PATTERNS CLASS - INTERNATIONAL CATALOG SYSTEMS (LEGACY)
# ============================================================================

class BilingualPatterns:
    """
    Comprehensive bilingual patterns based on international philatelic standards.
    
    Research sources:
    - Scott Publishing Company (USA) - North American standard
    - Stanley Gibbons (UK) - Commonwealth standard  
    - Michel Katalog (Germany) - German-speaking regions
    - Yvert & Tellier (France) - Francophone regions
    - Costa Rica specialized catalogs
    """
    
    # International Catalog Systems - Only Major 4 Systems (Legacy compatibility)
    CATALOG_PATTERNS = {
        'Scott': [
            r'Scott\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Sc\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)', 
            r'Scott\s+No\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            # Airmail and special prefixes
            r'Scott\s+([C]\d+[a-z]?(?:-\d+[a-z]?)?)',  # C for airmail
            r'Scott\s+([J]\d+[a-z]?(?:-\d+[a-z]?)?)',  # J for postage due
            r'Scott\s+([O]\d+[a-z]?(?:-\d+[a-z]?)?)'   # O for official
        ],
        'Michel': [
            r'Michel\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Mich\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Mi\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Michel\s+Nr\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)'  # German format
        ],
        'Yvert': [
            r'Yvert\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Yv\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Y&T\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'YT\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Yvert\s+et\s+Tellier\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)'  # Full name
        ],
        'Stanley_Gibbons': [
            r'Gibbons\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'SG\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'S\.G\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Stanley\s+Gibbons\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)',
            r'Gibbons\s+No\.?\s+([A-Z]?\d+[a-z]?(?:-\d+[a-z]?)?)'
        ]
    }
    
    # EFO Patterns - Bilingual (English/Spanish) 
    EFO_PATTERNS = {
        'overprint_errors': [
            # Inverted overprints
            (r'(?:sobrecarga\s+(?:\w+\s+)?invertida|invertida\s+sobrecarga|inverted\s+overprint|overprint\s+inverted)', 
             'inverted', 'sobrecarga invertida', 0.9),
            # Double overprints  
            (r'(?:sobrecarga\s+doble|double\s+overprint|overprint\s+double)',
             'double', 'sobrecarga doble', 0.85),
            # Missing overprints
            (r'(?:sobrecarga\s+(?:omitida|faltante)|(?:missing|omitted)\s+overprint|overprint\s+(?:missing|omitted))',
             'missing', 'sobrecarga omitida', 0.8),
            # Shifted overprints  
            (r'(?:sobrecarga\s+desplazada|(?:shifted|displaced)\s+overprint|overprint\s+(?:shifted|displaced))',
             'shifted', 'sobrecarga desplazada', 0.75)
        ],
        'color_errors': [
            # Wrong color
            (r'(?:error\s+de\s+color|color\s+(?:error|incorrecto)|wrong\s+colou?r|colou?r\s+error)',
             'wrong_color', 'error de color', 0.8),
            # Missing color  
            (r'(?:color\s+(?:omitido|faltante)|(?:missing|omitted)\s+colou?r|colou?r\s+(?:missing|omitted))',
             'missing_color', 'color omitido', 0.85),
            # Color shift
            (r'(?:desplazamiento\s+de\s+colou?r|colou?r\s+shift|registro\s+incorrecto)',
             'color_shift', 'desplazamiento de color', 0.75),
            # Inverted colors
            (r'(?:colou?res?\s+invertidos?|inverted\s+colou?rs?)',
             'inverted', 'colores invertidos', 0.8)
        ],
        'center_errors': [
            # Inverted center (classic error)
            (r'(?:centro\s+invertido|inverted\s+cent(?:er|re)|cent(?:er|re)\s+inverted)',
             'inverted', 'centro invertido', 0.95)
        ],
        'mirror_print': [
            (r'(?:impresión\s+espejo|mirror\s+print)', 'mirror', 'impresión espejo', 0.8),
            (r'(?:impresión\s+(?:invertida|reversa)|reversed?\s+print)', 'reversed', 'impresión invertida', 0.75)
        ]
    }
    
    # Technical Specifications - Bilingual
    TECHNICAL_PATTERNS = {
        'perforation': {
            'measurements': [
                r'(\d+(?:\.\d+)?)\s*(?:x|\×)\s*(\d+(?:\.\d+)?)',  # 12x11.5
                r'perf\.?\s*(\d+(?:\.\d+)?)',                     # perf 12
                r'dentado\s+(\d+(?:\.\d+)?)',                     # dentado 12
                r'(\d+(?:\.\d+)?)\s*(?:perf|dentado)'             # 12 perf
            ],
            'types': [
                (r'(?:sin\s+dentar|imperforate?|imperf)', 'imperforate'),
                (r'(?:dentado\s+de\s+peine|comb\s+perf)', 'comb perf'),  
                (r'(?:dentado\s+lineal|line\s+perf)', 'line perf'),
                (r'(?:perforación\s+de\s+alfiler|pin\s+perf)', 'pin perf'),
                (r'(?:roulette|ruleteo)', 'roulette'),
                (r'(?:serpentine\s+die\s+cut|corte\s+serpentino)', 'serpentine die cut')
            ]
        },
        'paper': {
            'types': [
                (r'(?:papel\s+(?:verjurado|vergé)|laid\s+paper)', 'laid paper'),
                (r'(?:papel\s+(?:satinado|alisado)|wove\s+paper)', 'wove paper'),  
                (r'(?:papel\s+(?:granito|granítico)|granite\s+paper)', 'granite paper'),
                (r'(?:papel\s+manila|manila\s+paper)', 'manila paper'),
                (r'(?:papel\s+de\s+seguridad|safety\s+paper)', 'safety paper'),
                (r'(?:papel\s+pelure|pelure\s+paper)', 'pelure paper')
            ],
            'thickness': [
                (r'(?:papel\s+)?(?:grueso|thick)', 'thick'),
                (r'(?:papel\s+)?(?:delgado|fino|thin)', 'thin'),  
                (r'(?:papel\s+)?(?:mediano|medium)', 'medium')
            ]
        },
        'printing': [
            (r'(?:litografía|litografiado|lithograph(?:y|ed))', 'lithography'),
            (r'(?:calcografía|grabado|engrav(?:ed|ing)|intaglio)', 'engraved'),
            (r'(?:offset|huecograbado)', 'offset'),
            (r'(?:fotograbado|photogravure)', 'photogravure'),
            (r'(?:tipografía|typography|letterpress)', 'typography')
        ],
        'watermark': {
            'types': [
                r'(?:filigrana|watermark)\s+([a-zA-Z\s]+)',
                r'(?:wmk|w\.m\.)\s+([a-zA-Z\s]+)',
                r'marca\s+de\s+agua\s+([a-zA-Z\s]+)'
            ],
            'positions': [
                (r'(?:filigrana\s+)?(?:invertida|inverted\s+watermark)', 'inverted'),
                (r'(?:filigrana\s+)?(?:lateral|sideways\s+watermark)', 'sideways'),
                (r'(?:filigrana\s+)?(?:normal|normal\s+watermark)', 'normal')
            ]
        },
        'gum': [
            (r'(?:goma\s+original|original\s+gum)', 'original gum'),
            (r'(?:goma\s+tropical|tropical\s+gum)', 'tropical gum'),
            (r'(?:goma\s+blanca|white\s+gum)', 'white gum'),
            (r'(?:goma\s+amarilla|yellow\s+gum)', 'yellow gum'),  
            (r'(?:sin\s+goma|no\s+gum)', 'no gum'),
            (r'(?:regomado|regummed)', 'regummed')
        ]
    }
    
    # Condition Assessment - Bilingual
    CONDITION_PATTERNS = {
        'mint_status': [
            (r'(?:nuevo\s+sin\s+(?:charnela|fijasellos)|mint\s+never\s+hinged|MNH)', 'mint never hinged'),
            (r'(?:nuevo\s+con\s+charnela\s+ligera|mint\s+lightly\s+hinged|MLH)', 'mint lightly hinged'),
            (r'(?:nuevo\s+con\s+charnela|mint\s+hinged|MH)', 'mint hinged'),
            (r'(?:nuevo\s+sin\s+goma|mint\s+no\s+gum|MNG)', 'mint no gum')
        ],
        'used_status': [
            (r'(?:usado\s+(?:postalmente|por\s+correo)|postally\s+used)', 'postally used'),
            (r'(?:matasellos?\s+de\s+(?:favor|complacencia)|cancelled?\s+to\s+order|CTO)', 'cancelled to order'),
            (r'(?:matasellos?\s+(?:de\s+)?primer\s+día|first\s+day\s+cancel)', 'first day cancel'),
            (r'(?:usado\s+comercialmente|commercially\s+used)', 'commercially used'),
            (r'(?:usado|matasellado|used)', 'used')
        ],
        'centering': [
            (r'(?:perfectamente\s+centrado|perfectly\s+cent(?:er|re)d)', 'perfectly centered'),
            (r'(?:soberbio|superb)', 'superb'),
            (r'(?:extremadamente\s+(?:fino|bueno)|extremely\s+fine|XF)', 'extremely fine'),
            (r'(?:muy\s+(?:fino|bueno)|very\s+fine|VF)', 'very fine'),
            (r'(?:fino|bueno|fine|F(?![A-Z]))', 'fine'),
            (r'(?:muy\s+(?:regular|bueno)|very\s+good|VG)', 'very good'),
            (r'(?:regular|bueno|good|G(?![A-Z]))', 'good'),
            (r'(?:malo|pobre|poor)', 'poor')
        ],
        'defects': [
            r'(?:adelgazamiento|thin(?:\s+spot)?)',
            r'(?:desgarro|tear)',
            r'(?:arruga|doblez|crease)', 
            r'(?:mancha|stain)',
            r'(?:descolorido|fade?d?)',
            r'(?:oxidación|oxidation)',
            r'(?:foxing|manchas\s+de\s+humedad)',
            r'(?:mancha\s+de\s+goma|gum\s+stain)',
            r'(?:falla\s+en\s+esquina|corner\s+fault)',
            r'(?:dentado\s+corto|short\s+perf)',
            r'(?:dentado\s+cortado|clipped\s+perf)'
        ]
    }
    
    # Costa Rica Context Patterns (Enhanced with research)
    COSTA_RICA_PATTERNS = {
        'guanacaste_period': [
            r'(?:período\s+)?Guanacaste\s+(?:1885-1891|overprint)',
            r'sobreimpresiones?\s+(?:de\s+)?Guanacaste',
            r'Guanacaste\s+(?:overprints?|sobrecargas?)',
            r'decreto\s+(?:no\.?\s+)?CXIX\s+(?:de\s+)?1885'  # Historical decree
        ],
        'historical_periods': [
            (r'(?:período\s+)?colonial', 'colonial'),
            (r'(?:período\s+de\s+)?independencia', 'independence'),
            (r'primera\s+república', 'early_republic'),
            (r'sobreimpresiones?\s+(?:de\s+)?Guanacaste', 'guanacaste_1885_1891')
        ],
        'personalities': [
            r'Jesús\s+Jiménez',
            r'Juan\s+Mora\s+Porras',
            r'José\s+María\s+Castro\s+Madriz',
            r'Braulio\s+Carrillo',
            r'Tomás\s+Guardia'
        ],
        'geography': [
            r'Volcán\s+Arenal',
            r'Puerto\s+Limón',
            r'Golfo\s+(?:de\s+)?Nicoya',
            r'Cartago',
            r'Puntarenas',
            r'Alajuela',
            r'Heredia',
            r'San\s+José'
        ]
    }
    
    # Price Patterns - Multi-currency
    PRICE_PATTERNS = [
        (r'\$([0-9,]+(?:\.[0-9]{2})?)', 'USD', '$'),
        (r'USD\s*([0-9,]+(?:\.[0-9]{2})?)', 'USD', 'USD '),
        (r'₡([0-9,]+(?:\.[0-9]{2})?)', 'CRC', '₡'),
        (r'colones?\s+([0-9,]+(?:\.[0-9]{2})?)', 'CRC', 'colones '),
        (r'([0-9,]+(?:\.[0-9]{2})?).\s*(?:cent(?:avos?|imos?)|céntimos?)', 'CENT', ' centavos'),
        (r'([0-9,]+(?:\.[0-9]{2})?).\s*c(?:ts?)?.?(?!\w)', 'CENT', 'c'),
        (r'([0-9,]+).\s*\.-', 'USD', '.-'),  # Auction format
        (r'€([0-9,]+(?:\.[0-9]{2})?)', 'EUR', '€'),  # Euro support
        (r'£([0-9,]+(?:\.[0-9]{2})?)', 'GBP', '£')   # British pound
    ]
    
    # Date Patterns - Multiple formats
    DATE_PATTERNS = [
        r'(\d{4}-\d{2}-\d{2})',      # ISO format YYYY-MM-DD
        r'(\d{1,2}/\d{1,2}/\d{4})',  # MM/DD/YYYY or DD/MM/YYYY
        r'(\d{1,2}-\d{1,2}-\d{4})',  # MM-DD-YYYY or DD-MM-YYYY  
        r'(\d{4})',                  # Just year
        r'(\d{1,2}\s+(?:de\s+)?(?:enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|octubre|noviembre|diciembre)\s+(?:de\s+)?\d{4})',  # Spanish dates
        r'(\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4})'  # English dates
    ]
    
    # Design and Theme Patterns - Bilingual
    DESIGN_PATTERNS = [
        r'(?:escudo\s+(?:de\s+)?armas|coat\s+of\s+arms)',
        r'(?:catedral|cathedral)',
        r'(?:iglesia|church)',
        r'(?:volcán|volcano)',
        r'(?:montaña|mountain)',
        r'(?:café|coffee)',
        r'(?:plátano|banana)',
        r'(?:ave|pájaro|bird)',
        r'(?:mariposa|butterfly)', 
        r'(?:flor|flower)',
        r'(?:árbol|tree)',
        r'(?:presidente|president)',
        r'(?:héroe|hero)',
        r'(?:bandera|flag)',
        r'(?:mapa|map)',
        r'(?:ferrocarril|tren|railway|train)',
        r'(?:barco|ship|vessel)',
        r'(?:avión|airplane|aircraft)'
    ]


# ============================================================================
# SECURE CATALOG EXTRACTOR - WITH CONTEXTUAL VALIDATION
# ============================================================================

class SecureCatalogExtractor:
    """
    Enhanced catalog extractor with contextual validation and confidence scoring.
    
    This extractor uses secure patterns with mandatory anchors and validates
    matches against philatelic context to prevent false positives.
    """
    
    def __init__(self, min_confidence: float = 0.8):
        self.secure_patterns = SecurePhilatelicPatterns()
        self.validator = ContextValidator()
        self.min_confidence = min_confidence
    
    def extract_catalog_numbers_secure(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract catalog numbers with security validation and confidence scoring.
        
        Args:
            text: Input text to analyze
            
        Returns:
            List of validated catalog entries with confidence scores
        """
        results = []
        
        # Process secure catalog patterns
        for system_name, pattern in self.secure_patterns.get_catalog_patterns():
            for match in pattern.finditer(text):
                # Validate match using context
                validation = self.validator.validate_pattern_match(
                    text, match.span(), 'catalog'
                )
                
                # Only accept high-confidence matches
                if validation['confidence_score'] >= self.min_confidence:
                    catalog_entry = {
                        'system': system_name,
                        'number': self._extract_number_from_match(match),
                        'confidence': validation['confidence_score'],
                        'context_valid': validation['context_valid'],
                        'has_anchor': validation['has_philatelic_anchor'],
                        'anchor_strength': validation['anchor_strength'],
                        'span': match.span(),
                        'match_text': validation['match_text'],
                        'context_snippet': validation['context_snippet']
                    }
                    results.append(catalog_entry)
        
        # Deduplicate and sort by confidence
        return self._deduplicate_and_rank_results(results)
    
    def _extract_number_from_match(self, match: re.Match) -> str:
        """Extract catalog number from regex match object."""
        # Use group 1 (first capture group) for the number
        if match.groups() and match.group(1):
            return match.group(1).strip()
        else:
            # Fallback to full match, clean up
            full_match = match.group(0)
            # Remove catalog system name and common prefixes
            number_part = re.sub(r'^.*?(?:Scott|Michel|Yvert|Gibbons|Zumstein)\s*(?:Nos?\.?|#|Cat\.?)?\s*', '', full_match, flags=re.IGNORECASE)
            return number_part.strip()
    
    def _deduplicate_and_rank_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicates and rank by confidence."""
        # Sort by system and number for grouping
        results.sort(key=lambda x: (x['system'], x['number'], -x['confidence']))
        
        # Deduplicate keeping highest confidence
        seen = set()
        unique_results = []
        
        for result in results:
            key = (result['system'], result['number'])
            if key not in seen:
                seen.add(key)
                unique_results.append(result)
        
        # Sort final results by confidence (highest first)
        unique_results.sort(key=lambda x: -x['confidence'])
        return unique_results
    
    def extract_with_fallback(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract catalogs with fallback to legacy patterns if secure fails.
        
        This method provides backward compatibility while preferring secure results.
        """
        secure_results = self.extract_catalog_numbers_secure(text)
        
        if secure_results:
            return secure_results
        
        # Fallback to legacy extractor with lower confidence
        legacy_extractor = CatalogExtractor()
        legacy_results = legacy_extractor.extract_catalog_numbers(text)
        
        # Convert legacy results to secure format with reduced confidence
        fallback_results = []
        for legacy_result in legacy_results:
            fallback_results.append({
                'system': legacy_result['system'],
                'number': legacy_result['number'],
                'confidence': 0.5,  # Lower confidence for legacy matches
                'context_valid': False,
                'has_anchor': False,
                'anchor_strength': 0,
                'span': (0, 0),  # Unknown span for legacy
                'match_text': f"{legacy_result['system']} {legacy_result['number']}",
                'context_snippet': 'Legacy extraction (no context validation)'
            })
        
        return fallback_results


class SecureEFODetector:
    """
    Enhanced EFO detector with mandatory contextual validation.
    
    Uses philatelic anchors to ensure EFO terms are in proper context,
    preventing false positives from bio-informatics and other domains.
    """
    
    def __init__(self, min_confidence: float = 0.6):
        self.secure_patterns = SecurePhilatelicPatterns()
        self.validator = ContextValidator()
        self.min_confidence = min_confidence
    
    def detect_efo_varieties_secure(self, text: str) -> List[Dict[str, Any]]:
        """
        Detect EFO varieties with contextual validation.
        
        Args:
            text: Input text to analyze
            
        Returns:
            List of validated EFO detections with confidence scores
        """
        results = []
        efo_patterns = self.secure_patterns.get_efo_patterns()
        
        # Process explicit EFO mentions
        explicit_pattern = efo_patterns['explicit_efo']
        for match in explicit_pattern.finditer(text):
            validation = self.validator.validate_pattern_match(
                text, match.span(), 'efo'
            )
            
            if validation['confidence_score'] >= self.min_confidence:
                results.append({
                    'efo_class': 'explicit_efo',
                    'subtype': 'general',
                    'label': 'EFO general',
                    'confidence': validation['confidence_score'],
                    'context_valid': validation['context_valid'],
                    'has_anchor': validation['has_philatelic_anchor'],
                    'span': match.span(),
                    'match_text': validation['match_text'],
                    'context_snippet': validation['context_snippet']
                })
        
        # Process specific error patterns
        for pattern, subtype, label, base_confidence in efo_patterns['error_patterns']:
            for match in pattern.finditer(text):
                validation = self.validator.validate_pattern_match(
                    text, match.span(), 'efo'
                )
                
                # Combine base confidence with context validation
                final_confidence = min(base_confidence * validation['confidence_score'], 1.0)
                
                if final_confidence >= self.min_confidence:
                    results.append({
                        'efo_class': 'error',
                        'subtype': subtype,
                        'label': label,
                        'confidence': final_confidence,
                        'context_valid': validation['context_valid'],
                        'has_anchor': validation['has_philatelic_anchor'],
                        'span': match.span(),
                        'match_text': validation['match_text'],
                        'context_snippet': validation['context_snippet']
                    })
        
        # Process freak patterns
        for pattern, subtype, label, base_confidence in efo_patterns['freak_patterns']:
            for match in pattern.finditer(text):
                validation = self.validator.validate_pattern_match(
                    text, match.span(), 'efo'
                )
                
                final_confidence = min(base_confidence * validation['confidence_score'], 1.0)
                
                if final_confidence >= self.min_confidence:
                    results.append({
                        'efo_class': 'freak',
                        'subtype': subtype,
                        'label': label,
                        'confidence': final_confidence,
                        'context_valid': validation['context_valid'],
                        'has_anchor': validation['has_philatelic_anchor'],
                        'span': match.span(),
                        'match_text': validation['match_text'],
                        'context_snippet': validation['context_snippet']
                    })
        
        # Process oddity patterns
        for pattern, subtype, label, base_confidence in efo_patterns['oddity_patterns']:
            for match in pattern.finditer(text):
                validation = self.validator.validate_pattern_match(
                    text, match.span(), 'efo'
                )
                
                final_confidence = min(base_confidence * validation['confidence_score'], 1.0)
                
                if final_confidence >= self.min_confidence:
                    results.append({
                        'efo_class': 'oddity',
                        'subtype': subtype,
                        'label': label,
                        'confidence': final_confidence,
                        'context_valid': validation['context_valid'],
                        'has_anchor': validation['has_philatelic_anchor'],
                        'span': match.span(),
                        'match_text': validation['match_text'],
                        'context_snippet': validation['context_snippet']
                    })
        
        # Remove duplicates and rank by confidence
        return self._deduplicate_efo_results(results)
    
    def _deduplicate_efo_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove overlapping EFO detections, keeping highest confidence."""
        if not results:
            return results
        
        # Sort by span start position
        results.sort(key=lambda x: x['span'][0])
        
        deduplicated = []
        for result in results:
            start, end = result['span']
            
            # Check for overlap with existing results
            overlaps = False
            for existing in deduplicated:
                ex_start, ex_end = existing['span']
                # Check for significant overlap (>50% of either span)
                overlap_start = max(start, ex_start)
                overlap_end = min(end, ex_end)
                overlap_length = max(0, overlap_end - overlap_start)
                
                current_length = end - start
                existing_length = ex_end - ex_start
                
                if (overlap_length > 0.5 * current_length or 
                    overlap_length > 0.5 * existing_length):
                    # Keep higher confidence result
                    if result['confidence'] > existing['confidence']:
                        deduplicated.remove(existing)
                        break
                    else:
                        overlaps = True
                        break
            
            if not overlaps:
                deduplicated.append(result)
        
        # Sort final results by confidence
        deduplicated.sort(key=lambda x: -x['confidence'])
        return deduplicated


# ============================================================================
# CATALOG EXTRACTOR CLASS (LEGACY)
# ============================================================================

class CatalogExtractor:
    """Extract and normalize catalog numbers from text."""
    
    def __init__(self, patterns: BilingualPatterns = None):
        self.patterns = patterns or BilingualPatterns()
    
    def extract_catalog_numbers(self, text: str) -> List[Dict[str, str]]:
        """
        Extract all catalog numbers from text.
        
        Args:
            text: Input text to analyze
            
        Returns:
            List of catalog entries with system and number
        """
        catalogs = []
        
        for system, system_patterns in self.patterns.CATALOG_PATTERNS.items():
            for pattern in system_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    catalogs.append({
                        'system': system,
                        'number': match.strip()
                    })
        
        return self._deduplicate_catalogs(catalogs)
    
    def _deduplicate_catalogs(self, catalogs: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Remove duplicate catalog entries while preserving order."""
        seen = set()
        unique_catalogs = []
        
        for cat in catalogs:
            key = (cat['system'], cat['number'])
            if key not in seen:
                seen.add(key)
                unique_catalogs.append(cat)
        
        return unique_catalogs


# ============================================================================
# EFO CLASSIFIER CLASS  
# ============================================================================

class EFOClassifier:
    """Classify Errors, Freaks, and Oddities with bilingual support."""
    
    def __init__(self, patterns: BilingualPatterns = None):
        self.patterns = patterns or BilingualPatterns()
    
    def classify_efo_varieties(self, text: str) -> List[Dict[str, Any]]:
        """
        Classify EFO varieties in text.
        
        Args:
            text: Input text to analyze
            
        Returns:
            List of EFO classifications with confidence scores
        """
        varieties = []
        
        # Process overprint errors
        for pattern, subtype, label, confidence in self.patterns.EFO_PATTERNS['overprint_errors']:
            if re.search(pattern, text, re.IGNORECASE):
                # Try to extract overprint text
                overprint_match = re.search(r'(?:overprint|sobrecarga)[^a-zA-Z]*([A-Z\s]+)', text, re.IGNORECASE)
                overprint_text = overprint_match.group(1).strip() if overprint_match else None
                
                varieties.append({
                    'efo_class': 'overprint',
                    'subtype': subtype,
                    'label': label,
                    'text': overprint_text,
                    'confidence': confidence
                })
        
        # Process color errors
        for pattern, subtype, label, confidence in self.patterns.EFO_PATTERNS['color_errors']:
            if re.search(pattern, text, re.IGNORECASE):
                varieties.append({
                    'efo_class': 'color_error',
                    'subtype': subtype,
                    'label': label,
                    'confidence': confidence
                })
        
        # Process center errors
        for pattern, subtype, label, confidence in self.patterns.EFO_PATTERNS['center_errors']:
            if re.search(pattern, text, re.IGNORECASE):
                varieties.append({
                    'efo_class': 'center_error',
                    'subtype': subtype,
                    'label': label,
                    'confidence': confidence
                })
        
        # Process mirror prints
        for pattern, subtype, label, confidence in self.patterns.EFO_PATTERNS['mirror_print']:
            if re.search(pattern, text, re.IGNORECASE):
                varieties.append({
                    'efo_class': 'mirror_print',
                    'subtype': subtype,
                    'label': label,
                    'confidence': confidence
                })
        
        return varieties


# ============================================================================
# TECHNICAL SPECS EXTRACTOR CLASS
# ============================================================================

class TechnicalSpecsExtractor:
    """Extract technical specifications with bilingual support."""
    
    def __init__(self, patterns: BilingualPatterns = None):
        self.patterns = patterns or BilingualPatterns()
    
    def extract_technical_specifications(self, text: str) -> Dict[str, Any]:
        """
        Extract comprehensive technical specifications.
        
        Args:
            text: Input text to analyze
            
        Returns:
            Dictionary with technical specifications
        """
        specs = {}
        
        # Perforation
        perf_info = self._extract_perforation(text)
        if perf_info:
            specs['perforation'] = perf_info
        
        # Paper
        paper_info = self._extract_paper(text)
        if paper_info:
            specs['paper'] = paper_info
        
        # Printing method
        printing_info = self._extract_printing(text)
        if printing_info:
            specs['printing'] = printing_info
        
        # Watermark
        watermark_info = self._extract_watermark(text)
        if watermark_info:
            specs['watermark'] = watermark_info
        
        # Gum
        gum_info = self._extract_gum(text)
        if gum_info:
            specs['gum'] = gum_info
        
        return specs
    
    def _extract_perforation(self, text: str) -> Optional[Dict[str, Any]]:
        """Extract perforation information."""
        perf_info = {}
        
        # Measurements
        measurements = []
        for pattern in self.patterns.TECHNICAL_PATTERNS['perforation']['measurements']:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    measurements.append(f"{match[0]}x{match[1]}")
                else:
                    measurements.append(match)
        
        if measurements:
            perf_info['measurements'] = list(set(measurements))
        
        # Types
        for pattern, perf_type in self.patterns.TECHNICAL_PATTERNS['perforation']['types']:
            if re.search(pattern, text, re.IGNORECASE):
                if perf_type == 'imperforate':
                    perf_info['type'] = perf_type
                else:
                    perf_info['method'] = perf_type
                break
        
        return perf_info if perf_info else None
    
    def _extract_paper(self, text: str) -> Optional[Dict[str, Any]]:
        """Extract paper information.""" 
        paper_info = {}
        
        # Paper types
        for pattern, paper_type in self.patterns.TECHNICAL_PATTERNS['paper']['types']:
            if re.search(pattern, text, re.IGNORECASE):
                paper_info['type'] = paper_type
                break
        
        # Paper thickness
        for pattern, thickness in self.patterns.TECHNICAL_PATTERNS['paper']['thickness']:
            if re.search(pattern, text, re.IGNORECASE):
                paper_info['thickness'] = thickness
                break
        
        return paper_info if paper_info else None
    
    def _extract_printing(self, text: str) -> Optional[Dict[str, str]]:
        """Extract printing method."""
        for pattern, method in self.patterns.TECHNICAL_PATTERNS['printing']:
            if re.search(pattern, text, re.IGNORECASE):
                return {'method': method}
        
        return None
    
    def _extract_watermark(self, text: str) -> Optional[Dict[str, str]]:
        """Extract watermark information."""
        watermark_info = {}
        
        # Watermark types
        for pattern in self.patterns.TECHNICAL_PATTERNS['watermark']['types']:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                watermark_info['type'] = matches[0].strip()
                break
        
        # Watermark positions
        for pattern, position in self.patterns.TECHNICAL_PATTERNS['watermark']['positions']:
            if re.search(pattern, text, re.IGNORECASE):
                watermark_info['position'] = position
                break
        
        return watermark_info if watermark_info else None
    
    def _extract_gum(self, text: str) -> Optional[Dict[str, str]]:
        """Extract gum information."""
        for pattern, gum_type in self.patterns.TECHNICAL_PATTERNS['gum']:
            if re.search(pattern, text, re.IGNORECASE):
                return {'type': gum_type}
        
        return None


# ============================================================================
# CONDITION ASSESSOR CLASS
# ============================================================================

class ConditionAssessor:
    """Assess stamp condition with bilingual support."""
    
    def __init__(self, patterns: BilingualPatterns = None):
        self.patterns = patterns or BilingualPatterns()
    
    def extract_condition_assessment(self, text: str) -> Dict[str, Any]:
        """
        Extract comprehensive condition assessment.
        
        Args:
            text: Input text to analyze
            
        Returns:
            Dictionary with condition information
        """
        condition = {}
        
        # Mint status
        for pattern, status in self.patterns.CONDITION_PATTERNS['mint_status']:
            if re.search(pattern, text, re.IGNORECASE):
                condition['mint_status'] = status
                break
        
        # Used status
        for pattern, status in self.patterns.CONDITION_PATTERNS['used_status']:
            if re.search(pattern, text, re.IGNORECASE):
                condition['used_status'] = status
                break
        
        # Centering
        for pattern, centering in self.patterns.CONDITION_PATTERNS['centering']:
            if re.search(pattern, text, re.IGNORECASE):
                condition['centering'] = centering
                break
        
        # Defects
        defects = []
        for pattern in self.patterns.CONDITION_PATTERNS['defects']:
            matches = re.findall(pattern, text, re.IGNORECASE)
            defects.extend([match.lower().strip() for match in matches])
        
        if defects:
            condition['defects'] = list(set(defects))  # Remove duplicates
        
        return condition


# ============================================================================
# COSTA RICA CONTEXT EXTRACTOR CLASS
# ============================================================================

class CostaRicaContextExtractor:
    """Extract Costa Rica specific philatelic context."""
    
    def __init__(self, patterns: BilingualPatterns = None):
        self.patterns = patterns or BilingualPatterns()
    
    def extract_costa_rica_context(self, text: str) -> Dict[str, Any]:
        """
        Extract Costa Rica specific context.
        
        Args:
            text: Input text to analyze
            
        Returns:
            Dictionary with Costa Rica context
        """
        context = {}
        
        # Guanacaste period detection
        for pattern in self.patterns.COSTA_RICA_PATTERNS['guanacaste_period']:
            if re.search(pattern, text, re.IGNORECASE):
                context['guanacaste_period'] = True
                context['historical_significance'] = '1885-1891 Guanacaste overprint period'
                break
        
        # Historical periods
        periods = []
        for pattern, period in self.patterns.COSTA_RICA_PATTERNS['historical_periods']:
            if re.search(pattern, text, re.IGNORECASE):
                periods.append(period)
        
        if periods:
            context['historical_periods'] = list(set(periods))
        
        # Personalities
        personalities = []
        for pattern in self.patterns.COSTA_RICA_PATTERNS['personalities']:
            if re.search(pattern, text, re.IGNORECASE):
                personalities.append(pattern.replace(r'\s+', ' '))
        
        if personalities:
            context['personalities'] = personalities
        
        # Geographic features
        geography = []
        for pattern in self.patterns.COSTA_RICA_PATTERNS['geography']:
            if re.search(pattern, text, re.IGNORECASE):
                geography.append(pattern.replace(r'\s+', ' '))
        
        if geography:
            context['geographic_features'] = geography
        
        return context


# ============================================================================
# SEMANTIC ENRICHER CLASS - MAIN ORCHESTRATOR
# ============================================================================

class SemanticEnricher:
    """Main class that orchestrates all enrichment processes."""
    
    def __init__(self, use_secure_patterns: bool = True, min_confidence: float = 0.8):
        self.use_secure_patterns = use_secure_patterns
        self.min_confidence = min_confidence
        
        # Legacy extractors (always available for fallback)
        self.patterns = BilingualPatterns()
        self.catalog_extractor = CatalogExtractor(self.patterns)
        self.efo_classifier = EFOClassifier(self.patterns)
        self.tech_extractor = TechnicalSpecsExtractor(self.patterns)
        self.condition_assessor = ConditionAssessor(self.patterns)
        self.cr_extractor = CostaRicaContextExtractor(self.patterns)
        
        # Secure extractors (preferred when enabled)
        if use_secure_patterns:
            self.secure_catalog_extractor = SecureCatalogExtractor(min_confidence)
            self.secure_efo_detector = SecureEFODetector(min_confidence)
            self.context_validator = ContextValidator()
    
    def enrich_chunk_advanced_bilingual(self, chunk: Dict[str, Any]) -> Dict[str, Any]:
        """
        Perform comprehensive bilingual enrichment of a philatelic chunk.
        
        Args:
            chunk: Input chunk dictionary
            
        Returns:
            Enriched chunk with enhanced metadata
        """
        if not chunk or not isinstance(chunk, dict):
            return chunk
        
        text = chunk.get('text', '')
        if not text:
            return chunk
        
        # Initialize metadata structure
        if 'metadata' not in chunk:
            chunk['metadata'] = {}
        
        if 'entities' not in chunk['metadata']:
            chunk['metadata']['entities'] = {}
        
        entities = chunk['metadata']['entities']
        
        # Extract catalog numbers (secure or legacy)
        catalogs = self._extract_catalogs_with_validation(text)
        if catalogs:
            entities['catalog'] = catalogs
        
        # Extract dates
        dates = self._extract_dates(text)
        if dates:
            entities['dates'] = dates
        
        # Extract prices
        prices = self._extract_prices(text)
        if prices:
            entities['prices'] = prices
        
        # Extract face values
        values = self._extract_face_values(text)
        if values:
            entities['values'] = values
        
        # Extract colors
        colors = self._extract_colors(text)
        if colors:
            entities['colors'] = colors
        
        # Extract designs
        designs = self._extract_designs(text)
        if designs:
            entities['designs'] = designs
        
        # Extract technical specifications
        tech_specs = self.tech_extractor.extract_technical_specifications(text)
        for spec_type, spec_data in tech_specs.items():
            entities[spec_type] = spec_data
        
        # Extract condition assessment
        condition = self.condition_assessor.extract_condition_assessment(text)
        if condition:
            entities['condition'] = condition
        
        # Classify EFO varieties (secure or legacy)
        varieties = self._extract_efo_with_validation(text)
        if varieties:
            entities['varieties'] = varieties
        
        # Extract Costa Rica context
        cr_context = self.cr_extractor.extract_costa_rica_context(text)
        if cr_context:
            entities['costa_rica_context'] = cr_context
        
        # Calculate quality score
        chunk['metadata']['quality_score'] = self._calculate_quality_score(entities)
        
        return chunk
    
    def _extract_catalogs_with_validation(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract catalog numbers using secure patterns when available.
        
        Args:
            text: Input text to analyze
            
        Returns:
            List of catalog entries with validation information
        """
        if self.use_secure_patterns and hasattr(self, 'secure_catalog_extractor'):
            # Use secure extractor with fallback
            secure_results = self.secure_catalog_extractor.extract_with_fallback(text)
            
            # Convert to legacy format for backward compatibility, but keep security info
            legacy_format_results = []
            for result in secure_results:
                legacy_entry = {
                    'system': result['system'],
                    'number': result['number']
                }
                
                # Add security metadata
                if 'confidence' in result:
                    legacy_entry['confidence'] = result['confidence']
                    legacy_entry['context_valid'] = result.get('context_valid', False)
                    legacy_entry['has_anchor'] = result.get('has_anchor', False)
                    legacy_entry['secure_extraction'] = True
                
                legacy_format_results.append(legacy_entry)
            
            return legacy_format_results
        else:
            # Use legacy extractor
            return self.catalog_extractor.extract_catalog_numbers(text)
    
    def _extract_efo_with_validation(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract EFO varieties using secure patterns when available.
        
        Args:
            text: Input text to analyze
            
        Returns:
            List of EFO entries with validation information
        """
        if self.use_secure_patterns and hasattr(self, 'secure_efo_detector'):
            # Use secure detector
            secure_results = self.secure_efo_detector.detect_efo_varieties_secure(text)
            
            # Enhance results with additional security info
            for result in secure_results:
                result['secure_extraction'] = True
                
                # Add validation summary
                if result.get('context_valid', False):
                    result['validation_status'] = 'validated'
                elif result.get('has_anchor', False):
                    result['validation_status'] = 'partial_context'
                else:
                    result['validation_status'] = 'no_context'
            
            return secure_results
        else:
            # Use legacy EFO classifier
            return self.efo_classifier.classify_efo_varieties(text)
    
    def get_extraction_statistics(self) -> Dict[str, Any]:
        """
        Get statistics about the extraction methods being used.
        
        Returns:
            Dictionary with extraction configuration and capabilities
        """
        stats = {
            'secure_patterns_enabled': self.use_secure_patterns,
            'min_confidence_threshold': self.min_confidence,
            'extractors_available': {
                'legacy_catalog': True,
                'legacy_efo': True,
                'secure_catalog': hasattr(self, 'secure_catalog_extractor'),
                'secure_efo': hasattr(self, 'secure_efo_detector'),
                'context_validator': hasattr(self, 'context_validator')
            }
        }
        
        if hasattr(self, 'context_validator'):
            stats['context_window_size'] = self.context_validator.window_size
            
        return stats
    
    def _extract_dates(self, text: str) -> List[str]:
        """Extract and normalize dates."""
        dates = []
        
        for pattern in self.patterns.DATE_PATTERNS:
            matches = re.findall(pattern, text)
            for match in matches:
                # Simple validation and normalization
                if len(match) == 4 and match.isdigit():  # Just year
                    year = int(match)
                    if 1800 <= year <= 2030:  # Reasonable philatelic year range
                        dates.append(match)
                elif '-' in match and len(match) == 10:  # ISO format
                    dates.append(match)
                else:
                    # Try to normalize other formats (basic implementation)
                    dates.append(match)
        
        return list(set(dates))  # Remove duplicates
    
    def _extract_prices(self, text: str) -> List[Dict[str, Any]]:
        """Extract price information."""
        prices = []
        
        for pattern, currency, prefix in self.patterns.PRICE_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    # Clean and convert to float
                    clean_amount = match.replace(',', '')
                    amount = float(clean_amount)
                    
                    prices.append({
                        'amount': amount,
                        'currency': currency,
                        'raw': f"{prefix}{match}"
                    })
                except ValueError:
                    continue
        
        return prices
    
    def _extract_face_values(self, text: str) -> List[Dict[str, Any]]:
        """Extract face value information."""
        values = []
        
        # Face value patterns
        value_patterns = [
            r'(\d+(?:\.\d+)?)\s*(?:centavos?|céntimos?)',
            r'(\d+(?:\.\d+)?)\s*c(?:ts?)?\.',
            r'(\d+(?:\.\d+)?)\s*(?:colones?|₡)',
            r'(\d+(?:\.\d+)?)\s*(?:pesos?)',
            r'(\d+(?:\.\d+)?)\s*(?:reales?)'
        ]
        
        units = ['centavos', 'c', 'colones', 'pesos', 'reales']
        
        for i, pattern in enumerate(value_patterns):
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    face_value = float(match)
                    values.append({
                        'face_value': face_value,
                        'unit': units[i]
                    })
                except ValueError:
                    continue
        
        return values
    
    def _extract_colors(self, text: str) -> List[str]:
        """Extract color information."""
        color_patterns = [
            r'(?:azul|blue)',
            r'(?:rojo|red)',
            r'(?:verde|green)',
            r'(?:amarillo|yellow)',
            r'(?:negro|black)',
            r'(?:blanco|white)',
            r'(?:rosa|pink)',
            r'(?:violeta|púrpura|violet|purple)',
            r'(?:naranja|orange)',
            r'(?:marrón|café|brown)',
            r'(?:gris|gray|grey)'
        ]
        
        colors = []
        for pattern in color_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            colors.extend([match.lower() for match in matches])
        
        return list(set(colors))  # Remove duplicates
    
    def _extract_designs(self, text: str) -> List[str]:
        """Extract design and theme information."""
        designs = []
        
        for pattern in self.patterns.DESIGN_PATTERNS:
            if re.search(pattern, text, re.IGNORECASE):
                # Normalize the pattern for output
                design = pattern.replace(r'(?:', '').replace(r'\s+', ' ').replace('|', '/').rstrip(')')
                designs.append(design)
        
        return list(set(designs))  # Remove duplicates
    
    def _calculate_quality_score(self, entities: Dict[str, Any]) -> float:
        """Calculate a quality score based on enrichment completeness."""
        score = 0.0
        
        # Base score for having any entities
        if entities:
            score += 0.1
        
        # Catalog information (high value)
        if entities.get('catalog'):
            score += 0.2
        
        # Technical specifications (medium-high value)
        if entities.get('perforation'):
            score += 0.1
        if entities.get('paper'):
            score += 0.1
        if entities.get('printing'):
            score += 0.1
        
        # Condition (medium value)
        if entities.get('condition'):
            score += 0.15
        
        # EFO varieties (high value for collectors)
        if entities.get('varieties'):
            score += 0.15
        
        # Other entities (small increments)
        for key in ['dates', 'prices', 'values', 'colors', 'designs']:
            if entities.get(key):
                score += 0.05
        
        # Costa Rica context (bonus for specialization)
        if entities.get('costa_rica_context'):
            score += 0.1
        
        return min(score, 1.0)  # Cap at 1.0


# ============================================================================
# CHUNK ANALYSIS AND UTILITY FUNCTIONS
# ============================================================================

import json
import copy
import sys
import glob
from pathlib import Path


class ChunkAnalyzer:
    """Advanced chunk analysis and processing utilities."""
    
    @staticmethod
    def analyze_chunk_text(text: str) -> Dict[str, Any]:
        """
        Analiza un chunk de texto y retorna métricas útiles para decisiones de combinación.
        
        Args:
            text: Texto del chunk a analizar
            
        Returns:
            Dictionary con métricas del análisis
        """
        if not text or not isinstance(text, str):
            return {
                'word_count': 0,
                'char_count': 0,
                'has_meaningful_content': False,
                'is_orphan_likely': True
            }
        
        # Limpiar texto para análisis
        clean_text = re.sub(r'\s+', ' ', text.strip())
        words = clean_text.split()
        
        return {
            'word_count': len(words),
            'char_count': len(clean_text),
            'has_meaningful_content': len(words) >= 3 and len(clean_text) >= 10,
            'is_orphan_likely': len(words) < 10
        }
    
    @staticmethod
    def get_chunk_labels(chunk: Dict[str, Any]) -> List[str]:
        """
        Extrae las etiquetas (labels) de un chunk de manera segura.
        
        Args:
            chunk: Chunk dictionary
            
        Returns:
            Lista de labels del chunk
        """
        try:
            return chunk.get('metadata', {}).get('labels', [])
        except (KeyError, AttributeError):
            return []
    
    @staticmethod
    def is_original_header(chunk: Dict[str, Any]) -> bool:
        """
        Verifica si un chunk es un header ORIGINAL (no combinado).
        
        Args:
            chunk: Chunk dictionary
            
        Returns:
            True si es un header original
        """
        # Verificar si es un chunk combinado
        metadata = chunk.get('metadata', {})
        if metadata.get('combined_with_header', False) or metadata.get('header_added', False):
            return False
        
        # Verificar si tiene labels de header
        labels = ChunkAnalyzer.get_chunk_labels(chunk)
        header_labels = ['sec', 'sub_sec', 'sub_sub_sec', 'title']
        return any(header_label in labels for header_label in header_labels)
    
    @staticmethod
    def estimate_chunk_size_bytes(chunk: Dict[str, Any]) -> int:
        """
        Estima el tamaño en bytes de un chunk.
        
        Args:
            chunk: Chunk dictionary
            
        Returns:
            Tamaño estimado en bytes
        """
        try:
            return sys.getsizeof(json.dumps(chunk, ensure_ascii=False))
        except:
            # Fallback: estimar por longitud de texto
            text_len = len(chunk.get('text', ''))
            return text_len * 4  # Factor conservador para incluir metadata


class ChunkCombiner:
    """Handles chunk combination logic with semantic enrichment."""
    
    def __init__(self, enricher: SemanticEnricher = None):
        self.enricher = enricher or SemanticEnricher()
        self.analyzer = ChunkAnalyzer()
    
    def should_combine_chunks(self, current_chunk: Dict[str, Any], 
                            previous_chunks: List[Dict[str, Any]]) -> Tuple[bool, Optional[int]]:
        """
        Determina si un chunk debe combinarse con un header previo.
        
        LÓGICA CORREGIDA - HEADER REUTILIZABLE:
        - TODOS los chunks de texto buscan su header anterior
        - Headers pueden ser REUTILIZADOS para múltiples chunks
        - Simplemente procesar chunk por chunk añadiendo contexto
        - NO marcar headers como "usados"
        
        Args:
            current_chunk: El chunk actual a evaluar
            previous_chunks: Lista de chunks anteriores procesados (ya ordenados)

        Returns:
            Tuple[bool, Optional[int]]: (should_combine, header_index)
        """
        current_labels = self.analyzer.get_chunk_labels(current_chunk)
        current_text = current_chunk.get('text', '')
        
        # CRITERIO 1: No procesar headers (estos no necesitan contexto adicional)
        if self.analyzer.is_original_header(current_chunk):
            return False, None
        
        # CRITERIO 2: Solo procesar chunks con texto significativo
        if not current_text or len(current_text.strip()) < 5:
            return False, None
        
        # CRITERIO 3: Buscar el header ORIGINAL MÁS CERCANO hacia atrás (máximo 20 chunks)
        search_limit = min(20, len(previous_chunks))
        
        # Buscar hacia atrás el primer header ORIGINAL disponible
        for i in range(len(previous_chunks) - 1, len(previous_chunks) - search_limit - 1, -1):
            if i < 0:
                continue
                
            prev_chunk = previous_chunks[i]
            
            # Solo considerar headers ORIGINALES
            if not self.analyzer.is_original_header(prev_chunk):
                continue
            
            prev_text = prev_chunk.get('text', '').strip()
            
            # Verificar que tenga texto significativo
            if not prev_text or len(prev_text) < 3:
                continue
            
            # Si llegamos aquí, es un header original válido
            return True, i
        
        # No se encontró header original disponible
        return False, None
    
    def combine_single_header_chunk(self, header_chunk: Dict[str, Any], 
                                   content_chunk: Dict[str, Any]) -> Dict[str, Any]:
        """
        Combina UN header ORIGINAL con el contenido usando ENRIQUECIMIENTO SEMÁNTICO AVANZADO.
        Genera text_original (sin enriquecimiento) y text (enriquecido para vectorización).
        
        Args:
            header_chunk: Chunk de header original
            content_chunk: Chunk de contenido
            
        Returns:
            Chunk combinado con enriquecimiento semántico y textos separados
        """
        # VERIFICACIÓN CRÍTICA: Asegurar que el header sea original
        if not self.analyzer.is_original_header(header_chunk):
            print(f"    ⚠️ ADVERTENCIA: Intentando usar chunk no-original como header")
            return copy.deepcopy(content_chunk)  # Retornar contenido sin combinar
        
        # Crear chunk combinado basado en el contenido
        combined_chunk = copy.deepcopy(content_chunk)
        
        # 1. CREAR TEXTO ORIGINAL (sin enriquecimiento) - para mostrar al usuario
        header_text = header_chunk.get('text', '').strip()
        content_text = content_chunk.get('text', '').strip()
        text_original = f"{header_text}\\n\\n{content_text}" if header_text and content_text else header_text + content_text
        
        # 2. CREAR TEXTO ENRIQUECIDO - para vectorización RAG
        try:
            enriched_text = self.create_enriched_combined_text_bilingual(header_chunk, content_chunk)
            combined_chunk['text'] = enriched_text  # Texto enriquecido para embeddings
            combined_chunk['text_original'] = text_original  # Texto original para mostrar usuario
            
        except Exception as enrich_error:
            # Fallback: usar texto original para ambos
            print(f"    ⚠️ Error en enriquecimiento semántico: {str(enrich_error)[:50]}...")
            combined_chunk['text'] = text_original
            combined_chunk['text_original'] = text_original
        
        # Combinar metadatos de labels
        header_labels = set(self.analyzer.get_chunk_labels(header_chunk))
        content_labels = set(self.analyzer.get_chunk_labels(combined_chunk))
        combined_chunk['metadata']['labels'] = list(header_labels | content_labels)
        
        # Reading order range expandido
        header_range = header_chunk.get('metadata', {}).get('reading_order_range', [0, 0])
        content_range = combined_chunk['metadata'].get('reading_order_range', [0, 0])
        
        combined_chunk['metadata']['reading_order_range'] = [
            min(header_range[0], content_range[0]),
            max(header_range[1], content_range[1])
        ]
        
        # Marcar procesamiento interno (se limpiará antes de Weaviate)
        combined_chunk['metadata']['_processing'] = {
            'combined_with_header': True,
            'header_added': True,
            'original_content_id': content_chunk.get('chunk_id', ''),
            'header_chunk_id': header_chunk.get('chunk_id', ''),
            'semantic_enrichment_applied': True
        }
        
        return combined_chunk
    
    def create_enriched_combined_text_bilingual(self, header_chunk: Dict[str, Any], 
                                              content_chunk: Dict[str, Any]) -> str:
        """
        Crear texto enriquecido combinando header y content usando el sistema bilingüe.
        
        Args:
            header_chunk: Chunk de header/título
            content_chunk: Chunk de contenido principal
            
        Returns:
            Texto enriquecido con metadatos filatélicos
        """
        enriched_parts = []
        
        # Combinar texto base
        header_text = header_chunk.get("text", "").strip()
        content_text = content_chunk.get("text", "").strip()
        full_text = f"{header_text}\\n\\n{content_text}"
        
        # Agregar texto base
        enriched_parts.append(full_text)
        
        # ================================
        # ENRIQUECIMIENTO CON NUEVO SISTEMA BILINGÜE
        # ================================
        
        # Enriquecer content chunk con nuevo sistema
        enriched_content = self.enricher.enrich_chunk_advanced_bilingual(content_chunk.copy())
        entities = enriched_content.get("metadata", {}).get("entities", {})
        
        # CATÁLOGOS INTERNACIONALES
        if entities.get("catalog"):
            catalog_systems = {}
            for cat in entities["catalog"]:
                system = cat["system"]
                if system not in catalog_systems:
                    catalog_systems[system] = []
                catalog_systems[system].append(cat["number"])
            
            catalog_text = "Catálogos internacionales: "
            catalog_entries = []
            for system, numbers in catalog_systems.items():
                numbers_str = ", ".join(numbers)
                catalog_entries.append(f"{system} {numbers_str}")
            
            catalog_text += ", ".join(catalog_entries) + ". "
            enriched_parts.append(f"\\n{catalog_text}")
        
        # FECHAS Y VALORES
        if entities.get("dates"):
            dates_str = ", ".join(entities["dates"])
            enriched_parts.append(f"\\nFechas: {dates_str}. ")
        
        if entities.get("values"):
            values_str = ", ".join([f"{v['face_value']} {v['unit']}" for v in entities["values"]])
            enriched_parts.append(f"\\nValores faciales: {values_str}. ")
        
        # ESPECIFICACIONES TÉCNICAS
        tech_specs = []
        if entities.get("perforation"):
            perf = entities["perforation"]
            if perf.get("measurements"):
                tech_specs.append(f"dentado {'/'.join(perf['measurements'])}")
            if perf.get("type"):
                tech_specs.append(perf["type"])
            if perf.get("method"):
                tech_specs.append(perf["method"])
        
        if entities.get("paper"):
            paper = entities["paper"]
            if paper.get("type"):
                tech_specs.append(paper["type"])
        
        if entities.get("printing"):
            tech_specs.append(entities["printing"]["method"])
        
        if tech_specs:
            enriched_parts.append(f"\\nEspecificaciones técnicas: {', '.join(tech_specs)}. ")
        
        # CONDICIÓN
        if entities.get("condition"):
            condition = entities["condition"]
            cond_parts = []
            
            if condition.get("mint_status"):
                cond_parts.append(condition["mint_status"])
            if condition.get("used_status"):
                cond_parts.append(condition["used_status"])
            if condition.get("centering"):
                cond_parts.append(f"centrado {condition['centering']}")
            if condition.get("defects"):
                cond_parts.append(f"defectos: {', '.join(condition['defects'])}")
            
            if cond_parts:
                enriched_parts.append(f"\\nCondición: {', '.join(cond_parts)}. ")
        
        # VARIEDADES EFO
        if entities.get("varieties"):
            efo_parts = []
            for variety in entities["varieties"]:
                efo_desc = f"{variety['label']} (confianza: {variety['confidence']:.1f})"
                if variety.get("text"):
                    efo_desc += f" - texto: {variety['text']}"
                efo_parts.append(efo_desc)
            
            enriched_parts.append(f"\\nVariedades EFO: {'; '.join(efo_parts)}. ")
        
        # CONTEXTO COSTA RICA
        if entities.get("costa_rica_context"):
            cr_context = entities["costa_rica_context"]
            if cr_context.get("guanacaste_period"):
                enriched_parts.append("\\nPeríodo Guanacaste 1885-1891. ")
            if cr_context.get("historical_significance"):
                enriched_parts.append(f"\\nSignificado histórico: {cr_context['historical_significance']}. ")
        
        # COLORES Y DISEÑOS
        if entities.get("colors"):
            colors_str = ", ".join(entities["colors"])
            enriched_parts.append(f"\\nColores: {colors_str}. ")
        
        if entities.get("designs"):
            designs_str = ", ".join(entities["designs"])
            enriched_parts.append(f"\\nDiseños: {designs_str}. ")
        
        # PRECIOS
        if entities.get("prices"):
            prices_str = ", ".join([f"{p['raw']}" for p in entities["prices"]])
            enriched_parts.append(f"\\nPrecios: {prices_str}. ")
        
        # CONTEXTO DEL HEADER
        if header_chunk:
            enriched_header = self.enricher.enrich_chunk_advanced_bilingual(header_chunk.copy())
            header_entities = enriched_header.get("metadata", {}).get("entities", {})
            
            if header_entities.get("costa_rica_context"):
                enriched_parts.append(f"\\nContexto de sección: {header_text}")
        
        return "".join(enriched_parts)


def prepare_chunk_for_weaviate(chunk: Dict[str, Any]) -> Dict[str, Any]:
    """
    Prepara un chunk para indexación en Weaviate eliminando metadatos innecesarios
    y asegurando que tenga la estructura correcta.
    
    Args:
        chunk: Chunk procesado con enriquecimiento semántico
        
    Returns:
        Chunk limpio listo para Weaviate
    """
    # Crear copia limpia
    clean_chunk = {
        'chunk_id': chunk.get('chunk_id', ''),
        'chunk_type': chunk.get('chunk_type', 'text'),
        'text': chunk.get('text', ''),  # Texto enriquecido para vectorización
        'text_original': chunk.get('text_original', chunk.get('text', '')),  # Texto original para UI
        'metadata': {}
    }
    
    # Copiar solo metadatos esenciales
    original_metadata = chunk.get('metadata', {})
    
    # Labels y estructura básica
    if 'labels' in original_metadata:
        clean_chunk['metadata']['labels'] = original_metadata['labels']
    
    if 'reading_order_range' in original_metadata:
        clean_chunk['metadata']['reading_order_range'] = original_metadata['reading_order_range']
    
    if 'quality_score' in original_metadata:
        clean_chunk['metadata']['quality_score'] = original_metadata['quality_score']
    
    # Entidades filatélicas (esenciales para búsquedas)
    if 'entities' in original_metadata:
        clean_chunk['metadata']['entities'] = original_metadata['entities']
    
    # Topics y clasificación
    if 'topics' in original_metadata:
        clean_chunk['metadata']['topics'] = original_metadata['topics']
    
    if 'axes' in original_metadata:
        clean_chunk['metadata']['axes'] = original_metadata['axes']
    
    # Metadatos de tabla (si aplica)
    if chunk.get('chunk_type') == 'table' or chunk.get('chunk_type') == 'table_row':
        for table_field in ['table_markdown', 'headers', 'n_rows', 'parent_table_chunk_id', 'row_index_range']:
            if table_field in original_metadata:
                clean_chunk['metadata'][table_field] = original_metadata[table_field]
    
    # NO incluir: grounding, _processing, campos internos de combinación
    
    return clean_chunk


def prepare_chunks_batch_for_weaviate(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Prepara un lote de chunks para Weaviate de manera eficiente.
    
    Args:
        chunks: Lista de chunks procesados
        
    Returns:
        Lista de chunks limpios para Weaviate
    """
    clean_chunks = []
    
    for chunk in chunks:
        try:
            clean_chunk = prepare_chunk_for_weaviate(chunk)
            
            # Validación básica
            if (clean_chunk.get('chunk_id') and 
                clean_chunk.get('text') and 
                len(clean_chunk.get('text', '').strip()) >= 3):
                
                clean_chunks.append(clean_chunk)
            else:
                print(f"    ⚠️ Chunk {chunk.get('chunk_id', 'unknown')} omitido: contenido insuficiente")
                
        except Exception as e:
            print(f"    ⚠️ Error preparando chunk {chunk.get('chunk_id', 'unknown')}: {str(e)[:50]}...")
    
    return clean_chunks


class PhilatelicFileUtils:
    """Utility functions for handling philatelic files."""
    
    @staticmethod
    def get_all_philatelic_files(base_dir: str = None) -> List[Path]:
        """
        Obtiene todos los archivos *_philatelic.json del directorio parsed_jsons.
        
        Args:
            base_dir: Directorio base (opcional)
            
        Returns:
            Lista de archivos philatelic ordenados
        """
        if base_dir is None:
            # Try to find results/parsed_jsons directory
            current_dir = Path(".")
            parsed_jsons_dir = current_dir / "results" / "parsed_jsons"
            if not parsed_jsons_dir.exists():
                parsed_jsons_dir = current_dir / "parsed_jsons"
        else:
            parsed_jsons_dir = Path(base_dir)
        
        pattern = str(parsed_jsons_dir / "*_philatelic.json")
        files = [Path(f) for f in glob.glob(pattern)]
        return sorted(files)
    
    @staticmethod
    def extract_doc_id_from_filename(filename: str) -> str:
        """
        Extrae el ID del documento del nombre del archivo.
        
        Args:
            filename: Nombre del archivo
            
        Returns:
            Document ID extraído
            
        Example:
            'OXCART75_philatelic.json' -> 'OXCART75'
        """
        return filename.replace('_philatelic.json', '').replace('.json', '')


# ============================================================================
# SECURE CONVENIENCE FUNCTIONS - NEW SECURE API
# ============================================================================

def enrich_chunk_secure_bilingual(chunk: Dict[str, Any], min_confidence: float = 0.8, 
                                 use_secure_patterns: bool = True) -> Dict[str, Any]:
    """
    Secure enrichment function with contextual validation.
    
    Args:
        chunk: Input chunk dictionary
        min_confidence: Minimum confidence threshold for pattern matches
        use_secure_patterns: Whether to use secure patterns with anchors
        
    Returns:
        Enriched chunk with validated philatelic data
    """
    enricher = SemanticEnricher(use_secure_patterns=use_secure_patterns, 
                               min_confidence=min_confidence)
    return enricher.enrich_chunk_advanced_bilingual(chunk)


def extract_catalog_numbers_secure(text: str, min_confidence: float = 0.8) -> List[Dict[str, Any]]:
    """
    Secure catalog number extraction with context validation.
    
    Args:
        text: Input text to analyze
        min_confidence: Minimum confidence threshold
        
    Returns:
        List of validated catalog entries
    """
    extractor = SecureCatalogExtractor(min_confidence)
    return extractor.extract_catalog_numbers_secure(text)


def detect_efo_varieties_secure(text: str, min_confidence: float = 0.6) -> List[Dict[str, Any]]:
    """
    Secure EFO variety detection with context validation.
    
    Args:
        text: Input text to analyze
        min_confidence: Minimum confidence threshold
        
    Returns:
        List of validated EFO detections
    """
    detector = SecureEFODetector(min_confidence)
    return detector.detect_efo_varieties_secure(text)


def validate_philatelic_context(text: str, match_span: Tuple[int, int], 
                               pattern_type: str = 'general') -> Dict[str, Any]:
    """
    Validate if a text match has proper philatelic context.
    
    Args:
        text: Full text being analyzed
        match_span: (start, end) positions of the match
        pattern_type: Type of pattern being validated
        
    Returns:
        Validation results with confidence score
    """
    validator = ContextValidator()
    return validator.validate_pattern_match(text, match_span, pattern_type)


def get_secure_extraction_statistics() -> Dict[str, Any]:
    """
    Get information about secure extraction capabilities.
    
    Returns:
        Dictionary with available extractors and their configurations
    """
    enricher = SemanticEnricher(use_secure_patterns=True)
    return enricher.get_extraction_statistics()


# ============================================================================
# CONVENIENCE FUNCTIONS FOR BACKWARD COMPATIBILITY (LEGACY)
# ============================================================================

def enrich_chunk_advanced_philatelic_bilingual(chunk: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convenience function for enriching a single chunk.
    Maintains backward compatibility with existing code.
    """
    # Use secure patterns by default now, but maintain legacy API
    enricher = SemanticEnricher(use_secure_patterns=True, min_confidence=0.8)
    return enricher.enrich_chunk_advanced_bilingual(chunk)


def extract_all_catalog_numbers_bilingual(text: str) -> List[Dict[str, str]]:
    """
    Convenience function for extracting catalog numbers.
    Maintains backward compatibility with existing code.
    """
    extractor = CatalogExtractor()
    return extractor.extract_catalog_numbers(text)


def classify_efo_varieties_bilingual(text: str) -> List[Dict[str, Any]]:
    """
    Convenience function for classifying EFO varieties.
    Maintains backward compatibility with existing code.
    """
    classifier = EFOClassifier()
    return classifier.classify_efo_varieties(text)


# ============================================================================
# CHUNK PROCESSING CONVENIENCE FUNCTIONS
# ============================================================================

def should_combine_chunks(current_chunk: Dict[str, Any], previous_chunks: List[Dict[str, Any]]) -> Tuple[bool, Optional[int]]:
    """
    Convenience function for chunk combination logic.
    Maintains backward compatibility with existing code.
    """
    combiner = ChunkCombiner()
    return combiner.should_combine_chunks(current_chunk, previous_chunks)


def combine_single_header_chunk(header_chunk: Dict[str, Any], content_chunk: Dict[str, Any]) -> Dict[str, Any]:
    """
    Convenience function for combining header and content chunks.
    Maintains backward compatibility with existing code.
    """
    combiner = ChunkCombiner()
    return combiner.combine_single_header_chunk(header_chunk, content_chunk)


def create_enriched_combined_text(header_chunk: Dict[str, Any], content_chunk: Dict[str, Any]) -> str:
    """
    Convenience function for creating enriched combined text.
    Maintains backward compatibility with existing code.
    """
    combiner = ChunkCombiner()
    return combiner.create_enriched_combined_text_bilingual(header_chunk, content_chunk)


def analyze_chunk_text(text: str) -> Dict[str, Any]:
    """
    Convenience function for analyzing chunk text.
    Maintains backward compatibility with existing code.
    """
    return ChunkAnalyzer.analyze_chunk_text(text)


def get_chunk_labels(chunk: Dict[str, Any]) -> List[str]:
    """
    Convenience function for getting chunk labels.
    Maintains backward compatibility with existing code.
    """
    return ChunkAnalyzer.get_chunk_labels(chunk)


def is_original_header(chunk: Dict[str, Any]) -> bool:
    """
    Convenience function for checking if chunk is original header.
    Maintains backward compatibility with existing code.
    """
    return ChunkAnalyzer.is_original_header(chunk)


def estimate_chunk_size_bytes(chunk: Dict[str, Any]) -> int:
    """
    Convenience function for estimating chunk size.
    Maintains backward compatibility with existing code.
    """
    return ChunkAnalyzer.estimate_chunk_size_bytes(chunk)


def get_all_philatelic_files(base_dir: str = None) -> List[Path]:
    """
    Convenience function for getting philatelic files.
    Maintains backward compatibility with existing code.
    """
    return PhilatelicFileUtils.get_all_philatelic_files(base_dir)


def extract_doc_id_from_filename(filename: str) -> str:
    """
    Convenience function for extracting document ID from filename.
    Maintains backward compatibility with existing code.
    """
    return PhilatelicFileUtils.extract_doc_id_from_filename(filename)


# ============================================================================
# MAIN EXECUTION AND TESTING
# ============================================================================

if __name__ == "__main__":
    """
    Example usage and testing of the secure bilingual philatelic logic.
    """
    
    # Test cases for secure pattern validation
    test_cases = [
        {
            "name": "Spanish Philatelic - Positive Case",
            "text": "Scott 147, 5 centavos azul, nuevo sin charnela, muy fino centrado. Sobrecarga GUANACASTE invertida, extremadamente raro. Litografiado en papel satinado con goma original.",
            "expected_secure": True,
            "expected_catalogs": ["Scott"],
            "expected_efo": True
        },
        {
            "name": "English Philatelic - Positive Case", 
            "text": "Michel 23a, 10c red, mint never hinged, very fine. Double overprint variety, signed Peralta. Engraved on wove paper with original gum.",
            "expected_secure": True,
            "expected_catalogs": ["Michel"],
            "expected_efo": True
        },
        {
            "name": "Bio-informatics False Positive Test",
            "text": "The EFO ontology database contains experimental factor information for gene expression analysis. Software version Scott 2.1 implements the algorithm.",
            "expected_secure": False,
            "expected_catalogs": [],
            "expected_efo": False
        },
        {
            "name": "Software Context False Positive",
            "text": "In the software specification, Scott configuration parameter S 147 controls the timeout. The EFO module handles errors and debugging.",
            "expected_secure": False,
            "expected_catalogs": [],
            "expected_efo": False
        },
        {
            "name": "Ambiguous Context Test",
            "text": "Scott 25 was mentioned in the document. The system encountered various errors during processing.",
            "expected_secure": False,
            "expected_catalogs": [],
            "expected_efo": False
        }
    ]
    
    print("Testing Secure Bilingual Philatelic Pattern Recognition")
    print("=" * 70)
    
    # Test secure extraction functions
    print("\n[SECURE] PATTERN VALIDATION TESTS:")
    print("-" * 50)
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. {test_case['name']}")
        print(f"Text: {test_case['text'][:80]}{'...' if len(test_case['text']) > 80 else ''}")
        
        # Test secure catalog extraction
        secure_catalogs = extract_catalog_numbers_secure(test_case['text'], min_confidence=0.8)
        catalog_systems = [cat['system'] for cat in secure_catalogs]
        
        # Test secure EFO detection
        secure_efo = detect_efo_varieties_secure(test_case['text'], min_confidence=0.6)
        
        print(f"Secure Catalogs Found: {catalog_systems}")
        if secure_catalogs:
            for cat in secure_catalogs:
                print(f"  - {cat['system']} {cat['number']} (confidence: {cat['confidence']:.2f})")
        
        print(f"Secure EFO Found: {len(secure_efo) > 0}")
        if secure_efo:
            for efo in secure_efo:
                print(f"  - {efo['label']} (confidence: {efo['confidence']:.2f}, valid: {efo['context_valid']})")
        
        # Validation check
        catalogs_match = set(catalog_systems) == set(test_case['expected_catalogs'])
        efo_match = (len(secure_efo) > 0) == test_case['expected_efo']
        
        print(f"[CHECK] Validation: Catalogs {'PASS' if catalogs_match else 'FAIL'}, EFO {'PASS' if efo_match else 'FAIL'}")
    
    print("\n" + "=" * 70)
    print("[COMPARE] TESTING: Secure vs Legacy")
    print("-" * 70)
    
    # Test case with clear philatelic context
    philatelic_text = "This 1885 Scott 1-5 stamp from Costa Rica shows the coat of arms. The 5 centavos green has inverted center error, extremely rare variety."
    
    # Test case with ambiguous context
    ambiguous_text = "Scott procedure 147 was implemented in software version 2.5. The system reports various EFO exceptions during runtime."
    
    for text_name, text in [("Philatelic Context", philatelic_text), ("Ambiguous Context", ambiguous_text)]:
        print(f"\n{text_name}:")
        print(f"Text: {text}")
        
        # Legacy extraction
        legacy_enricher = SemanticEnricher(use_secure_patterns=False)
        legacy_chunk = {"text": text, "metadata": {}}
        legacy_result = legacy_enricher.enrich_chunk_advanced_bilingual(legacy_chunk)
        
        # Secure extraction
        secure_enricher = SemanticEnricher(use_secure_patterns=True, min_confidence=0.8)
        secure_chunk = {"text": text, "metadata": {}}
        secure_result = secure_enricher.enrich_chunk_advanced_bilingual(secure_chunk)
        
        legacy_catalogs = legacy_result.get('metadata', {}).get('entities', {}).get('catalog', [])
        secure_catalogs = secure_result.get('metadata', {}).get('entities', {}).get('catalog', [])
        
        legacy_efo = legacy_result.get('metadata', {}).get('entities', {}).get('varieties', [])
        secure_efo = secure_result.get('metadata', {}).get('entities', {}).get('varieties', [])
        
        print(f"  Legacy Catalogs: {len(legacy_catalogs)} found")
        print(f"  Secure Catalogs: {len(secure_catalogs)} found")
        if secure_catalogs:
            for cat in secure_catalogs:
                print(f"    - {cat.get('system', 'Unknown')} {cat.get('number', '')} (confidence: {cat.get('confidence', 'N/A')})")
        
        print(f"  Legacy EFO: {len(legacy_efo)} found")
        print(f"  Secure EFO: {len(secure_efo)} found")
        if secure_efo:
            for efo in secure_efo:
                print(f"    - {efo.get('label', 'Unknown')} (confidence: {efo.get('confidence', 'N/A')})")
    
    # Display system statistics
    print("\n" + "=" * 70)
    print("[STATS] SYSTEM STATISTICS")
    print("-" * 70)
    
    stats = get_secure_extraction_statistics()
    print(f"Secure Patterns Enabled: {stats['secure_patterns_enabled']}")
    print(f"Minimum Confidence Threshold: {stats['min_confidence_threshold']}")
    print(f"Context Window Size: {stats.get('context_window_size', 'N/A')} characters")
    
    print("\nAvailable Extractors:")
    for extractor, available in stats['extractors_available'].items():
        status = "[OK]" if available else "[NO]"
        print(f"  {status} {extractor.replace('_', ' ').title()}")
    
    print("\n[SUCCESS] Secure Bilingual Philatelic Logic System fully operational!")
    print("[SECURITY] Enhanced security features active - context validation enabled")
    print("[ACCURACY] Improved accuracy through philatelic anchor verification")