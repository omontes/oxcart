{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99d6479",
   "metadata": {},
   "source": [
    "## LangChain RAG for Philately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import weaviate\n",
    "from philatelic_weaviate import *\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_weaviate import WeaviateVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.retrievers import MultiQueryRetriever, EnsembleRetriever\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "print(f\"üìö LangChain RAG for Philately - Weaviate v{weaviate.__version__}\")\n",
    "print(\"üîß All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bf1da",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7294638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üéõÔ∏è CONFIGURATION - Centralized Settings for RAG System\n",
    "# ========================================================================================\n",
    "\n",
    "# Environment variables - Import from philatelic_weaviate if not already defined\n",
    "if 'WEAVIATE_URL' not in locals():\n",
    "    from philatelic_weaviate import WEAVIATE_URL, OPENAI_API_KEY\n",
    "\n",
    "# Collection Settings\n",
    "COLLECTION_NAME = os.getenv('WEAVIATE_COLLECTION_NAME', 'Oxcart')\n",
    "\n",
    "# Document Limits (configurable in one place)\n",
    "RAG_DOCUMENT_LIMIT = 30        # Main limit for RAG evaluation with optimal balance\n",
    "RETRIEVER_TEST_LIMIT = 15      # For individual retriever testing  \n",
    "DEMO_LIMIT = 8                 # For demonstration purposes\n",
    "SEARCH_MAX_LIMIT = 100         # Maximum for broad searches\n",
    "\n",
    "# Content Processing\n",
    "MAX_CONTENT_LENGTH = 400       # Characters per document in context\n",
    "PRESERVE_METADATA_ALWAYS = True  # Always show Scott numbers, years, etc.\n",
    "\n",
    "# Logging Control\n",
    "VERBOSE_LOGGING = False        # Set to True for detailed debug info\n",
    "SHOW_PROGRESS = True          # Show progress indicators\n",
    "\n",
    "# Test Query\n",
    "TEST_QUERY = \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\"\n",
    "TEST_MODE = \"hybrid\"\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded:\")\n",
    "print(f\"   üåê Weaviate URL: {WEAVIATE_URL}\")\n",
    "print(f\"   üîë OpenAI API Key: {'‚úÖ Set' if OPENAI_API_KEY else '‚ùå Missing'}\")\n",
    "print(f\"   üìä RAG Document Limit: {RAG_DOCUMENT_LIMIT}\")\n",
    "print(f\"   üß™ Test Limit: {RETRIEVER_TEST_LIMIT}\")\n",
    "print(f\"   üí¨ Verbose Logging: {VERBOSE_LOGGING}\")\n",
    "print(f\"   üìù Test Query: {TEST_QUERY[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae0895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üîå WEAVIATE & LANGCHAIN SETUP\n",
    "# ========================================================================================\n",
    "\n",
    "def log_info(message, force=False):\n",
    "    \"\"\"Clean logging that only shows when needed\"\"\"\n",
    "    if VERBOSE_LOGGING or force:\n",
    "        print(f\"‚ÑπÔ∏è {message}\")\n",
    "\n",
    "def log_result(message):\n",
    "    \"\"\"Always show important results\"\"\"\n",
    "    print(f\"‚úÖ {message}\")\n",
    "\n",
    "def log_error(message):\n",
    "    \"\"\"Always show errors\"\"\"\n",
    "    print(f\"‚ùå {message}\")\n",
    "\n",
    "# Connect to Weaviate\n",
    "try:\n",
    "    client = create_weaviate_client(WEAVIATE_URL, OPENAI_API_KEY)    \n",
    "    # Quick health check - Fixed for Weaviate v4\n",
    "    try:\n",
    "        # Check if collection exists using the exists method\n",
    "        if client.collections.exists(COLLECTION_NAME):\n",
    "            collection = client.collections.get(COLLECTION_NAME)\n",
    "            total_objects = collection.aggregate.over_all(total_count=True).total_count\n",
    "            log_result(f\"Connected to Weaviate - Collection '{COLLECTION_NAME}' has {total_objects:,} documents\")\n",
    "        else:\n",
    "            log_result(f\"Connected to Weaviate - Collection '{COLLECTION_NAME}' will be created\")\n",
    "    except Exception as collection_check_error:\n",
    "        log_info(f\"Collection check failed: {collection_check_error}, but client is connected\")\n",
    "        log_result(f\"Connected to Weaviate - Collection '{COLLECTION_NAME}' status unknown\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log_error(f\"Weaviate connection failed: {e}\")\n",
    "    client = None\n",
    "\n",
    "# Setup LangChain LLM and Embeddings\n",
    "if client:\n",
    "    try:\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-5-nano\", \n",
    "            api_key=OPENAI_API_KEY, \n",
    "            temperature=1,  # obligatorio para gpt-5-nano\n",
    "            timeout=120.0,\n",
    "            model_kwargs={\n",
    "                \"verbosity\": \"medium\",\n",
    "                \"reasoning_effort\" : \"high\"\n",
    "            })\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
    "        log_result(\"LangChain LLM and Embeddings configured\")\n",
    "    except Exception as e:\n",
    "        log_error(f\"LangChain setup failed: {e}\")\n",
    "        llm = embeddings = None\n",
    "else:\n",
    "    llm = embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608762ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure collection exists\n",
    "if client:\n",
    "    collection_created = create_oxcart_collection(client, COLLECTION_NAME)\n",
    "    if collection_created:\n",
    "        log_info(f\"Collection '{COLLECTION_NAME}' ready\", force=True)\n",
    "    else:\n",
    "        log_error(f\"Failed to setup collection '{COLLECTION_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8afc88",
   "metadata": {},
   "source": [
    "## 2. Core RAG Functions\n",
    "\n",
    "Implementaci√≥n optimizada y modular de los componentes RAG centrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e9eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üõ†Ô∏è UTILITY FUNCTIONS - Content Processing and Formatting\n",
    "# ========================================================================================\n",
    "\n",
    "def compress_content(content: str, max_length: int = MAX_CONTENT_LENGTH) -> str:\n",
    "    \"\"\"Intelligently compress content while preserving key philatelic information\"\"\"\n",
    "    if len(content) <= max_length:\n",
    "        return content\n",
    "    \n",
    "    # Try to find a good break point near the limit\n",
    "    if max_length < len(content):\n",
    "        # Look for sentence endings near the limit\n",
    "        for i in range(max_length - 50, max_length):\n",
    "            if i < len(content) and content[i] in '.!?':\n",
    "                return content[:i+1] + \" [...]\"\n",
    "    \n",
    "    return content[:max_length-5] + \" [...]\"\n",
    "\n",
    "def format_philatelic_metadata(doc: Dict) -> List[str]:\n",
    "    \"\"\"Extract and format key philatelic metadata\"\"\"\n",
    "    metadata_lines = []\n",
    "    \n",
    "    if doc.get('scott_numbers'):\n",
    "        metadata_lines.append(f\"üî¢ Scott: {', '.join(doc['scott_numbers'])}\")\n",
    "    if doc.get('years'):\n",
    "        metadata_lines.append(f\"üìÖ Years: {', '.join(map(str, doc['years']))}\")\n",
    "    if doc.get('colors'):\n",
    "        metadata_lines.append(f\"üé® Colors: {', '.join(doc['colors'])}\")\n",
    "    if doc.get('catalog_systems'):\n",
    "        metadata_lines.append(f\"üìñ Catalogs: {', '.join(doc['catalog_systems'])}\")\n",
    "    if doc.get('variety_classes'):\n",
    "        metadata_lines.append(f\"üîÄ Varieties: {', '.join(doc['variety_classes'])}\")\n",
    "        \n",
    "    return metadata_lines\n",
    "\n",
    "def classify_document_authority(doc_id: str) -> tuple:\n",
    "    \"\"\"Classify document by philatelic authority level\"\"\"\n",
    "    doc_id_lower = doc_id.lower()\n",
    "    \n",
    "    if any(keyword in doc_id_lower for keyword in ['scott', 'michel', 'catalog', 'mayer']):\n",
    "        return ('catalog', '‚≠ê AUTHORITATIVE')\n",
    "    elif any(keyword in doc_id_lower for keyword in ['postal history', 'literature', 'frajola']):\n",
    "        return ('literature', 'üü¢ RELIABLE') \n",
    "    elif any(keyword in doc_id_lower for keyword in ['collection', 'nordberg', 'escalante', 'pinto']):\n",
    "        return ('collection', 'üü° SUPPLEMENTARY')\n",
    "    else:\n",
    "        return ('reference', 'üü° SUPPLEMENTARY')\n",
    "\n",
    "log_result(\"Core utility functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üìù RAG PROMPT TEMPLATE - Professional Philatelic Consultation\n",
    "# ========================================================================================\n",
    "\n",
    "philatelic_rag_template = \"\"\"You are a senior philatelic researcher and catalog specialist with expertise in Costa Rican stamps and postal history. Provide comprehensive, well-structured analysis based strictly on the source materials provided.\n",
    "\n",
    "SOURCE MATERIALS:\n",
    "{context}\n",
    "\n",
    "RESEARCH QUERY: {question}\n",
    "\n",
    "RESPONSE REQUIREMENTS:\n",
    "\n",
    "FORMATTING & STRUCTURE:\n",
    "‚Ä¢ Use clear hierarchical organization with descriptive headers\n",
    "‚Ä¢ Group related information under logical categories using ## and **bold subheadings**\n",
    "‚Ä¢ Use bullet points (‚Ä¢) for individual facts and varieties\n",
    "‚Ä¢ Include relevant emojis for major sections (üîç üìÆ üìö üéØ) to enhance readability\n",
    "‚Ä¢ Bold key terms, catalog numbers, and important details\n",
    "‚Ä¢ KEEP SECTIONS CONCISE - avoid excessive repetition or overly detailed explanations\n",
    "\n",
    "CITATION FORMAT:\n",
    "‚Ä¢ Every factual statement must include: (SOURCE, page #)\n",
    "‚Ä¢ Multiple sources: (SOURCE1, page #; SOURCE2, page #)\n",
    "‚Ä¢ Always cite catalog numbers (scott, yvert, michell, etc), varieties, dates, quantities, and technical specifications\n",
    "‚Ä¢ When quoting directly, use quotation marks around quoted text\n",
    "\n",
    "CONTENT ORGANIZATION:\n",
    "‚Ä¢ Lead with the most direct answer to the query\n",
    "‚Ä¢ Organize by catalog numbers, chronological order, or logical categories as appropriate\n",
    "‚Ä¢ Include technical specifications: dates, quantities, colors, perforations, varieties\n",
    "‚Ä¢ Provide brief historical context and collecting significance\n",
    "‚Ä¢ Note relationships between issues, varieties, or catalog entries\n",
    "‚Ä¢ Address valuation or rarity when relevant to the query\n",
    "\n",
    "RESPONSE LENGTH:\n",
    "‚Ä¢ Aim for clear, informative responses that are thorough but not excessive\n",
    "‚Ä¢ Eliminate redundant information and repetitive explanations\n",
    "‚Ä¢ Focus on the most relevant information that directly answers the query\n",
    "‚Ä¢ If information is extensive, prioritize the most important catalog entries and varieties\n",
    "\n",
    "TECHNICAL STANDARDS:\n",
    "‚Ä¢ Use precise philatelic terminology (definitive, commemorative, variety, error, overprint, etc.)\n",
    "‚Ä¢ Specify exact catalog numbers with proper formatting (Scott C216, not just C216)\n",
    "‚Ä¢ Include denomination and color details when available\n",
    "‚Ä¢ Note printing quantities, dates, and technical varieties\n",
    "‚Ä¢ Distinguish between verified catalog facts and expert opinions\n",
    "‚Ä¢ Flag incomplete or uncertain information clearly\n",
    "\n",
    "RESEARCH COMPLETENESS:\n",
    "‚Ä¢ If source materials are insufficient, state: \"The provided documents do not contain sufficient information about...\"\n",
    "‚Ä¢ Suggest what additional sources or information would be needed\n",
    "‚Ä¢ Note any gaps in catalog coverage or missing details\n",
    "\n",
    "PROFESSIONAL TONE:\n",
    "‚Ä¢ Maintain authoritative but accessible language\n",
    "‚Ä¢ Present information objectively without unnecessary qualifiers\n",
    "‚Ä¢ Use active voice and clear, direct statements\n",
    "‚Ä¢ Avoid speculation beyond what sources support\n",
    "\n",
    "RESPONSE:\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=philatelic_rag_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd87b02",
   "metadata": {},
   "source": [
    "## 3. Document Processing & RAG Chain\n",
    "\n",
    "Funciones optimizadas para formateo de contexto y ejecuci√≥n de RAG con 20 documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7bf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üìÑ OPTIMIZED DOCUMENT FORMATTING - For Academic Citation Style\n",
    "# ========================================================================================\n",
    "\n",
    "def format_docs_for_rag(docs_results: List[Dict]) -> str:\n",
    "    \"\"\"Efficient document formatting optimized for academic citation style (Document Name, p. Page)\"\"\"\n",
    "    \n",
    "    if not docs_results:\n",
    "        return \"\\nNo source documents available.\"\n",
    "    \n",
    "    # Group and sort documents by authority\n",
    "    #doc_groups = {'catalog': [], 'literature': [], 'collection': [], 'reference': []}\n",
    "    docs = []\n",
    "    \n",
    "    for i, doc in enumerate(docs_results, 1):\n",
    "        #category, reliability = classify_document_authority(doc.metadata.get('doc_id', 'Unknown'))\n",
    "        \n",
    "        doc_info = {\n",
    "            'doc_num': i,\n",
    "            'doc_id': doc.metadata.get('doc_id', 'Unknown'),\n",
    "            'page': doc.metadata.get('page_number', 'N/A'),\n",
    "            #'score': doc.metadata.get('score', 0.0),\n",
    "           # 'type': doc.metadata.get('chunk_type', 'text'),\n",
    "           # 'reliability': reliability,\n",
    "            'content': doc.page_content, #doc.get('text_original', ''), #compress_content(doc.get('text', ''), MAX_CONTENT_LENGTH),\n",
    "            #'metadata_lines': format_philatelic_metadata(doc.metadata)\n",
    "        }\n",
    "        #doc_groups[category].append(doc_info)\n",
    "        docs.append(doc_info)\n",
    "    return docs\n",
    "    \n",
    "    # Format with academic citation style\n",
    "#     formatted_sections = [f\"=== PHILATELIC SOURCES ({len(docs_results)} documents) ===\\n\"]\n",
    "    \n",
    "#     for category, category_name in [('catalog', 'CATALOGS'), ('literature', 'LITERATURE'), \n",
    "#                                   ('collection', 'COLLECTIONS'), ('reference', 'REFERENCE')]:\n",
    "        \n",
    "#         if doc_groups[category]:\n",
    "#             formatted_sections.append(f\"\\nüìö {category_name}:\")\n",
    "            \n",
    "#             for doc in doc_groups[category]:\n",
    "#                 # Academic citation format\n",
    "#                 doc_line = f\"\\n[{doc['doc_id']}] {doc['reliability']}\"\n",
    "#                 source_line = f\"üìÑ Page {doc['page']} (Confidence: {doc['score']:.3f})\"\n",
    "                \n",
    "#                 if doc['metadata_lines']:\n",
    "#                     metadata_summary = \" | \".join(doc['metadata_lines'][:3])  # Top 3 metadata\n",
    "#                     source_line += f\"\\nüéØ {metadata_summary}\"\n",
    "                \n",
    "#                 content_line = f\"üìù {doc['content']}\"\n",
    "#                 citation_line = f\"üí° Cite as: ({doc['doc_id']}, p. {doc['page']})\"\n",
    "                \n",
    "#                 formatted_sections.extend([doc_line, source_line, content_line, citation_line, \"\"])\n",
    "    \n",
    "#     # Add concise citation guide\n",
    "#     formatted_sections.append(\"\"\"\n",
    "# üîç CITATION FORMAT: (Document Name, p. Page)\n",
    "# üìö Multiple sources: (Source1, p. X; Source2, p. Y)\n",
    "# ‚≠ê Higher confidence scores = more authoritative sources\n",
    "# \"\"\")\n",
    "    \n",
    "    # return \"\\n\".join(formatted_sections)\n",
    "\n",
    "def create_rag_response(retriever_results: List[Dict], query: str) -> Dict:\n",
    "    \"\"\"Streamlined RAG chain execution with academic citation style\"\"\"\n",
    "    \n",
    "    if not retriever_results:\n",
    "        return {\"response\": \"No documents found for this query.\", \"generation_time\": 0}\n",
    "    \n",
    "    # Format context efficiently for academic citations\n",
    "    context = format_docs_for_rag(retriever_results)\n",
    "    \n",
    "    #limited_llm = llm.bind(max_tokens=4000)  # Limita la respuesta a 4000 tokens\n",
    "\n",
    "    \n",
    "    # Execute RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": lambda x: context, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = rag_chain.invoke(query)\n",
    "    generation_time = round(time.time() - start_time, 2)\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"generation_time\": generation_time,\n",
    "        \"context_docs_count\": len(retriever_results),\n",
    "        \"context_length\": len(context),\n",
    "       # \"max_tokens\": 4000  # Agregar para tracking\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "esg4b9ej0om",
   "metadata": {},
   "source": [
    "## 4. Retriever Implementations\n",
    "\n",
    "Tres enfoques de retrieval optimizados para consultas filat√©licas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5rdlgbqihtd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üîÑ BASE RETRIEVER FUNCTIONS - Shared Logic\n",
    "# ========================================================================================\n",
    "\n",
    "def format_retriever_results(raw_results: List[Dict], retriever_type: str, **metadata) -> List[Dict]:\n",
    "    \"\"\"Standardized formatting for all retriever results\"\"\"\n",
    "    formatted_results = []\n",
    "    \n",
    "    for i, result in enumerate(raw_results, 1):\n",
    "        formatted_result = {\n",
    "            \"rank\": i,\n",
    "            \"score\": result.get(\"score\", 0.0),\n",
    "            \"text\": result.get(\"text_original\", \"\"),\n",
    "            #\"chunk_id\": result.get(\"chunk_id\", \"\"),\n",
    "            #\"chunk_type\": result.get(\"chunk_type\", \"text\"),\n",
    "            \"doc_id\": result.get(\"doc_id\", \"\"),\n",
    "            # \"page_number\": result.get(\"page_number\", 0),\n",
    "            # \"catalog_systems\": result.get(\"catalog_systems\", []),\n",
    "            # \"scott_numbers\": result.get(\"scott_numbers\", []),\n",
    "            # \"years\": result.get(\"years\", []),\n",
    "            # \"colors\": result.get(\"colors\", []),\n",
    "            # \"variety_classes\": result.get(\"variety_classes\", []),\n",
    "            # \"topics_primary\": result.get(\"topics_primary\", \"\"),\n",
    "            # \"has_catalog\": result.get(\"has_catalog\", False),\n",
    "            # \"has_varieties\": result.get(\"has_varieties\", False),\n",
    "            # \"is_guanacaste\": result.get(\"is_guanacaste\", False),\n",
    "            # \"quality_score\": result.get(\"quality_score\", 0.0),\n",
    "            # \"retriever_type\": retriever_type,\n",
    "            #**metadata  # Additional metadata from specific retrievers\n",
    "        }\n",
    "        formatted_results.append(formatted_result)\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def execute_with_fallback(primary_func, fallback_func, *args, **kwargs):\n",
    "    \"\"\"Execute retriever with fallback option\"\"\"\n",
    "    try:\n",
    "        return primary_func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        log_info(f\"Primary method failed: {e}, using fallback\")\n",
    "        return fallback_func(*args, **kwargs)\n",
    "\n",
    "log_result(\"Base retriever functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vsvqvticeqe",
   "metadata": {},
   "source": [
    "### 4.1 Vector Store-Backed Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dty1ew7x51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_store_retriever(client, query: str, limit: int = RAG_DOCUMENT_LIMIT, \n",
    "                          mode: str = \"vector\", alpha: float = 0.5) -> List[Dict]:\n",
    "    \"\"\"Clean vector store retriever using LangChain + Weaviate\"\"\"\n",
    "    \n",
    "    log_info(f\"Vector Store Retriever: {mode} mode, limit={limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Create LangChain vector store\n",
    "        vector_store = WeaviateVectorStore(\n",
    "            client=client,\n",
    "            index_name=COLLECTION_NAME,\n",
    "            text_key=\"text\",\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        \n",
    "        # Configure retriever based on mode\n",
    "        search_config = {\"k\": limit}\n",
    "        if mode == \"hybrid\":\n",
    "            search_config[\"lambda_mult\"] = alpha\n",
    "            \n",
    "        retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\" if mode == \"hybrid\" else \"similarity\",\n",
    "            search_kwargs=search_config\n",
    "        )\n",
    "        \n",
    "        # Execute search\n",
    "        docs = retriever.invoke(query)\n",
    "        \n",
    "        # Convert to standard format\n",
    "        results = []\n",
    "        for doc in docs:\n",
    "            result = {\n",
    "                \"score\": doc.metadata.get(\"score\", 0.0),\n",
    "                \"text\": doc.page_content,\n",
    "                **doc.metadata\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "        log_info(f\"Retrieved {len(results)} documents\")\n",
    "        return format_retriever_results(results, \"vector_store\", search_mode=mode)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Vector store retriever failed: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lygcvhb4tt9",
   "metadata": {},
   "source": [
    "### 4.2 Multi-Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9add24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e337ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "def pretty_print_docs(docs):\n",
    "    print(\n",
    "        f\"\\n{'-' * 100}\\n\".join(\n",
    "            [f\"Document {i + 1}:\\n\\n\" + \"Score: \"+str(d.metadata.get(\"quality_score\")) + d.page_content + \"\\nSource: \"+ str(d.metadata.get(\"doc_id\")) +\"\\n\" + \" page: \" + str(d.metadata.get(\"page_number\")) for i, d in enumerate(docs)]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65505f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_stamps_with_compression(query, client, embeddings, llm, limit=100, \n",
    "                                 alpha=0.30, diversity_lambda=0.75):\n",
    "    \"\"\"\n",
    "    B√∫squeda optimizada para consultas filat√©licas con estrategia dual y compresi√≥n.\n",
    "    \n",
    "    Args:\n",
    "        query (str): La consulta sobre sellos\n",
    "        client: Cliente de Weaviate\n",
    "        embeddings: Modelo de embeddings\n",
    "        llm: Modelo de lenguaje\n",
    "        limit (int): N√∫mero m√°ximo de documentos a recuperar\n",
    "        alpha (float): Factor lambda para h√≠brido (0.30 = 30% vectorial, 70% keywords)\n",
    "        diversity_lambda (float): Factor de diversidad para MMR (0.75 = buena diversidad)\n",
    "    \n",
    "    Returns:\n",
    "        list: Documentos comprimidos optimizados para filatelia\n",
    "    \"\"\"\n",
    "    from langchain_weaviate import WeaviateVectorStore\n",
    "    from langchain.retrievers import MultiQueryRetriever, ContextualCompressionRetriever, EnsembleRetriever\n",
    "    from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "    from langchain.prompts import PromptTemplate\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    # Crear vector store\n",
    "    vector_store = WeaviateVectorStore(\n",
    "        client=client,\n",
    "        index_name=COLLECTION_NAME,\n",
    "        text_key=\"text\",\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Try to create hybrid retriever - some Weaviate versions support this\n",
    "    hybrid_kwargs = {\"k\": limit // 2}\n",
    "    if alpha is not None:\n",
    "        hybrid_kwargs[\"alpha\"] = alpha  # Use 'alpha'\n",
    "    \n",
    "    # 1. Retriever h√≠brido para precisi√≥n (captura n√∫meros exactos + contexto)\n",
    "    precision_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs=hybrid_kwargs\n",
    "    )\n",
    "    \n",
    "    # 2. Retriever MMR para diversidad (evita sellos duplicados)\n",
    "    diversity_retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": limit // 2, \"lambda_mult\": diversity_lambda}\n",
    "    )\n",
    "    \n",
    "    # 3. Ensemble con estrategia dual\n",
    "    base_retriever = EnsembleRetriever(\n",
    "        retrievers=[precision_retriever, diversity_retriever],\n",
    "        weights=[0.7, 0.3]  # 70% precisi√≥n h√≠brida + 30% diversidad\n",
    "    )\n",
    "    \n",
    "    # Prompt especializado para filatelia\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are a specialized philatelic researcher expert in stamp catalogues and varieties.\n",
    "Generate 3 strategically different versions of the question to capture comprehensive stamp information:\n",
    "\n",
    "ORIGINAL: {question}\n",
    "\n",
    "Create variations that target:\n",
    "1. CATALOG PRECISION: Focus on exact catalog numbers, dates, and technical specifications\n",
    "2. CONTEXTUAL SEARCH: Include related series, printings, varieties, and historical context  \n",
    "3. TERMINOLOGY ALTERNATIVES: Use alternative philatelic terms and synonyms\n",
    "\n",
    "Consider these philatelic elements:\n",
    "- Catalog systems: Scott, Michel, Yvert, SG, local catalogs\n",
    "- Technical terms: definitive/commemorative, variety/error, overprint/surcharge\n",
    "- Time references: issue dates, printing dates, first day covers\n",
    "- Denominations: face values, colors, perforations\n",
    "\n",
    "Alternative searches:\n",
    "1.\n",
    "2. \n",
    "3.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # MultiQueryRetriever con prompt especializado\n",
    "    multi_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=base_retriever,\n",
    "        llm=llm,\n",
    "        prompt=query_prompt,\n",
    "        parser_key=\"lines\"\n",
    "    )\n",
    "    \n",
    "    # Compresi√≥n optimizada para datos t√©cnicos\n",
    "    summarizer_llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        temperature=0.1,  # M√°s determin√≠stico para preservar n√∫meros exactos\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    compressor = LLMChainExtractor.from_llm(summarizer_llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=multi_retriever\n",
    "    )\n",
    "    \n",
    "    # Ejecutar b√∫squeda\n",
    "    results = compression_retriever.invoke(query)\n",
    "\n",
    "     # Reorder by quality_score if it exists\n",
    "    def get_quality_score(doc):\n",
    "        return doc.metadata.get('quality_score', 0.0)\n",
    "    \n",
    "    sorted_results = sorted(results, key=get_quality_score, reverse=True)\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Quien es alfonso molina o alfonso molina chacon o alfonso molina ch? Que articulos filatelicos o de estampillas ha escrito?\",\n",
    "compressed_docs = search_stamps_with_compression(\n",
    "     query,\n",
    "     client=client, \n",
    "     embeddings=embeddings, \n",
    "     llm=llm,\n",
    "     alpha=0.30,  # 30% vectorial, 70% keywords para n√∫meros exactos\n",
    "     diversity_lambda=0.75)  # 75% relevancia, 25% diversidad )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3822d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965940ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_rag_response(compressed_docs,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312839d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b294cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8607c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f232490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_retriever_langchain(client, query: str, limit: int = RAG_DOCUMENT_LIMIT, \n",
    "                          mode: str = \"hybrid\", alpha: float = 0.35) -> List[Dict]:\n",
    "      \n",
    "    # Create Weaviate vector store\n",
    "    # Use the new langchain-weaviate integration\n",
    "    vector_store = WeaviateVectorStore(\n",
    "            client=client,\n",
    "            index_name=COLLECTION_NAME,\n",
    "            text_key=\"text\",\n",
    "            embedding=embeddings\n",
    "        )\n",
    "        \n",
    "    # Configure retriever based on mode\n",
    "    search_config = {\"k\": limit}\n",
    "    if mode == \"hybrid\":\n",
    "        search_config[\"lambda_mult\"] = alpha\n",
    "        \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\" if mode == \"hybrid\" else \"similarity\",\n",
    "        search_kwargs=search_config\n",
    "    )\n",
    "    \n",
    "    # Custom prompt for philatelic queries\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an expert in philatelic (stamp collecting) terminology. \n",
    "Generate 3 different versions of the given question that would retrieve similar stamp-related documents.\n",
    "\n",
    "Consider:\n",
    "- Alternative terminology (stamp/postage, variety/error, issue/emission)\n",
    "- Date formats and variations\n",
    "- Technical philatelic terms\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Alternative questions:\n",
    "1.\n",
    "2.\n",
    "3.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create MultiQueryRetriever\n",
    "    multi_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=retriever,\n",
    "        llm=llm,\n",
    "        prompt=query_prompt,\n",
    "        parser_key=\"lines\"  # Parse line by line\n",
    "    )\n",
    "    \n",
    "    # Get documents\n",
    "    documents = multi_retriever.invoke(query)\n",
    "    \n",
    "    # Convert to your format\n",
    "    results = []\n",
    "    seen_chunks = set()\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunk_id = doc.metadata.get('chunk_id', doc.metadata.get('id'))\n",
    "        if chunk_id not in seen_chunks:\n",
    "            seen_chunks.add(chunk_id)\n",
    "            \n",
    "            result = {\n",
    "                'content': doc.page_content,\n",
    "                'chunk_id': chunk_id,\n",
    "                'score': doc.metadata.get('score', 0),\n",
    "                'source': doc.metadata.get('source', ''),\n",
    "                'metadata': doc.metadata\n",
    "            }\n",
    "            results.append(result)\n",
    "    \n",
    "    log_info(f\"Retrieved {len(results)} unique documents via LangChain\")\n",
    "    \n",
    "    return format_retriever_results(results, \"langchain_multi_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_retriever_ensamble(client, query: str, limit: int = RAG_DOCUMENT_LIMIT, \n",
    "                          mode: str = \"hybrid\", alpha: float = 0.35, \n",
    "                          rrf_k: int = 60) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Multi-query retriever with RRF (Reciprocal Rank Fusion) for philatelic research.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Weaviate vector store\n",
    "    vector_store = WeaviateVectorStore(\n",
    "        client=client,\n",
    "        index_name=COLLECTION_NAME,\n",
    "        text_key=\"text\",\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Build retrievers with proper Weaviate parameters\n",
    "    retrievers = []\n",
    "    \n",
    "    try:\n",
    "        # 1. PRECISION: Exact similarity for specific stamps/catalogs\n",
    "        precision_retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": limit * 2}\n",
    "        )\n",
    "        retrievers.append(precision_retriever)\n",
    "        log_info(\"Created precision retriever (similarity)\")\n",
    "        \n",
    "        # 2. DIVERSITY: MMR to avoid similar stamps from same series\n",
    "        # Only add lambda_mult for MMR if supported\n",
    "        mmr_kwargs = {\"k\": limit * 2}\n",
    "        if mode == \"hybrid\":\n",
    "            mmr_kwargs[\"fetch_k\"] = limit * 3  # Use fetch_k instead of lambda_mult for MMR\n",
    "        \n",
    "        diversity_retriever = vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs=mmr_kwargs\n",
    "        )\n",
    "        retrievers.append(diversity_retriever)\n",
    "        log_info(\"Created diversity retriever (MMR)\")\n",
    "        \n",
    "        # 3. HYBRID: Only if mode is hybrid and supported\n",
    "        if mode == \"hybrid\":\n",
    "            try:\n",
    "                # Try to create hybrid retriever - some Weaviate versions support this\n",
    "                hybrid_kwargs = {\"k\": limit * 2}\n",
    "                if alpha is not None:\n",
    "                    hybrid_kwargs[\"alpha\"] = alpha  # Use 'alpha' instead of 'lambda_mult'\n",
    "                \n",
    "                hybrid_retriever = vector_store.as_retriever(\n",
    "                    search_type=\"similarity\",  # Fallback to similarity\n",
    "                    search_kwargs=hybrid_kwargs\n",
    "                )\n",
    "                retrievers.append(hybrid_retriever)\n",
    "                log_info(\"Created hybrid retriever\")\n",
    "            except Exception as e:\n",
    "                print(f\"Hybrid retriever not supported: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Error creating retrievers: {e}\")\n",
    "        # Fallback to basic similarity retriever\n",
    "        basic_retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": limit * 2}\n",
    "        )\n",
    "        retrievers = [basic_retriever]\n",
    "        log_info(\"Using fallback basic retriever\")\n",
    "    \n",
    "    # ENHANCED PHILATELIC RESEARCH PROMPT (in English)\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are a professional philatelic researcher and stamp collecting expert. Generate 3 alternative research queries for comprehensive stamp documentation retrieval.\n",
    "\n",
    "PHILATELIC RESEARCH PRINCIPLES:\n",
    "- Use both common and technical terminology\n",
    "- Include catalog numbers, varieties, and printing details  \n",
    "- Consider historical context and postal history\n",
    "- Include alternative relevance from philatelic world vocabulary\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Generate exactly 3 research-focused alternative queries:\n",
    "\n",
    "1. TECHNICAL: Include Scott/Michel/SG numbers, perforations, watermarks, varieties, printing methods\n",
    "\n",
    "2. HISTORICAL: Include historical period, postal usage, commemorative events, postal rates, printing runs (plates)\n",
    "\n",
    "3. DESCRIPTIVE: Include colors, designs, denominations, condition terms, market aspects\n",
    "\n",
    "Each query must be on a separate line and focus on different philatelic research angles.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Initialize documents variable to avoid reference error\n",
    "    documents = []\n",
    "    \n",
    "    try:\n",
    "        # Create MultiQueryRetriever for each strategy\n",
    "        multi_query_retrievers = []\n",
    "        \n",
    "        for i, base_retriever in enumerate(retrievers):\n",
    "            try:\n",
    "                multi_retriever = MultiQueryRetriever.from_llm(\n",
    "                    retriever=base_retriever,\n",
    "                    llm=llm,\n",
    "                    prompt=query_prompt,\n",
    "                    parser_key=\"lines\",\n",
    "                    include_original=True\n",
    "                )\n",
    "                multi_query_retrievers.append(multi_retriever)\n",
    "                log_info(f\"Created MultiQueryRetriever #{i+1}\")\n",
    "            except Exception as e:\n",
    "                log_error(f\"Failed to create MultiQueryRetriever #{i+1}: {e}\")\n",
    "        \n",
    "        if not multi_query_retrievers:\n",
    "            raise Exception(\"No MultiQueryRetrievers could be created\")\n",
    "        \n",
    "        print(f\"\\nüîç ORIGINAL QUERY: {query}\")\n",
    "        try:\n",
    "            llm_result = llm.invoke(query_prompt.format(question=query))\n",
    "            generated_text = llm_result.content if hasattr(llm_result, 'content') else str(llm_result)\n",
    "            print(f\"ü§ñ GENERATED QUERIES:\\n{generated_text}\")\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not preview queries: {e}\")\n",
    "        \n",
    "        # Try EnsembleRetriever with RRF\n",
    "        if len(multi_query_retrievers) > 1:\n",
    "            try:\n",
    "                ensemble_retriever = EnsembleRetriever(\n",
    "                    retrievers=multi_query_retrievers,\n",
    "                    weights=None,  # Equal weights\n",
    "                    c=rrf_k,  # RRF constant\n",
    "                )\n",
    "                \n",
    "                documents = ensemble_retriever.invoke(query)\n",
    "                print(f\"EnsembleRetriever with RRF returned {len(documents)} documents\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"EnsembleRetriever failed: {e}\")\n",
    "        else:\n",
    "            # Single retriever fallback\n",
    "            print(\"Single retriever\")\n",
    "            documents = multi_query_retrievers[0].invoke(query)\n",
    "            print(f\"Single MultiQueryRetriever returned {len(documents)} documents\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"All retrieval strategies failed: {e}\")\n",
    "        # Ultimate fallback to basic retrieval\n",
    "        return []\n",
    "    \n",
    "    # Process results\n",
    "    if not documents:\n",
    "        print(\"No documents retrieved\")\n",
    "        return []\n",
    "    \n",
    "    #     # Convert to your format\n",
    "    # Convert to standard format\n",
    "    results = []\n",
    "    for doc in documents:\n",
    "        result = {\n",
    "            \"score\": doc.metadata.get(\"score\", 0.0),\n",
    "            \"text\": doc.page_content,\n",
    "            **doc.metadata\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "    log_info(f\"Retrieved {len(results)} documents\")\n",
    "        \n",
    "    log_info(f\"Retrieved {len(results)} documents\")\n",
    "    return format_retriever_results(results, \"multi_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea96e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_retriever_ensemble(client, query: str, limit: int = RAG_DOCUMENT_LIMIT, \n",
    "                                   mode: str = \"hybrid\", alpha: float = 0.45, \n",
    "                                   rrf_k: int = 60) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Multi-query retriever with proper RRF scoring for philatelic research.\n",
    "    Combines multiple retrieval strategies with proper score calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Weaviate vector store\n",
    "    vector_store = WeaviateVectorStore(\n",
    "        client=client,\n",
    "        index_name=COLLECTION_NAME,\n",
    "        text_key=\"text\",\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Enhanced philatelic query generation prompt\n",
    "    \n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an expert philatelic researcher. Analyze the user's question and generate 3-4 diverse search queries that will retrieve the most relevant information from a philatelic database.\n",
    "\n",
    "    Original question: {question}\n",
    "\n",
    "    Consider these aspects when generating queries:\n",
    "    - What specific philatelic information is being sought?\n",
    "    - What synonyms or related terms might be used in documentation?\n",
    "    - What different perspectives (collector, dealer, historian, cataloger) might describe this?\n",
    "    - What time periods, countries, or technical details are relevant?\n",
    "\n",
    "    Generate your queries as a simple numbered list. Make each query substantially different from the others to maximize coverage:\n",
    "\n",
    "    1. [First alternative query]\n",
    "    2. [Second alternative query] \n",
    "    3. [Third alternative query]\n",
    "    4. [Fourth alternative query if needed]\n",
    "\n",
    "    Focus on creating queries that would find information even if documented differently than the original question.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Generate multiple queries using LLM\n",
    "    try:\n",
    "        llm_result = llm.invoke(query_prompt.format(question=query))\n",
    "        generated_text = llm_result.content if hasattr(llm_result, 'content') else str(llm_result)\n",
    "        \n",
    "        # Parse generated queries\n",
    "        alternative_queries = [q.strip() for q in generated_text.split('\\n') \n",
    "                             if q.strip() and not q.strip().startswith(('#', '-', '*'))]\n",
    "        # Keep only numbered queries (1., 2., 3.)\n",
    "        alternative_queries = [q.split('.', 1)[1].strip() if '.' in q else q \n",
    "                             for q in alternative_queries[:3]]\n",
    "        \n",
    "        # Add original query\n",
    "        all_queries = [query] + alternative_queries\n",
    "        \n",
    "        log_info(f\"Generated {len(all_queries)} queries total\")\n",
    "        print(f\"üîç Queries: {all_queries}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Query generation failed: {e}\")\n",
    "        all_queries = [query]  # Fallback to original query only\n",
    "    \n",
    "    # Collect all results with proper scoring\n",
    "    all_results = []\n",
    "    query_results_map = {}  # Track which results came from which query\n",
    "    \n",
    "    for query_idx, search_query in enumerate(all_queries):\n",
    "        try:\n",
    "            # Strategy 1: Similarity search (includes scores)\n",
    "            similarity_results = vector_store.similarity_search_with_score(\n",
    "                query=search_query,\n",
    "                k=limit * 2\n",
    "            )\n",
    "            \n",
    "            for doc, score in similarity_results:\n",
    "                # Create unique ID for deduplication\n",
    "                doc_id = doc.metadata.get('id', hash(doc.page_content[:100]))\n",
    "                \n",
    "                result = {\n",
    "                    'doc_id': doc_id,\n",
    "                    'content': doc.page_content,\n",
    "                    'metadata': doc.metadata,\n",
    "                    'similarity_score': float(score),  # Original similarity score\n",
    "                    'query_idx': query_idx,\n",
    "                    'retrieval_method': 'similarity'\n",
    "                }\n",
    "                all_results.append(result)\n",
    "            \n",
    "            # Strategy 2: MMR search (for diversity)\n",
    "            if mode in [\"hybrid\", \"mmr\"]:\n",
    "                try:\n",
    "                    mmr_results = vector_store.max_marginal_relevance_search(\n",
    "                        query=search_query,\n",
    "                        k=limit,\n",
    "                        fetch_k=limit * 3,\n",
    "                        lambda_mult=0.5  # Balance diversity vs relevance\n",
    "                    )\n",
    "                    \n",
    "                    for rank, doc in enumerate(mmr_results):\n",
    "                        doc_id = doc.metadata.get('id', hash(doc.page_content[:100]))\n",
    "                        \n",
    "                        # MMR doesn't provide scores, so we use rank-based scoring\n",
    "                        mmr_score = 1.0 / (rank + 1)\n",
    "                        \n",
    "                        result = {\n",
    "                            'doc_id': doc_id,\n",
    "                            'content': doc.page_content,\n",
    "                            'metadata': doc.metadata,\n",
    "                            'mmr_score': mmr_score,\n",
    "                            'query_idx': query_idx,\n",
    "                            'retrieval_method': 'mmr'\n",
    "                        }\n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    log_error(f\"MMR search failed for query {query_idx}: {e}\")\n",
    "            \n",
    "            # Strategy 3: Hybrid search (if Weaviate supports it)\n",
    "            if mode == \"hybrid\" and hasattr(vector_store, 'hybrid_search'):\n",
    "                try:\n",
    "                    hybrid_results = vector_store.hybrid_search(\n",
    "                        query=search_query,\n",
    "                        alpha=alpha,  # Balance between vector and keyword search\n",
    "                        limit=limit * 2\n",
    "                    )\n",
    "                    \n",
    "                    for doc in hybrid_results:\n",
    "                        doc_id = doc.metadata.get('id', hash(doc.page_content[:100]))\n",
    "                        \n",
    "                        result = {\n",
    "                            'doc_id': doc_id,\n",
    "                            'content': doc.page_content,\n",
    "                            'metadata': doc.metadata,\n",
    "                            'hybrid_score': doc.metadata.get('score', 0.5),\n",
    "                            'query_idx': query_idx,\n",
    "                            'retrieval_method': 'hybrid'\n",
    "                        }\n",
    "                        all_results.append(result)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    log_info(f\"Hybrid search not available: {e}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            log_error(f\"Search failed for query '{search_query}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_results:\n",
    "        log_error(\"No results retrieved\")\n",
    "        return []\n",
    "    \n",
    "    # Apply Reciprocal Rank Fusion (RRF) with proper scoring\n",
    "    doc_scores = {}  # doc_id -> combined score\n",
    "    doc_data = {}    # doc_id -> document data\n",
    "    \n",
    "    # Group results by document ID\n",
    "    from collections import defaultdict\n",
    "    doc_groups = defaultdict(list)\n",
    "    \n",
    "    for result in all_results:\n",
    "        doc_groups[result['doc_id']].append(result)\n",
    "    \n",
    "    # Calculate RRF scores for each unique document\n",
    "    for doc_id, results in doc_groups.items():\n",
    "        # Initialize with the first occurrence's data\n",
    "        doc_data[doc_id] = {\n",
    "            'content': results[0]['content'],\n",
    "            'metadata': results[0]['metadata']\n",
    "        }\n",
    "        \n",
    "        # Calculate RRF score combining all retrieval methods\n",
    "        rrf_score = 0.0\n",
    "        method_scores = defaultdict(list)\n",
    "        \n",
    "        for result in results:\n",
    "            # Collect scores by method\n",
    "            if 'similarity_score' in result:\n",
    "                method_scores['similarity'].append(result['similarity_score'])\n",
    "            if 'mmr_score' in result:\n",
    "                method_scores['mmr'].append(result['mmr_score'])\n",
    "            if 'hybrid_score' in result:\n",
    "                method_scores['hybrid'].append(result['hybrid_score'])\n",
    "        \n",
    "        # Weighted combination of scores\n",
    "        weights = {\n",
    "            'similarity': 0.4,\n",
    "            'mmr': 0.3,\n",
    "            'hybrid': 0.6\n",
    "        }\n",
    "        \n",
    "        combined_score = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for method, scores in method_scores.items():\n",
    "            if scores:\n",
    "                # Use max score for each method (best performance across queries)\n",
    "                method_score = max(scores)\n",
    "                weight = weights.get(method, 0.33)\n",
    "                combined_score += method_score * weight\n",
    "                total_weight += weight\n",
    "        \n",
    "        # Normalize by total weight\n",
    "        if total_weight > 0:\n",
    "            combined_score /= total_weight\n",
    "        \n",
    "        # Apply RRF formula for final ranking\n",
    "        # RRF score = sum(1 / (k + rank)) for each query where doc appeared\n",
    "        query_appearances = set(r['query_idx'] for r in results)\n",
    "        rrf_bonus = sum(1.0 / (rrf_k + idx + 1) for idx in query_appearances)\n",
    "        \n",
    "        # Final score combines normalized score with RRF bonus\n",
    "        final_score = (combined_score * 0.7) + (rrf_bonus * 0.3)\n",
    "        \n",
    "        doc_scores[doc_id] = final_score\n",
    "    \n",
    "    # Sort by final score and create output\n",
    "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Format results\n",
    "    final_results = []\n",
    "    for doc_id, score in sorted_docs[:limit]:\n",
    "        doc_info = doc_data[doc_id]\n",
    "        \n",
    "        result = {\n",
    "            'score': float(score),  # Ensure score is included and is float\n",
    "            'text': doc_info['content'],\n",
    "            **doc_info['metadata']\n",
    "        }\n",
    "        final_results.append(result)\n",
    "    \n",
    "    log_info(f\"Retrieved {len(final_results)} documents with RRF scoring\")\n",
    "    \n",
    "    # Format for output (assuming format_retriever_results adds additional formatting)\n",
    "    return format_retriever_results(final_results, \"multi_query_rrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Research about the 1952 and 1953 Airmail Costa Rica Definitive Stamps. Mail plane type of 1934. C216-C219.\"\n",
    "results_multi_query = multi_query_retriever_ensemble(client, query, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_multi_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6887026",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_rag_response(results_multi_query,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c5b184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ts9eilrz18",
   "metadata": {},
   "source": [
    "### 4.3 Self-Querying Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4vw4bk02n45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_querying_retriever(client, query: str, limit: int = RAG_DOCUMENT_LIMIT) -> List[Dict]:\n",
    "    \"\"\"Self-querying retriever with automatic filter extraction\"\"\"\n",
    "    \n",
    "    log_info(f\"Self-Querying Retriever: extracting filters, limit={limit}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract filters using LLM\n",
    "        filter_prompt = f\"\"\"Extract philatelic search filters from this query. Respond in JSON format:\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "{{\n",
    "    \"years\": [integers or null],\n",
    "    \"scott_numbers\": [strings or null], \n",
    "    \"colors\": [strings or null],\n",
    "    \"catalog_systems\": [strings or null],\n",
    "    \"has_varieties\": boolean or null,\n",
    "    \"is_guanacaste\": boolean or null,\n",
    "    \"chunk_type\": string or null\n",
    "}}\"\"\"\n",
    "\n",
    "        response = llm.invoke([HumanMessage(content=filter_prompt)])\n",
    "        \n",
    "        try:\n",
    "            extracted_filters = json.loads(response.content.strip())\n",
    "            log_info(f\"Extracted filters: {list(extracted_filters.keys())}\")\n",
    "        except json.JSONDecodeError:\n",
    "            log_info(\"Could not parse filters, using no filters\")\n",
    "            extracted_filters = {}\n",
    "        \n",
    "        # Convert to Weaviate filters\n",
    "        weaviate_filters = {}\n",
    "        \n",
    "        if extracted_filters.get(\"years\") and isinstance(extracted_filters[\"years\"], list):\n",
    "            years = [y for y in extracted_filters[\"years\"] if isinstance(y, int)]\n",
    "            if years:\n",
    "                weaviate_filters[\"year_range\"] = [min(years), max(years)] if len(years) > 1 else [years[0], years[0]]\n",
    "        \n",
    "        if extracted_filters.get(\"scott_numbers\"):\n",
    "            scott_nums = [s for s in extracted_filters[\"scott_numbers\"] if s]\n",
    "            if scott_nums:\n",
    "                weaviate_filters[\"scott_number\"] = scott_nums[0]\n",
    "        \n",
    "        if extracted_filters.get(\"colors\"):\n",
    "            colors = [c for c in extracted_filters[\"colors\"] if c]\n",
    "            if colors:\n",
    "                weaviate_filters[\"color\"] = colors[0].lower()\n",
    "                \n",
    "        if extracted_filters.get(\"has_varieties\") is True:\n",
    "            weaviate_filters[\"has_varieties\"] = True\n",
    "            \n",
    "        if extracted_filters.get(\"is_guanacaste\") is True:\n",
    "            weaviate_filters[\"is_guanacaste\"] = True\n",
    "            \n",
    "        if extracted_filters.get(\"chunk_type\"):\n",
    "            weaviate_filters[\"chunk_type\"] = extracted_filters[\"chunk_type\"]\n",
    "        \n",
    "        # Execute search with filters\n",
    "        results = search_chunks_semantic(\n",
    "            client=client,\n",
    "            query=query,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            limit=limit,\n",
    "            filters=weaviate_filters,\n",
    "            mode=\"vector\",\n",
    "            distance_metric=\"cosine\"\n",
    "        )\n",
    "        \n",
    "        log_info(f\"Retrieved {len(results)} documents with {len(weaviate_filters)} filter(s)\")\n",
    "        \n",
    "        return format_retriever_results(\n",
    "            results, \n",
    "            \"self_querying\", \n",
    "            applied_filters=weaviate_filters,\n",
    "            filters_count=len(weaviate_filters)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Self-querying retriever failed: {e}\")\n",
    "        # Fallback to simple search\n",
    "        return execute_with_fallback(\n",
    "            lambda: format_retriever_results(\n",
    "                search_chunks_semantic(client, query, COLLECTION_NAME, limit=limit), \n",
    "                \"self_querying_fallback\"\n",
    "            ),\n",
    "            lambda: [],\n",
    "            client, query, COLLECTION_NAME, limit=limit\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c86131",
   "metadata": {},
   "source": [
    "## 5. RAG Evaluation System\n",
    "\n",
    "Evaluaci√≥n completa del pipeline RAG con 20 documentos y sistema de juez automatizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v98mny2l9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üèÜ COMPLETE RAG EVALUATION - 20 Documents per Retriever\n",
    "# ========================================================================================\n",
    "\n",
    "def execute_rag_evaluation():\n",
    "    \"\"\"Execute complete RAG pipeline with all retrievers using optimized 20-document limit\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        log_error(\"No Weaviate client available for RAG evaluation\")\n",
    "        return {}\n",
    "    \n",
    "    if SHOW_PROGRESS:\n",
    "        print(\"üöÄ Executing complete RAG evaluation...\")\n",
    "        print(f\"   üìù Query: {TEST_QUERY}\")\n",
    "        print(f\"   üìä Document limit: {RAG_DOCUMENT_LIMIT} per retriever\")\n",
    "        print(f\"   üéØ Mode: {TEST_MODE}\")\n",
    "    \n",
    "    # Define retrievers with 20-document limit\n",
    "    retrievers = [\n",
    "        (\"Vector Store\", lambda: vector_store_retriever(client, TEST_QUERY, RAG_DOCUMENT_LIMIT, TEST_MODE)),\n",
    "        (\"Multi Query\", lambda: multi_query_retriever_ensamble(client, TEST_QUERY, RAG_DOCUMENT_LIMIT)), #multi_query_retriever_langchain\n",
    "        (\"Self Querying\", lambda: self_querying_retriever(client, TEST_QUERY, RAG_DOCUMENT_LIMIT))\n",
    "    ]\n",
    "    \n",
    "    rag_results = {}\n",
    "    \n",
    "    for name, retriever_func in retrievers:\n",
    "        try:\n",
    "            if SHOW_PROGRESS:\n",
    "                print(f\"   üîÑ {name} Retriever + RAG...\")\n",
    "            \n",
    "            # Execute retriever\n",
    "            retriever_results = retriever_func()\n",
    "            \n",
    "            if not retriever_results:\n",
    "                log_error(f\"{name}: No documents retrieved\")\n",
    "                continue\n",
    "            \n",
    "            # Execute RAG chain\n",
    "            rag_result = create_rag_response(retriever_results, TEST_QUERY)\n",
    "            \n",
    "            # Store complete result\n",
    "            key = name.lower().replace(\" \", \"_\")\n",
    "            rag_results[key] = {\n",
    "                \"retriever_name\": name,\n",
    "                \"retriever_results\": retriever_results,\n",
    "                \"rag_response\": rag_result[\"response\"],\n",
    "                \"generation_time\": rag_result[\"generation_time\"],\n",
    "                \"context_docs_count\": rag_result[\"context_docs_count\"],\n",
    "                \"context_length\": rag_result.get(\"context_length\", 0),\n",
    "                \"retriever_type\": retriever_results[0].get(\"retriever_type\", \"unknown\")\n",
    "            }\n",
    "            \n",
    "            log_result(f\"{name}: {rag_result['context_docs_count']} docs ‚Üí {len(rag_result['response'])} chars ({rag_result['generation_time']}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_error(f\"{name} RAG failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if SHOW_PROGRESS and rag_results:\n",
    "        print(f\"üéâ Generated {len(rag_results)} RAG responses for comparison\")\n",
    "    \n",
    "    return rag_results\n",
    "\n",
    "# Execute RAG evaluation\n",
    "rag_evaluation_results = execute_rag_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in rag_evaluation_results['vector_store']['retriever_results']:\n",
    "#     print(x['doc_id'])\n",
    "#     print(x['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wb7ymgz1xfk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üìä RESPONSE DISPLAY & COMPARISON\n",
    "# ========================================================================================\n",
    "\n",
    "def display_rag_responses(rag_results: Dict, show_full_responses: bool = True):\n",
    "    \"\"\"Clean display of RAG responses for comparison\"\"\"\n",
    "    \n",
    "    if not rag_results:\n",
    "        log_error(\"No RAG results to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"üìä RAG RESPONSE COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Query: {TEST_QUERY}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for key, result in rag_results.items():\n",
    "        retriever_name = result[\"retriever_name\"]\n",
    "        response = result[\"rag_response\"]\n",
    "        docs_count = result[\"context_docs_count\"]\n",
    "        gen_time = result[\"generation_time\"]\n",
    "        context_size = result.get(\"context_length\", 0)\n",
    "        \n",
    "        # Icon based on retriever type\n",
    "        icon = \"üîµ\" if \"vector\" in key else \"üü°\" if \"multi\" in key else \"üü¢\"\n",
    "        \n",
    "        print(f\"\\n{icon} {retriever_name.upper()}\")\n",
    "        print(\"‚îÄ\" * 50)\n",
    "        print(f\"üìÑ Documents: {docs_count} | ‚è±Ô∏è Time: {gen_time}s | üìù Context: {context_size:,} chars\")\n",
    "        \n",
    "        if show_full_responses:\n",
    "            print(\"\\nüìù RESPONSE:\")\n",
    "            print(\"‚îÄ\" * 30)\n",
    "            print(response)\n",
    "            print(\"‚îÄ\" * 50)\n",
    "        else:\n",
    "            # Show preview only\n",
    "            preview = response[:200] + \"...\" if len(response) > 200 else response\n",
    "            print(f\"üìù Preview: {preview}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Displayed {len(rag_results)} RAG responses\")\n",
    "\n",
    "# Display responses (set to False for preview only)\n",
    "if rag_evaluation_results:\n",
    "    display_rag_responses(rag_evaluation_results, show_full_responses=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oxxvzqshuin",
   "metadata": {},
   "source": [
    "### 5.1 LLM Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3st9r3gmam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_judge_evaluation(query: str, rag_results: Dict) -> Dict:\n",
    "    \"\"\"Streamlined LLM judge evaluation with focus on key criteria\"\"\"\n",
    "    \n",
    "    if len(rag_results) < 2:\n",
    "        log_error(\"Need at least 2 responses for comparison\")\n",
    "        return {}\n",
    "    \n",
    "    log_info(\"Executing LLM judge evaluation...\")\n",
    "    \n",
    "    # Prepare responses for evaluation\n",
    "    responses_text = \"\"\n",
    "    for i, (key, result) in enumerate(rag_results.items(), 1):\n",
    "        responses_text += f\"\"\"\n",
    "RESPONSE {i} - {result['retriever_name']}:\n",
    "Documents Used: {result['context_docs_count']}\n",
    "Generation Time: {result['generation_time']}s\n",
    "Response: {result['rag_response']}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # Streamlined judge prompt focusing on key criteria\n",
    "    judge_prompt = f\"\"\"You are an expert philatelic consultant evaluating RAG system responses.\n",
    "\n",
    "ORIGINAL QUESTION: {query}\n",
    "\n",
    "Evaluate each response on these 4 key criteria (1-10 scale):\n",
    "1. **ACCURACY** - Factual correctness of philatelic information\n",
    "2. **CITATION QUALITY** - Proper use of [Doc#-P.##-Score] format\n",
    "3. **COMPLETENESS** - Addresses all aspects of the question\n",
    "4. **PHILATELIC EXPERTISE** - Professional terminology and domain knowledge\n",
    "\n",
    "RESPONSES:\n",
    "{responses_text}\n",
    "\n",
    "Provide concise evaluation in this format:\n",
    "\n",
    "SCORES:\n",
    "Response 1: Accuracy=X, Citations=X, Completeness=X, Expertise=X (Total: X/40)\n",
    "Response 2: Accuracy=X, Citations=X, Completeness=X, Expertise=X (Total: X/40)\n",
    "Response 3: Accuracy=X, Citations=X, Completeness=X, Expertise=X (Total: X/40)\n",
    "\n",
    "WINNER: Response X ({{\"retriever name\"}})\n",
    "REASONING: [Brief explanation of why this response is superior]\n",
    "\n",
    "CITATION ANALYSIS: [Count and quality of citations in each response]\"\"\"\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        judge_response = llm.invoke([HumanMessage(content=judge_prompt)])\n",
    "        evaluation_time = round(time.time() - start_time, 2)\n",
    "        \n",
    "        return {\n",
    "            \"evaluation\": judge_response.content,\n",
    "            \"evaluation_time\": evaluation_time,\n",
    "            \"responses_compared\": len(rag_results)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Judge evaluation failed: {e}\")\n",
    "        return {}\n",
    "\n",
    "def execute_judge_evaluation():\n",
    "    \"\"\"Execute LLM judge if we have results to compare\"\"\"\n",
    "    \n",
    "    if not rag_evaluation_results or len(rag_evaluation_results) < 2:\n",
    "        log_info(\"Insufficient RAG results for judge evaluation\")\n",
    "        return None\n",
    "        \n",
    "    if SHOW_PROGRESS:\n",
    "        print(f\"‚öñÔ∏è Running LLM Judge evaluation on {len(rag_evaluation_results)} responses...\")\n",
    "    \n",
    "    judge_result = llm_judge_evaluation(TEST_QUERY, rag_evaluation_results)\n",
    "    \n",
    "    if judge_result:\n",
    "        print(\"\\nüèÜ LLM JUDGE EVALUATION\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"‚è±Ô∏è Evaluation time: {judge_result['evaluation_time']}s\")\n",
    "        print(f\"üìä Responses compared: {judge_result['responses_compared']}\")\n",
    "        print(\"\\nüìã DETAILED EVALUATION:\")\n",
    "        print(\"‚îÄ\" * 50)\n",
    "        print(judge_result['evaluation'])\n",
    "        print(\"‚îÄ\" * 50)\n",
    "        \n",
    "        log_result(\"LLM Judge evaluation completed\")\n",
    "    \n",
    "    return judge_result\n",
    "\n",
    "# Execute judge evaluation\n",
    "judge_results = execute_judge_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4l3ytjxmyed",
   "metadata": {},
   "source": [
    "### 5.2 Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2k4rqjsvwzh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_citations():\n",
    "    \"\"\"Analyze citation quality in RAG responses using academic format\"\"\"\n",
    "    \n",
    "    if not rag_evaluation_results:\n",
    "        log_info(\"No RAG results available for citation analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç CITATION QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Updated pattern for academic citation format: (Document Name, p. Page)\n",
    "    citation_pattern = r'\\([^(),]+,\\s*p\\.\\s*[^)]+\\)'\n",
    "    \n",
    "    for key, result in rag_evaluation_results.items():\n",
    "        response_text = result[\"rag_response\"]\n",
    "        retriever_name = result[\"retriever_name\"]\n",
    "        \n",
    "        citations_found = re.findall(citation_pattern, response_text)\n",
    "        \n",
    "        # Rough estimate of factual claims (sentences with numbers, years, Scott numbers)\n",
    "        factual_patterns = [r'\\b\\d{4}\\b', r'Scott\\s+#?\\d+', r'\\b\\d+\\s*colones?\\b', r'\\bmint\\b', r'\\bused\\b']\n",
    "        factual_claims = 0\n",
    "        for pattern in factual_patterns:\n",
    "            factual_claims += len(re.findall(pattern, response_text, re.IGNORECASE))\n",
    "        \n",
    "        citation_rate = len(citations_found) / max(factual_claims, 1) * 100\n",
    "        \n",
    "        icon = \"üîµ\" if \"vector\" in key else \"üü°\" if \"multi\" in key else \"üü¢\"\n",
    "        \n",
    "        print(f\"\\n{icon} {retriever_name}:\")\n",
    "        print(f\"   üìä Citations found: {len(citations_found)}\")\n",
    "        print(f\"   üìã Estimated factual claims: {factual_claims}\")\n",
    "        print(f\"   üìà Citation rate: {citation_rate:.1f}%\")\n",
    "        \n",
    "        if citations_found:\n",
    "            print(f\"   üìù Sample citations: {citations_found[:2]}\")\n",
    "        \n",
    "        # Quality assessment\n",
    "        if citation_rate > 80:\n",
    "            quality = \"‚úÖ Excellent\"\n",
    "        elif citation_rate > 50:\n",
    "            quality = \"üü° Good\"\n",
    "        else:\n",
    "            quality = \"‚ùå Needs improvement\"\n",
    "            \n",
    "        print(f\"   üéØ Quality: {quality}\")\n",
    "    \n",
    "    log_result(\"Citation analysis completed for academic format\")\n",
    "\n",
    "# Run citation analysis\n",
    "analyze_citations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
