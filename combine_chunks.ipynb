{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Enriquecimiento Filatelico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS Y CONFIGURACIÓN - SISTEMA BILINGÜE\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "# Import our new bilingual philatelic logic module\n",
    "from philatelic_chunk_logic import *\n",
    "\n",
    "# Initialize main enricher\n",
    "global_enricher = SemanticEnricher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Chunks - Mejora de Contexto Semántico\n",
    "\n",
    "Lógica de backtracking para combinar chunks problemáticos donde párrafos cortos (`para`) pierden contexto al estar separados de sus headers/secciones (`sec`) correspondientes.\n",
    "\n",
    "## Objetivo\n",
    "- Mejorar la calidad de los chunks para indexación en Weaviate\n",
    "- Mantener contexto semántico combinando headers con párrafos relacionados\n",
    "- Procesar todos los PDFs de `results/parsed_jsons/` y generar versiones mejoradas en `results/final_jsons/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de directorios\n",
    "BASE_DIR = Path('.')\n",
    "PARSED_JSONS_DIR = BASE_DIR / 'results' / 'parsed_jsons'\n",
    "MARKDOWN_DIR = BASE_DIR / 'results' / 'markdown'\n",
    "FINAL_JSONS_DIR = BASE_DIR / 'results' / 'final_jsons'\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "FINAL_JSONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorios configurados:\")\n",
    "print(f\"- Input JSON: {PARSED_JSONS_DIR}\")\n",
    "print(f\"- Input Markdown: {MARKDOWN_DIR}\")\n",
    "print(f\"- Output Final: {FINAL_JSONS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Función Principal de Combinación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(sec_chunk: Dict, para_chunk: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Combina un chunk de sección (sec) con un chunk de párrafo (para).\n",
    "    \n",
    "    El chunk resultante tendrá:\n",
    "    - Texto: sec_text + \"\\n\\n\" + para_text\n",
    "    - Metadata combinada inteligentemente\n",
    "    - Grounding de ambos chunks\n",
    "    \"\"\"\n",
    "    # Crear una copia profunda del chunk de sección como base\n",
    "    combined_chunk = copy.deepcopy(sec_chunk)\n",
    "    \n",
    "    # Combinar textos\n",
    "    sec_text = sec_chunk.get('text', '').strip()\n",
    "    para_text = para_chunk.get('text', '').strip()\n",
    "    \n",
    "    combined_text = f\"{sec_text}\\n\\n{para_text}\" if sec_text and para_text else sec_text + para_text\n",
    "    combined_chunk['text'] = combined_text\n",
    "    \n",
    "    # Actualizar chunk_id para reflejar la combinación\n",
    "    sec_id = sec_chunk.get('chunk_id', '')\n",
    "    para_id = para_chunk.get('chunk_id', '')\n",
    "    combined_chunk['chunk_id'] = f\"{sec_id}+{para_id.split(':')[-1]}\"  # Formato más limpio\n",
    "    \n",
    "    # Combinar grounding (coordenadas de ubicación)\n",
    "    sec_grounding = sec_chunk.get('grounding', [])\n",
    "    para_grounding = para_chunk.get('grounding', [])\n",
    "    combined_chunk['grounding'] = sec_grounding + para_grounding\n",
    "    \n",
    "    # Combinar metadata\n",
    "    if 'metadata' in combined_chunk and 'metadata' in para_chunk:\n",
    "        # Combinar labels manteniendo 'sec' como principal\n",
    "        sec_labels = set(combined_chunk['metadata'].get('labels', []))\n",
    "        para_labels = set(para_chunk['metadata'].get('labels', []))\n",
    "        \n",
    "        # El chunk combinado mantiene 'sec' pero añade información de que incluye 'para'\n",
    "        combined_labels = list(sec_labels | para_labels)\n",
    "        combined_chunk['metadata']['labels'] = combined_labels\n",
    "        \n",
    "        # Combinar reading_order_range\n",
    "        sec_range = combined_chunk['metadata'].get('reading_order_range', [0, 0])\n",
    "        para_range = para_chunk['metadata'].get('reading_order_range', [0, 0])\n",
    "        \n",
    "        combined_range = [\n",
    "            min(sec_range[0], para_range[0]),\n",
    "            max(sec_range[1], para_range[1])\n",
    "        ]\n",
    "        combined_chunk['metadata']['reading_order_range'] = combined_range\n",
    "        \n",
    "        # Actualizar combined_chunks counter\n",
    "        combined_chunk['metadata']['combined_chunks'] = (\n",
    "            combined_chunk['metadata'].get('combined_chunks', 1) + \n",
    "            para_chunk['metadata'].get('combined_chunks', 1)\n",
    "        )\n",
    "        \n",
    "        # Promedio de quality_score si existe\n",
    "        sec_quality = combined_chunk['metadata'].get('quality_score', 0.5)\n",
    "        para_quality = para_chunk['metadata'].get('quality_score', 0.5)\n",
    "        combined_chunk['metadata']['quality_score'] = (sec_quality + para_quality) / 2\n",
    "    \n",
    "    return combined_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_chunks(doc_data: Dict) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Procesa los chunks aplicando la LÓGICA CORREGIDA DE REUTILIZACIÓN con ENRIQUECIMIENTO SEMÁNTICO:\n",
    "    - TODOS los chunks de texto buscan su header anterior\n",
    "    - Headers pueden ser REUTILIZADOS para múltiples chunks\n",
    "    - Cada chunk simplemente busca el header más cercano disponible\n",
    "    - NO hay concepto de \"headers usados\" - todos son reutilizables\n",
    "    - NUEVO: Integra enriquecimiento semántico usando philatelic_chunk_logic.py\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict, Dict]: (processed_document, stats)\n",
    "    \"\"\"\n",
    "    chunks = doc_data.get('chunks', [])\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Inicializar ChunkCombiner con el enricher global\n",
    "    from philatelic_chunk_logic import ChunkCombiner\n",
    "    combiner = ChunkCombiner(global_enricher)\n",
    "    \n",
    "    stats = {\n",
    "        'original_chunks': len(chunks),\n",
    "        'combined_chunks': 0,\n",
    "        'final_chunks': 0,\n",
    "        'combinations': [],\n",
    "        'no_header_found': 0,\n",
    "        'estimated_size_bytes': 0,\n",
    "        'largest_chunk_size': 0,\n",
    "        'header_reuse_stats': {},  # Estadísticas de reutilización de headers\n",
    "        'unique_headers_used': set(),  # Para tracking informativo\n",
    "        'semantic_enrichment_applied': 0  # NUEVO: contador de enriquecimientos semánticos\n",
    "    }\n",
    "    \n",
    "    print(f\"📄 Procesando {len(chunks)} chunks con lógica de REUTILIZACIÓN de headers + ENRIQUECIMIENTO SEMÁNTICO...\")\n",
    "    \n",
    "    for i, current_chunk in enumerate(chunks):\n",
    "        try:\n",
    "            # Verificar estructura del chunk\n",
    "            if not isinstance(current_chunk, dict):\n",
    "                print(f\"    ⚠️ Chunk {i} inválido: {type(current_chunk)}\")\n",
    "                processed_chunks.append(current_chunk)\n",
    "                continue\n",
    "            \n",
    "            current_labels = get_chunk_labels(current_chunk)\n",
    "            current_text = current_chunk.get('text', '')\n",
    "            \n",
    "            # NUEVA LÓGICA CON CLASES INTEGRADAS: Usar ChunkCombiner\n",
    "            should_combine, header_index = combiner.should_combine_chunks(current_chunk, processed_chunks)\n",
    "            \n",
    "            if should_combine and header_index is not None:\n",
    "                # Combinar con el header encontrado usando enriquecimiento semántico\n",
    "                header_chunk = processed_chunks[header_index]\n",
    "                \n",
    "                try:\n",
    "                    # NUEVO: Usar ChunkCombiner que integra enriquecimiento semántico automáticamente\n",
    "                    combined = combiner.combine_single_header_chunk(header_chunk, current_chunk)\n",
    "                    \n",
    "                    # Verificar tamaño del resultado\n",
    "                    chunk_size = estimate_chunk_size_bytes(combined)\n",
    "                    stats['largest_chunk_size'] = max(stats['largest_chunk_size'], chunk_size)\n",
    "                    \n",
    "                    # Límite de seguridad: 100KB por chunk\n",
    "                    MAX_CHUNK_SIZE = 100 * 1024  # 100KB\n",
    "                    if chunk_size > MAX_CHUNK_SIZE:\n",
    "                        print(f\"    ⚠️ Chunk {i}: Combinación muy grande ({chunk_size:,} bytes), usando original\")\n",
    "                        processed_chunks.append(current_chunk)\n",
    "                        continue\n",
    "                    \n",
    "                    stats['estimated_size_bytes'] += chunk_size\n",
    "                    \n",
    "                    # Verificar si se aplicó enriquecimiento semántico\n",
    "                    if combined.get('metadata', {}).get('semantic_enrichment_applied', False):\n",
    "                        stats['semantic_enrichment_applied'] += 1\n",
    "                    \n",
    "                except Exception as combine_error:\n",
    "                    print(f\"    ❌ Error combinando chunk {i}: {str(combine_error)[:50]}...\")\n",
    "                    processed_chunks.append(current_chunk)\n",
    "                    continue\n",
    "                \n",
    "                # ÉXITO: Registrar estadísticas (pero NO marcar header como \"usado\")\n",
    "                stats['combined_chunks'] += 1\n",
    "                \n",
    "                # Estadísticas de reutilización de headers\n",
    "                header_id = header_chunk.get('chunk_id', 'unknown')\n",
    "                header_labels = get_chunk_labels(header_chunk)\n",
    "                header_text = header_chunk.get('text', '')[:30] + '...'\n",
    "                \n",
    "                # Contar cuántas veces se reutiliza cada header\n",
    "                if header_id not in stats['header_reuse_stats']:\n",
    "                    stats['header_reuse_stats'][header_id] = {\n",
    "                        'labels': header_labels,\n",
    "                        'text_preview': header_text,\n",
    "                        'reuse_count': 0,\n",
    "                        'combined_with': []\n",
    "                    }\n",
    "                \n",
    "                stats['header_reuse_stats'][header_id]['reuse_count'] += 1\n",
    "                stats['header_reuse_stats'][header_id]['combined_with'].append({\n",
    "                    'chunk_id': current_chunk.get('chunk_id', 'unknown'),\n",
    "                    'labels': current_labels\n",
    "                })\n",
    "                \n",
    "                # Tracking único de headers (para estadísticas)\n",
    "                stats['unique_headers_used'].add(header_id)\n",
    "                \n",
    "                # Registrar ejemplo de combinación (simplificado)\n",
    "                stats['combinations'].append({\n",
    "                    'content_id': current_chunk.get('chunk_id', 'unknown')[:15],\n",
    "                    'content_labels': ', '.join(current_labels),\n",
    "                    'header_labels': ', '.join(header_labels),\n",
    "                    'header_preview': header_text,\n",
    "                    'combined_size_kb': round(chunk_size / 1024, 1),\n",
    "                    'header_reuse_count': stats['header_reuse_stats'][header_id]['reuse_count'],\n",
    "                    'semantic_enriched': combined.get('metadata', {}).get('semantic_enrichment_applied', False)  # NUEVO\n",
    "                })\n",
    "                \n",
    "                # Añadir chunk combinado\n",
    "                processed_chunks.append(combined)\n",
    "                \n",
    "                enrichment_status = \"🎯\" if combined.get('metadata', {}).get('semantic_enrichment_applied', False) else \"📝\"\n",
    "                print(f\"    ✅ Chunk {i} ({', '.join(current_labels)}): combinado con {', '.join(header_labels)} (reutilización #{stats['header_reuse_stats'][header_id]['reuse_count']}) {enrichment_status}\")\n",
    "                \n",
    "            else:\n",
    "                # No se pudo combinar - aplicar enriquecimiento semántico individual\n",
    "                if any(label in ['para', 'marginalia', 'tab'] for label in current_labels):\n",
    "                    try:\n",
    "                        # NUEVO: Aplicar enriquecimiento semántico aún sin header\n",
    "                        enriched_chunk = global_enricher.enrich_chunk_advanced_bilingual(current_chunk.copy())\n",
    "                        if enriched_chunk.get('metadata', {}).get('quality_score', 0) > current_chunk.get('metadata', {}).get('quality_score', 0):\n",
    "                            current_chunk = enriched_chunk\n",
    "                            stats['semantic_enrichment_applied'] += 1\n",
    "                            print(f\"    🎯 Chunk {i} ({', '.join(current_labels)}): enriquecimiento semántico individual aplicado\")\n",
    "                    except Exception as enrich_error:\n",
    "                        print(f\"    ⚠️ Error enriqueciendo chunk {i}: {str(enrich_error)[:30]}...\")\n",
    "                    \n",
    "                    stats['no_header_found'] += 1\n",
    "                    print(f\"    📝 Chunk {i} ({', '.join(current_labels)}): sin header disponible\")\n",
    "                \n",
    "                # Añadir chunk sin combinar (pero posiblemente enriquecido)\n",
    "                chunk_size = estimate_chunk_size_bytes(current_chunk)\n",
    "                stats['estimated_size_bytes'] += chunk_size\n",
    "                stats['largest_chunk_size'] = max(stats['largest_chunk_size'], chunk_size)\n",
    "                processed_chunks.append(current_chunk)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Error general\n",
    "            error_info = f\"{type(e).__name__}: {str(e)[:50]}...\"\n",
    "            print(f\"    ❌ Error en chunk {i}: {error_info}\")\n",
    "            \n",
    "            processed_chunks.append(current_chunk)\n",
    "    \n",
    "    stats['final_chunks'] = len(processed_chunks)\n",
    "    \n",
    "    # Verificar eficiencia del algoritmo\n",
    "    size_mb = stats['estimated_size_bytes'] / (1024 * 1024)\n",
    "    combination_rate = (stats['combined_chunks'] / stats['original_chunks']) * 100 if stats['original_chunks'] > 0 else 0\n",
    "    enrichment_rate = (stats['semantic_enrichment_applied'] / stats['original_chunks']) * 100 if stats['original_chunks'] > 0 else 0\n",
    "    \n",
    "    # Estadísticas de reutilización\n",
    "    total_reuses = sum(header_stats['reuse_count'] for header_stats in stats['header_reuse_stats'].values())\n",
    "    unique_headers_count = len(stats['unique_headers_used'])\n",
    "    avg_reuse_per_header = total_reuses / unique_headers_count if unique_headers_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 RESULTADOS:\")\n",
    "    print(f\"  Chunks combinados: {stats['combined_chunks']}/{stats['original_chunks']} ({combination_rate:.1f}%)\")\n",
    "    print(f\"  Enriquecimiento semántico: {stats['semantic_enrichment_applied']}/{stats['original_chunks']} ({enrichment_rate:.1f}%)\")  # NUEVO\n",
    "    print(f\"  Headers únicos utilizados: {unique_headers_count}\")\n",
    "    print(f\"  Promedio reutilización por header: {avg_reuse_per_header:.1f}\")\n",
    "    print(f\"  Sin header encontrado: {stats['no_header_found']}\")\n",
    "    print(f\"  Tamaño estimado: {size_mb:.2f} MB\")\n",
    "    print(f\"  Chunk más grande: {stats['largest_chunk_size']//1024}KB\")\n",
    "    \n",
    "    # Mostrar headers más reutilizados (top 3)\n",
    "    most_reused = sorted(stats['header_reuse_stats'].items(), \n",
    "                        key=lambda x: x[1]['reuse_count'], reverse=True)[:3]\n",
    "    \n",
    "    if most_reused:\n",
    "        print(f\"  Headers más reutilizados:\")\n",
    "        for header_id, header_info in most_reused:\n",
    "            reuse_count = header_info['reuse_count']\n",
    "            labels = ', '.join(header_info['labels'])\n",
    "            preview = header_info['text_preview']\n",
    "            print(f\"    - {labels}: \\\"{preview}\\\" (usado {reuse_count} veces)\")\n",
    "    \n",
    "    # Crear documento procesado\n",
    "    processed_doc = copy.deepcopy(doc_data)\n",
    "    processed_doc['chunks'] = processed_chunks\n",
    "    \n",
    "    # Metadata del documento\n",
    "    if 'metadata' not in processed_doc:\n",
    "        processed_doc['metadata'] = {}\n",
    "    \n",
    "    processed_doc['metadata']['processing_date'] = datetime.now().isoformat()\n",
    "    processed_doc['metadata']['header_reuse_strategy_applied'] = True\n",
    "    processed_doc['metadata']['semantic_enrichment_applied'] = True  # NUEVO\n",
    "    processed_doc['metadata']['original_chunk_count'] = stats['original_chunks']\n",
    "    processed_doc['metadata']['final_chunk_count'] = stats['final_chunks']\n",
    "    processed_doc['metadata']['combined_chunk_count'] = stats['combined_chunks']\n",
    "    processed_doc['metadata']['semantic_enriched_count'] = stats['semantic_enrichment_applied']  # NUEVO\n",
    "    processed_doc['metadata']['combination_rate_percent'] = round(combination_rate, 1)\n",
    "    processed_doc['metadata']['enrichment_rate_percent'] = round(enrichment_rate, 1)  # NUEVO\n",
    "    processed_doc['metadata']['unique_headers_used'] = unique_headers_count\n",
    "    processed_doc['metadata']['avg_header_reuse'] = round(avg_reuse_per_header, 1)\n",
    "    processed_doc['metadata']['estimated_size_mb'] = round(size_mb, 2)\n",
    "    \n",
    "    return processed_doc, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos *_philatelic.json aplicando header backtracking + enriquecimiento semántico.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary con estadísticas completas del procesamiento\n",
    "    \"\"\"\n",
    "    # Obtener todos los archivos philatelic usando la función del módulo\n",
    "    philatelic_files = get_all_philatelic_files()\n",
    "    \n",
    "    if not philatelic_files:\n",
    "        print(\"❌ No se encontraron archivos *_philatelic.json en results/parsed_jsons/\")\n",
    "        return {\n",
    "            'total_files': 0,\n",
    "            'processed_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'errors': [],\n",
    "            'total_original_chunks': 0,\n",
    "            'total_final_chunks': 0,\n",
    "            'total_combinations': 0,\n",
    "            'total_semantic_enrichments': 0,\n",
    "            'total_size_mb': 0.0\n",
    "        }\n",
    "    \n",
    "    print(f\"📁 Encontrados {len(philatelic_files)} archivos para procesar\")\n",
    "    \n",
    "    # Estadísticas globales\n",
    "    global_stats = {\n",
    "        'total_files': len(philatelic_files),\n",
    "        'processed_files': 0,\n",
    "        'failed_files': 0,\n",
    "        'errors': [],\n",
    "        'total_original_chunks': 0,\n",
    "        'total_final_chunks': 0,\n",
    "        'total_combinations': 0,\n",
    "        'total_semantic_enrichments': 0,\n",
    "        'total_size_mb': 0.0,\n",
    "        'file_details': [],\n",
    "        'size_warnings': [],\n",
    "        'efficiency_report': {}\n",
    "    }\n",
    "    \n",
    "    for i, file_path in enumerate(philatelic_files):\n",
    "        try:\n",
    "            doc_id = extract_doc_id_from_filename(file_path.name)\n",
    "            print(f\"\\n[{i+1}/{len(philatelic_files)}] Procesando {doc_id}...\")\n",
    "            \n",
    "            # Cargar archivo JSON\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            \n",
    "            original_chunks_count = len(doc_data.get('chunks', []))\n",
    "            \n",
    "            # Procesar el documento\n",
    "            processed_doc, file_stats = process_document_chunks(doc_data)\n",
    "            \n",
    "            # Generar nombre de archivo de salida\n",
    "            output_filename = f\"{doc_id}_final.json\"\n",
    "            output_path = FINAL_JSONS_DIR / output_filename\n",
    "            \n",
    "            # Verificar tamaño antes de guardar\n",
    "            estimated_size_mb = file_stats.get('estimated_size_mb', 0)\n",
    "            if estimated_size_mb > 50:  # Warning para archivos grandes\n",
    "                global_stats['size_warnings'].append({\n",
    "                    'file': doc_id,\n",
    "                    'final_mb': estimated_size_mb,\n",
    "                    'reason': 'Large file size'\n",
    "                })\n",
    "            \n",
    "            # Guardar archivo procesado\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_doc, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Actualizar estadísticas globales\n",
    "            global_stats['processed_files'] += 1\n",
    "            global_stats['total_original_chunks'] += file_stats['original_chunks']\n",
    "            global_stats['total_final_chunks'] += file_stats['final_chunks']\n",
    "            global_stats['total_combinations'] += file_stats['combined_chunks']\n",
    "            global_stats['total_semantic_enrichments'] += file_stats.get('semantic_enrichment_applied', 0)\n",
    "            global_stats['total_size_mb'] += estimated_size_mb\n",
    "            \n",
    "            # Detalles por archivo\n",
    "            global_stats['file_details'].append({\n",
    "                'doc_id': doc_id,\n",
    "                'original_chunks': file_stats['original_chunks'],\n",
    "                'final_chunks': file_stats['final_chunks'],\n",
    "                'combinations': file_stats['combined_chunks'],\n",
    "                'semantic_enrichments': file_stats.get('semantic_enrichment_applied', 0),\n",
    "                'file_size_mb': estimated_size_mb\n",
    "            })\n",
    "            \n",
    "            print(f\"✅ {doc_id}: {file_stats['combined_chunks']} combinaciones + {file_stats.get('semantic_enrichment_applied', 0)} enriquecimientos → {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            global_stats['failed_files'] += 1\n",
    "            global_stats['errors'].append({\n",
    "                'file': file_path.name,\n",
    "                'error': error_msg\n",
    "            })\n",
    "            \n",
    "            print(f\"❌ Error procesando {file_path.name}: {error_msg[:60]}...\")\n",
    "            continue\n",
    "    \n",
    "    # Generar reporte de eficiencia\n",
    "    if global_stats['processed_files'] > 0:\n",
    "        avg_combinations = global_stats['total_combinations'] / global_stats['processed_files']\n",
    "        avg_enrichments = global_stats['total_semantic_enrichments'] / global_stats['processed_files']\n",
    "        avg_size = global_stats['total_size_mb'] / global_stats['processed_files']\n",
    "        \n",
    "        global_stats['efficiency_report'] = {\n",
    "            'avg_combinations_per_file': round(avg_combinations, 1),\n",
    "            'avg_enrichments_per_file': round(avg_enrichments, 1),\n",
    "            'avg_file_size_mb': round(avg_size, 1),\n",
    "            'memory_efficient_files': sum(1 for details in global_stats['file_details'] \n",
    "                                        if details['file_size_mb'] < 10),\n",
    "            'combination_success_rate': round((global_stats['total_combinations'] / \n",
    "                                             max(global_stats['total_original_chunks'], 1)) * 100, 1),\n",
    "            'enrichment_success_rate': round((global_stats['total_semantic_enrichments'] / \n",
    "                                            max(global_stats['total_original_chunks'], 1)) * 100, 1)\n",
    "        }\n",
    "    \n",
    "    return global_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TEST DE CHUNK REAL CON ENRIQUECIMIENTO COMPLETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 TEST CHUNK REAL - ENRIQUECIMIENTO SEMÁNTICO COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chunk real de tabla filatélica para testing\n",
    "test_table_chunk = {\n",
    "    \"chunk_id\": \"Oxcart253:025:3-3:0\",\n",
    "    \"chunk_type\": \"text\",\n",
    "    \"text\": \"0\\t1\\t2\\r\\nScott 147\\tGR20 Type G7 2c used perf 12x11.5 wove paper lithographed mint never hinged very fine\\t25.00\\r\\nMichel 23a\\tGR21-28 Type G13 mostly used (ex-Saenz collection) comb perf yellow gum\\t40.00\\r\\nYvert 45\\tGR29-36 Type G14 postally used (ex-Saenz collection) thin spot crease\\t40.00\\r\\n66\\tGR38-45 Type G8 mint lightly hinged (ex-Colonel Green collection) superb centering\\t20.00\\r\\n67\\tGR47 Type G9 block of 4 mint no gum imperforate (ex-Alvarez) 1885 coat of arms\\t5.00\\r\\n68\\tG47-54 Type G9 not used (ex-Colonel Green collection) watermark inverted\\t25.00\\r\\n70\\tGR pair position 24 & 25 mint hinged gum signed Peralta 1880s cathedral design\\t75.00\\r\\n71\\tGR pair position 27 & 28 mint never hinged original gum signed Peralta engraved\\t75.00\\r\\n72\\tG3-4 pair not used gum signed Peralta and Napier laid paper extremely fine $50 USD\\t75.00\\r\\n80\\tRevenue Guanacaste Overprints Collection lots 48 to 80 inverted overprint color error\\t869.-\\r\\n\",\n",
    "    \"grounding\": [{\"page\": 25, \"box\": None}],\n",
    "    \"metadata\": {\n",
    "        \"labels\": [\"tab\"],\n",
    "        \"reading_order_range\": [3, 3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Header simulado (como sería en el procesamiento real)\n",
    "test_header_chunk = {\n",
    "    \"chunk_id\": \"Oxcart253:025:2-2:0\",\n",
    "    \"chunk_type\": \"text\", \n",
    "    \"text\": \"COSTA RICA PHILATELIC AUCTION CATALOG 1885-1891 GUANACASTE OVERPRINT PERIOD\",\n",
    "    \"metadata\": {\n",
    "        \"labels\": [\"sec\"],\n",
    "        \"reading_order_range\": [2, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📋 CHUNK ORIGINAL (solo tabla):\")\n",
    "print(f\"Texto: {test_table_chunk['text'][:100]}...\")\n",
    "print(f\"Longitud: {len(test_table_chunk['text'])} caracteres\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🔍 APLICANDO ENRIQUECIMIENTO SEMÁNTICO...\")\n",
    "\n",
    "# 1. ENRIQUECIMIENTO INDIVIDUAL del chunk tabla\n",
    "enriched_chunk = global_enricher.enrich_chunk_advanced_bilingual(test_table_chunk.copy())\n",
    "entities = enriched_chunk.get('metadata', {}).get('entities', {})\n",
    "\n",
    "print(f\"\\n📊 ENTIDADES EXTRAÍDAS:\")\n",
    "print(f\"Quality Score: {enriched_chunk.get('metadata', {}).get('quality_score', 0):.2f}\")\n",
    "\n",
    "# Mostrar solo las entidades importantes encontradas\n",
    "if entities.get('catalog'):\n",
    "    sistemas = ', '.join(f\"{c['system']} {c['number']}\" for c in entities['catalog'][:3])\n",
    "    print(f\"Catálogos: {len(entities['catalog'])}, sistemas → {sistemas}\")\n",
    "if entities.get('condition'):\n",
    "    print(f\"Condiciones: {entities['condition']}\")\n",
    "if entities.get('perforation'):\n",
    "    print(f\"Perforación: {entities['perforation']}\")\n",
    "if entities.get('varieties'):\n",
    "    print(f\"EFO: {len(entities['varieties'])} variedades → {[v['label'] for v in entities['varieties']]}\")\n",
    "if entities.get('prices'):\n",
    "    valores = ', '.join(f\"{p['amount']} {p['currency']}\" for p in entities['prices'][:3])\n",
    "    print(f\"Precios: {len(entities['prices'])} valores → {valores}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🎯 COMBINACIÓN CON HEADER + ENRIQUECIMIENTO AVANZADO\")\n",
    "\n",
    "# 2. COMBINACIÓN CON HEADER usando ChunkCombiner (incluye enriquecimiento automático)\n",
    "combiner = ChunkCombiner(global_enricher)\n",
    "combined_chunk = combiner.combine_single_header_chunk(test_header_chunk, test_table_chunk)\n",
    "\n",
    "print(f\"\\n📝 TEXTO ORIGINAL (Header + Tabla):\")\n",
    "original_combined = f\"{test_header_chunk['text']}\\\\n\\\\n{test_table_chunk['text']}\"\n",
    "print(f\"Longitud: {len(original_combined)} caracteres\")\n",
    "\n",
    "print(f\"\\n🚀 TEXTO FINAL ENRIQUECIDO:\")\n",
    "final_text = combined_chunk.get('text', '')\n",
    "print(f\"Longitud: {len(final_text)} caracteres\")\n",
    "print(f\"Incremento: {((len(final_text) - len(original_combined)) / len(original_combined) * 100):.0f}%\")\n",
    "\n",
    "# Mostrar el resultado final de manera legible\n",
    "print(f\"\\n📖 RESULTADO FINAL:\")\n",
    "print(\"-\" * 40)\n",
    "# Solo mostrar primeras líneas del texto enriquecido para ver la estructura\n",
    "lines = final_text.split('\\\\n')\n",
    "for i, line in enumerate(lines[:10]):  # Primeras 10 líneas\n",
    "    if line.strip():\n",
    "        print(f\"{line.strip()}\")\n",
    "\n",
    "if len(lines) > 10:\n",
    "    print(f\"... [{len(lines)-10} líneas adicionales de enriquecimiento]\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\n🎉 RESUMEN:\")\n",
    "print(f\"✅ Chunk original: {len(test_table_chunk['text'])} chars\")\n",
    "print(f\"✅ Con header: {len(original_combined)} chars\") \n",
    "print(f\"✅ Completamente enriquecido: {len(final_text)} chars\")\n",
    "print(f\"✅ Mejora total: {((len(final_text) - len(test_table_chunk['text'])) / len(test_table_chunk['text']) * 100):.0f}%\")\n",
    "print(f\"✅ Semantic enrichment aplicado: {combined_chunk.get('metadata', {}).get('semantic_enrichment_applied', False)}\")\n",
    "\n",
    "# Verificar qué tipos de enriquecimiento se aplicaron\n",
    "enrichment_applied = []\n",
    "if \"Catálogos internacionales:\" in final_text:\n",
    "    enrichment_applied.append(\"Catálogos\")\n",
    "if \"Especificaciones técnicas:\" in final_text:\n",
    "    enrichment_applied.append(\"Técnicas\")\n",
    "if \"Condición:\" in final_text:\n",
    "    enrichment_applied.append(\"Condición\")\n",
    "if \"Precios:\" in final_text:\n",
    "    enrichment_applied.append(\"Precios\")\n",
    "if \"Guanacaste\" in final_text:\n",
    "    enrichment_applied.append(\"Contexto CR\")\n",
    "\n",
    "print(f\"✅ Tipos de enriquecimiento aplicados: {', '.join(enrichment_applied) if enrichment_applied else 'Básico'}\")\n",
    "\n",
    "print(\"\\\\n🎯 ¡Listo para RAG! El chunk ahora tiene contexto completo y metadata rica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Procesamiento Masivo Todos los PDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"🚀 INICIANDO PROCESAMIENTO MASIVO - NUEVA ESTRATEGIA PDF + ENRIQUECIMIENTO SEMÁNTICO\")\n",
    "print(\"TODOS LOS TEXTOS BUSCARÁN SU HEADER/SEC/SUB_SEC ANTERIOR + PHILATELIC ENRICHMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Ejecutar procesamiento con manejo de errores robusto\n",
    "try:\n",
    "    results = process_all_documents()\n",
    "    processing_successful = True\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ ERROR DURANTE EL PROCESAMIENTO:\")\n",
    "    print(f\"Tipo: {type(e).__name__}\")\n",
    "    print(f\"Mensaje: {str(e)}\")\n",
    "    processing_successful = False\n",
    "    results = {\n",
    "        'total_files': 0,\n",
    "        'processed_files': 0,\n",
    "        'failed_files': 1,\n",
    "        'errors': [{'file': 'procesamiento_general', 'error': str(e)}],\n",
    "        'total_semantic_enrichments': 0\n",
    "    }\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 RESUMEN FINAL DEL PROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if processing_successful:\n",
    "    print(f\"✅ ÉXITO: Procesamiento completado con nueva estrategia PDF + ENRIQUECIMIENTO SEMÁNTICO\")\n",
    "    \n",
    "    # Estadísticas principales\n",
    "    print(f\"\\n📁 ARCHIVOS:\")\n",
    "    print(f\"  Total encontrados:     {results.get('total_files', 0)}\")\n",
    "    print(f\"  Procesados con éxito:  {results.get('processed_files', 0)}\")\n",
    "    print(f\"  Fallidos:              {results.get('failed_files', 0)}\")\n",
    "    \n",
    "    print(f\"\\n📊 CHUNKS:\")\n",
    "    print(f\"  Chunks originales:     {results.get('total_original_chunks', 0):,}\")\n",
    "    print(f\"  Chunks finales:        {results.get('total_final_chunks', 0):,}\")\n",
    "    print(f\"  Combinaciones:         {results.get('total_combinations', 0):,}\")\n",
    "    print(f\"  Enriquecimientos:      {results.get('total_semantic_enrichments', 0):,}\")  # NUEVO\n",
    "    \n",
    "    # Calcular estadísticas\n",
    "    if results.get('total_original_chunks', 0) > 0:\n",
    "        combination_rate = (results.get('total_combinations', 0) / results.get('total_original_chunks', 0)) * 100\n",
    "        enrichment_rate = (results.get('total_semantic_enrichments', 0) / results.get('total_original_chunks', 0)) * 100  # NUEVO\n",
    "        print(f\"  Tasa combinación:      {combination_rate:.1f}%\")\n",
    "        print(f\"  Tasa enriquecimiento:  {enrichment_rate:.1f}%\")  # NUEVO\n",
    "    \n",
    "    print(f\"\\n💾 TAMAÑOS:\")\n",
    "    print(f\"  Tamaño total:          {results.get('total_size_mb', 0):.1f} MB\")\n",
    "    \n",
    "    if results.get('processed_files', 0) > 0:\n",
    "        avg_size = results.get('total_size_mb', 0) / results.get('processed_files', 0)\n",
    "        print(f\"  Tamaño promedio:       {avg_size:.1f} MB por archivo\")\n",
    "    \n",
    "    print(f\"\\n⏱️ RENDIMIENTO:\")\n",
    "    print(f\"  Tiempo total:          {processing_time:.1f} segundos\")\n",
    "    \n",
    "    if results.get('processed_files', 0) > 0:\n",
    "        avg_time = processing_time / results.get('processed_files', 0)\n",
    "        print(f\"  Tiempo promedio:       {avg_time:.1f}s por archivo\")\n",
    "    \n",
    "    # Reportar eficiencia con enriquecimiento semántico\n",
    "    efficiency_report = results.get('efficiency_report', {})\n",
    "    if efficiency_report:\n",
    "        print(f\"\\n📈 EFICIENCIA:\")\n",
    "        print(f\"  Archivos eficientes:     {efficiency_report.get('memory_efficient_files', 0)}\")\n",
    "        print(f\"  Combinaciones/archivo:   {efficiency_report.get('avg_combinations_per_file', 0)}\")\n",
    "        print(f\"  Enriquecimientos/archivo: {efficiency_report.get('avg_enrichments_per_file', 0)}\")  # NUEVO\n",
    "        print(f\"  Tasa éxito combinación:  {efficiency_report.get('combination_success_rate', 0):.1f}%\")\n",
    "        print(f\"  Tasa éxito enriquecimiento: {efficiency_report.get('enrichment_success_rate', 0):.1f}%\")  # NUEVO\n",
    "    \n",
    "    # Mostrar archivos más exitosos (combinando combinaciones + enriquecimientos)\n",
    "    file_details = results.get('file_details', [])\n",
    "    if file_details:\n",
    "        print(f\"\\n🏆 TOP 5 ARCHIVOS CON MAYOR MEJORA TOTAL:\")\n",
    "        # Ordenar por la suma de combinaciones + enriquecimientos\n",
    "        top_files = sorted(file_details, \n",
    "                          key=lambda x: x.get('combinations', 0) + x.get('semantic_enrichments', 0), \n",
    "                          reverse=True)[:5]\n",
    "        \n",
    "        for i, file_info in enumerate(top_files, 1):\n",
    "            combinations = file_info.get('combinations', 0)\n",
    "            enrichments = file_info.get('semantic_enrichments', 0)\n",
    "            total_chunks = file_info.get('original_chunks', 0)\n",
    "            file_size = file_info.get('file_size_mb', 0)\n",
    "            total_improvements = combinations + enrichments\n",
    "            \n",
    "            if total_chunks > 0:\n",
    "                improvement_rate = (total_improvements / total_chunks) * 100\n",
    "                print(f\"  {i}. {file_info.get('doc_id', 'unknown'):10} - {combinations} comb + {enrichments} enr = {total_improvements} mejoras ({improvement_rate:4.1f}%) - {file_size:.1f}MB\")\n",
    "    \n",
    "    # Advertencias y errores\n",
    "    size_warnings = results.get('size_warnings', [])\n",
    "    if size_warnings:\n",
    "        print(f\"\\n⚠️ ADVERTENCIAS DE TAMAÑO ({len(size_warnings)}):\")\n",
    "        for warning in size_warnings[:3]:  # Solo primeras 3\n",
    "            print(f\"  - {warning.get('file', 'unknown')}: {warning.get('final_mb', 0):.1f}MB ({warning.get('reason', 'unknown')})\")\n",
    "    \n",
    "    errors = results.get('errors', [])\n",
    "    if errors:\n",
    "        print(f\"\\n❌ ERRORES ({len(errors)}):\")\n",
    "        for error in errors[:3]:  # Solo primeros 3\n",
    "            print(f\"  - {error.get('file', 'unknown')}: {str(error.get('error', 'unknown'))[:60]}...\")\n",
    "    \n",
    "    print(f\"\\n📂 ARCHIVOS DE SALIDA:\")\n",
    "    print(f\"  Directorio: {FINAL_JSONS_DIR}\")\n",
    "    print(f\"  Formato: OXCART##_final.json\")\n",
    "    print(f\"  ✅ Listos para indexación en Weaviate\")\n",
    "    \n",
    "    print(f\"\\n🎯 NUEVA ESTRATEGIA PDF + ENRIQUECIMIENTO SEMÁNTICO - RESULTADOS:\")\n",
    "    print(f\"✅ Chunks como 'POSITION 2: There are two blue dots...'\")\n",
    "    print(f\"✅ AHORA tienen su contexto de header/sección anterior\")\n",
    "    print(f\"✅ PLUS: Enriquecimiento semántico filatélico avanzado aplicado\")\n",
    "    print(f\"✅ Detección de catálogos internacionales (Scott, Michel, Yvert, etc.)\")\n",
    "    print(f\"✅ Clasificación EFO (Errores, variedades, oddities)\")\n",
    "    print(f\"✅ Especificaciones técnicas (perforación, papel, impresión)\")\n",
    "    print(f\"✅ Evaluación de condición (mint/used, centering, defects)\")\n",
    "    print(f\"✅ Contexto Costa Rica (período Guanacaste, historia)\")\n",
    "    print(f\"✅ Contexto semántico completo para RAG queries\")\n",
    "    print(f\"✅ Tamaños controlados (no más archivos enormes)\")\n",
    "    print(f\"✅ Deduplicación inteligente aplicada\")\n",
    "\n",
    "else:\n",
    "    print(f\"❌ FALLO: El procesamiento no se pudo completar\")\n",
    "    print(f\"⏱️ Tiempo antes del fallo: {processing_time:.1f} segundos\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "if processing_successful and results.get('processed_files', 0) > 0:\n",
    "    total_improvements = results.get('total_combinations', 0) + results.get('total_semantic_enrichments', 0)\n",
    "    print(f\"🎉 ¡PROCESAMIENTO COMPLETADO CON ÉXITO!\")\n",
    "    print(f\"La nueva estrategia PDF + ENRIQUECIMIENTO SEMÁNTICO funcionó perfectamente.\")\n",
    "    print(f\"{total_improvements:,} chunks totales mejorados ({results.get('total_combinations', 0):,} combinaciones + {results.get('total_semantic_enrichments', 0):,} enriquecimientos).\")\n",
    "    print(f\"Los textos finales son significativamente más ricos para embeddings RAG.\")\n",
    "else:\n",
    "    print(f\"⚠️ Procesamiento completado con limitaciones.\")\n",
    "    print(f\"Revisar errores mostrados arriba.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
