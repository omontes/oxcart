{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Enriquecimiento Filatelico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS Y CONFIGURACI√ìN - SISTEMA BILING√úE\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "# Import our new bilingual philatelic logic module\n",
    "from philatelic_chunk_logic import *\n",
    "\n",
    "# Initialize main enricher\n",
    "global_enricher = SemanticEnricher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Chunks - Mejora de Contexto Sem√°ntico\n",
    "\n",
    "L√≥gica de backtracking para combinar chunks problem√°ticos donde p√°rrafos cortos (`para`) pierden contexto al estar separados de sus headers/secciones (`sec`) correspondientes.\n",
    "\n",
    "## Objetivo\n",
    "- Mejorar la calidad de los chunks para indexaci√≥n en Weaviate\n",
    "- Mantener contexto sem√°ntico combinando headers con p√°rrafos relacionados\n",
    "- Procesar todos los PDFs de `results/parsed_jsons/` y generar versiones mejoradas en `results/final_jsons/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de directorios\n",
    "BASE_DIR = Path('.')\n",
    "PARSED_JSONS_DIR = BASE_DIR / 'results' / 'parsed_jsons'\n",
    "MARKDOWN_DIR = BASE_DIR / 'results' / 'markdown'\n",
    "FINAL_JSONS_DIR = BASE_DIR / 'results' / 'final_jsons'\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "FINAL_JSONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorios configurados:\")\n",
    "print(f\"- Input JSON: {PARSED_JSONS_DIR}\")\n",
    "print(f\"- Input Markdown: {MARKDOWN_DIR}\")\n",
    "print(f\"- Output Final: {FINAL_JSONS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funci√≥n Principal de Combinaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(sec_chunk: Dict, para_chunk: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Combina un chunk de secci√≥n (sec) con un chunk de p√°rrafo (para).\n",
    "    \n",
    "    El chunk resultante tendr√°:\n",
    "    - Texto: sec_text + \"\\n\\n\" + para_text\n",
    "    - Metadata combinada inteligentemente\n",
    "    - Grounding de ambos chunks\n",
    "    \"\"\"\n",
    "    # Crear una copia profunda del chunk de secci√≥n como base\n",
    "    combined_chunk = copy.deepcopy(sec_chunk)\n",
    "    \n",
    "    # Combinar textos\n",
    "    sec_text = sec_chunk.get('text', '').strip()\n",
    "    para_text = para_chunk.get('text', '').strip()\n",
    "    \n",
    "    combined_text = f\"{sec_text}\\n\\n{para_text}\" if sec_text and para_text else sec_text + para_text\n",
    "    combined_chunk['text'] = combined_text\n",
    "    \n",
    "    # Actualizar chunk_id para reflejar la combinaci√≥n\n",
    "    sec_id = sec_chunk.get('chunk_id', '')\n",
    "    para_id = para_chunk.get('chunk_id', '')\n",
    "    combined_chunk['chunk_id'] = f\"{sec_id}+{para_id.split(':')[-1]}\"  # Formato m√°s limpio\n",
    "    \n",
    "    # Combinar grounding (coordenadas de ubicaci√≥n)\n",
    "    sec_grounding = sec_chunk.get('grounding', [])\n",
    "    para_grounding = para_chunk.get('grounding', [])\n",
    "    combined_chunk['grounding'] = sec_grounding + para_grounding\n",
    "    \n",
    "    # Combinar metadata\n",
    "    if 'metadata' in combined_chunk and 'metadata' in para_chunk:\n",
    "        # Combinar labels manteniendo 'sec' como principal\n",
    "        sec_labels = set(combined_chunk['metadata'].get('labels', []))\n",
    "        para_labels = set(para_chunk['metadata'].get('labels', []))\n",
    "        \n",
    "        # El chunk combinado mantiene 'sec' pero a√±ade informaci√≥n de que incluye 'para'\n",
    "        combined_labels = list(sec_labels | para_labels)\n",
    "        combined_chunk['metadata']['labels'] = combined_labels\n",
    "        \n",
    "        # Combinar reading_order_range\n",
    "        sec_range = combined_chunk['metadata'].get('reading_order_range', [0, 0])\n",
    "        para_range = para_chunk['metadata'].get('reading_order_range', [0, 0])\n",
    "        \n",
    "        combined_range = [\n",
    "            min(sec_range[0], para_range[0]),\n",
    "            max(sec_range[1], para_range[1])\n",
    "        ]\n",
    "        combined_chunk['metadata']['reading_order_range'] = combined_range\n",
    "        \n",
    "        # Actualizar combined_chunks counter\n",
    "        combined_chunk['metadata']['combined_chunks'] = (\n",
    "            combined_chunk['metadata'].get('combined_chunks', 1) + \n",
    "            para_chunk['metadata'].get('combined_chunks', 1)\n",
    "        )\n",
    "        \n",
    "        # Promedio de quality_score si existe\n",
    "        sec_quality = combined_chunk['metadata'].get('quality_score', 0.5)\n",
    "        para_quality = para_chunk['metadata'].get('quality_score', 0.5)\n",
    "        combined_chunk['metadata']['quality_score'] = (sec_quality + para_quality) / 2\n",
    "    \n",
    "    return combined_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_chunks(doc_data: Dict) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Procesa los chunks aplicando la L√ìGICA CORREGIDA DE REUTILIZACI√ìN con ENRIQUECIMIENTO SEM√ÅNTICO:\n",
    "    - TODOS los chunks de texto buscan su header anterior\n",
    "    - Headers pueden ser REUTILIZADOS para m√∫ltiples chunks\n",
    "    - Cada chunk simplemente busca el header m√°s cercano disponible\n",
    "    - NO hay concepto de \"headers usados\" - todos son reutilizables\n",
    "    - NUEVO: Integra enriquecimiento sem√°ntico usando philatelic_chunk_logic.py\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict, Dict]: (processed_document, stats)\n",
    "    \"\"\"\n",
    "    chunks = doc_data.get('chunks', [])\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Inicializar ChunkCombiner con el enricher global\n",
    "    from philatelic_chunk_logic import ChunkCombiner\n",
    "    combiner = ChunkCombiner(global_enricher)\n",
    "    \n",
    "    stats = {\n",
    "        'original_chunks': len(chunks),\n",
    "        'combined_chunks': 0,\n",
    "        'final_chunks': 0,\n",
    "        'combinations': [],\n",
    "        'no_header_found': 0,\n",
    "        'estimated_size_bytes': 0,\n",
    "        'largest_chunk_size': 0,\n",
    "        'header_reuse_stats': {},  # Estad√≠sticas de reutilizaci√≥n de headers\n",
    "        'unique_headers_used': set(),  # Para tracking informativo\n",
    "        'semantic_enrichment_applied': 0  # NUEVO: contador de enriquecimientos sem√°nticos\n",
    "    }\n",
    "    \n",
    "    print(f\"üìÑ Procesando {len(chunks)} chunks con l√≥gica de REUTILIZACI√ìN de headers + ENRIQUECIMIENTO SEM√ÅNTICO...\")\n",
    "    \n",
    "    for i, current_chunk in enumerate(chunks):\n",
    "        try:\n",
    "            # Verificar estructura del chunk\n",
    "            if not isinstance(current_chunk, dict):\n",
    "                print(f\"    ‚ö†Ô∏è Chunk {i} inv√°lido: {type(current_chunk)}\")\n",
    "                processed_chunks.append(current_chunk)\n",
    "                continue\n",
    "            \n",
    "            current_labels = get_chunk_labels(current_chunk)\n",
    "            current_text = current_chunk.get('text', '')\n",
    "            \n",
    "            # NUEVA L√ìGICA CON CLASES INTEGRADAS: Usar ChunkCombiner\n",
    "            should_combine, header_index = combiner.should_combine_chunks(current_chunk, processed_chunks)\n",
    "            \n",
    "            if should_combine and header_index is not None:\n",
    "                # Combinar con el header encontrado usando enriquecimiento sem√°ntico\n",
    "                header_chunk = processed_chunks[header_index]\n",
    "                \n",
    "                try:\n",
    "                    # NUEVO: Usar ChunkCombiner que integra enriquecimiento sem√°ntico autom√°ticamente\n",
    "                    combined = combiner.combine_single_header_chunk(header_chunk, current_chunk)\n",
    "                    \n",
    "                    # Verificar tama√±o del resultado\n",
    "                    chunk_size = estimate_chunk_size_bytes(combined)\n",
    "                    stats['largest_chunk_size'] = max(stats['largest_chunk_size'], chunk_size)\n",
    "                    \n",
    "                    # L√≠mite de seguridad: 100KB por chunk\n",
    "                    MAX_CHUNK_SIZE = 100 * 1024  # 100KB\n",
    "                    if chunk_size > MAX_CHUNK_SIZE:\n",
    "                        print(f\"    ‚ö†Ô∏è Chunk {i}: Combinaci√≥n muy grande ({chunk_size:,} bytes), usando original\")\n",
    "                        processed_chunks.append(current_chunk)\n",
    "                        continue\n",
    "                    \n",
    "                    stats['estimated_size_bytes'] += chunk_size\n",
    "                    \n",
    "                    # Verificar si se aplic√≥ enriquecimiento sem√°ntico\n",
    "                    if combined.get('metadata', {}).get('semantic_enrichment_applied', False):\n",
    "                        stats['semantic_enrichment_applied'] += 1\n",
    "                    \n",
    "                except Exception as combine_error:\n",
    "                    print(f\"    ‚ùå Error combinando chunk {i}: {str(combine_error)[:50]}...\")\n",
    "                    processed_chunks.append(current_chunk)\n",
    "                    continue\n",
    "                \n",
    "                # √âXITO: Registrar estad√≠sticas (pero NO marcar header como \"usado\")\n",
    "                stats['combined_chunks'] += 1\n",
    "                \n",
    "                # Estad√≠sticas de reutilizaci√≥n de headers\n",
    "                header_id = header_chunk.get('chunk_id', 'unknown')\n",
    "                header_labels = get_chunk_labels(header_chunk)\n",
    "                header_text = header_chunk.get('text', '')[:30] + '...'\n",
    "                \n",
    "                # Contar cu√°ntas veces se reutiliza cada header\n",
    "                if header_id not in stats['header_reuse_stats']:\n",
    "                    stats['header_reuse_stats'][header_id] = {\n",
    "                        'labels': header_labels,\n",
    "                        'text_preview': header_text,\n",
    "                        'reuse_count': 0,\n",
    "                        'combined_with': []\n",
    "                    }\n",
    "                \n",
    "                stats['header_reuse_stats'][header_id]['reuse_count'] += 1\n",
    "                stats['header_reuse_stats'][header_id]['combined_with'].append({\n",
    "                    'chunk_id': current_chunk.get('chunk_id', 'unknown'),\n",
    "                    'labels': current_labels\n",
    "                })\n",
    "                \n",
    "                # Tracking √∫nico de headers (para estad√≠sticas)\n",
    "                stats['unique_headers_used'].add(header_id)\n",
    "                \n",
    "                # Registrar ejemplo de combinaci√≥n (simplificado)\n",
    "                stats['combinations'].append({\n",
    "                    'content_id': current_chunk.get('chunk_id', 'unknown')[:15],\n",
    "                    'content_labels': ', '.join(current_labels),\n",
    "                    'header_labels': ', '.join(header_labels),\n",
    "                    'header_preview': header_text,\n",
    "                    'combined_size_kb': round(chunk_size / 1024, 1),\n",
    "                    'header_reuse_count': stats['header_reuse_stats'][header_id]['reuse_count'],\n",
    "                    'semantic_enriched': combined.get('metadata', {}).get('semantic_enrichment_applied', False)  # NUEVO\n",
    "                })\n",
    "                \n",
    "                # A√±adir chunk combinado\n",
    "                processed_chunks.append(combined)\n",
    "                \n",
    "                enrichment_status = \"üéØ\" if combined.get('metadata', {}).get('semantic_enrichment_applied', False) else \"üìù\"\n",
    "                print(f\"    ‚úÖ Chunk {i} ({', '.join(current_labels)}): combinado con {', '.join(header_labels)} (reutilizaci√≥n #{stats['header_reuse_stats'][header_id]['reuse_count']}) {enrichment_status}\")\n",
    "                \n",
    "            else:\n",
    "                # No se pudo combinar - aplicar enriquecimiento sem√°ntico individual\n",
    "                if any(label in ['para', 'marginalia', 'tab'] for label in current_labels):\n",
    "                    try:\n",
    "                        # NUEVO: Aplicar enriquecimiento sem√°ntico a√∫n sin header\n",
    "                        enriched_chunk = global_enricher.enrich_chunk_advanced_bilingual(current_chunk.copy())\n",
    "                        if enriched_chunk.get('metadata', {}).get('quality_score', 0) > current_chunk.get('metadata', {}).get('quality_score', 0):\n",
    "                            current_chunk = enriched_chunk\n",
    "                            stats['semantic_enrichment_applied'] += 1\n",
    "                            print(f\"    üéØ Chunk {i} ({', '.join(current_labels)}): enriquecimiento sem√°ntico individual aplicado\")\n",
    "                    except Exception as enrich_error:\n",
    "                        print(f\"    ‚ö†Ô∏è Error enriqueciendo chunk {i}: {str(enrich_error)[:30]}...\")\n",
    "                    \n",
    "                    stats['no_header_found'] += 1\n",
    "                    print(f\"    üìù Chunk {i} ({', '.join(current_labels)}): sin header disponible\")\n",
    "                \n",
    "                # A√±adir chunk sin combinar (pero posiblemente enriquecido)\n",
    "                chunk_size = estimate_chunk_size_bytes(current_chunk)\n",
    "                stats['estimated_size_bytes'] += chunk_size\n",
    "                stats['largest_chunk_size'] = max(stats['largest_chunk_size'], chunk_size)\n",
    "                processed_chunks.append(current_chunk)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Error general\n",
    "            error_info = f\"{type(e).__name__}: {str(e)[:50]}...\"\n",
    "            print(f\"    ‚ùå Error en chunk {i}: {error_info}\")\n",
    "            \n",
    "            processed_chunks.append(current_chunk)\n",
    "    \n",
    "    stats['final_chunks'] = len(processed_chunks)\n",
    "    \n",
    "    # Verificar eficiencia del algoritmo\n",
    "    size_mb = stats['estimated_size_bytes'] / (1024 * 1024)\n",
    "    combination_rate = (stats['combined_chunks'] / stats['original_chunks']) * 100 if stats['original_chunks'] > 0 else 0\n",
    "    enrichment_rate = (stats['semantic_enrichment_applied'] / stats['original_chunks']) * 100 if stats['original_chunks'] > 0 else 0\n",
    "    \n",
    "    # Estad√≠sticas de reutilizaci√≥n\n",
    "    total_reuses = sum(header_stats['reuse_count'] for header_stats in stats['header_reuse_stats'].values())\n",
    "    unique_headers_count = len(stats['unique_headers_used'])\n",
    "    avg_reuse_per_header = total_reuses / unique_headers_count if unique_headers_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADOS:\")\n",
    "    print(f\"  Chunks combinados: {stats['combined_chunks']}/{stats['original_chunks']} ({combination_rate:.1f}%)\")\n",
    "    print(f\"  Enriquecimiento sem√°ntico: {stats['semantic_enrichment_applied']}/{stats['original_chunks']} ({enrichment_rate:.1f}%)\")  # NUEVO\n",
    "    print(f\"  Headers √∫nicos utilizados: {unique_headers_count}\")\n",
    "    print(f\"  Promedio reutilizaci√≥n por header: {avg_reuse_per_header:.1f}\")\n",
    "    print(f\"  Sin header encontrado: {stats['no_header_found']}\")\n",
    "    print(f\"  Tama√±o estimado: {size_mb:.2f} MB\")\n",
    "    print(f\"  Chunk m√°s grande: {stats['largest_chunk_size']//1024}KB\")\n",
    "    \n",
    "    # Mostrar headers m√°s reutilizados (top 3)\n",
    "    most_reused = sorted(stats['header_reuse_stats'].items(), \n",
    "                        key=lambda x: x[1]['reuse_count'], reverse=True)[:3]\n",
    "    \n",
    "    if most_reused:\n",
    "        print(f\"  Headers m√°s reutilizados:\")\n",
    "        for header_id, header_info in most_reused:\n",
    "            reuse_count = header_info['reuse_count']\n",
    "            labels = ', '.join(header_info['labels'])\n",
    "            preview = header_info['text_preview']\n",
    "            print(f\"    - {labels}: \\\"{preview}\\\" (usado {reuse_count} veces)\")\n",
    "    \n",
    "    # Crear documento procesado\n",
    "    processed_doc = copy.deepcopy(doc_data)\n",
    "    processed_doc['chunks'] = processed_chunks\n",
    "    \n",
    "    # Metadata del documento\n",
    "    if 'metadata' not in processed_doc:\n",
    "        processed_doc['metadata'] = {}\n",
    "    \n",
    "    processed_doc['metadata']['processing_date'] = datetime.now().isoformat()\n",
    "    processed_doc['metadata']['header_reuse_strategy_applied'] = True\n",
    "    processed_doc['metadata']['semantic_enrichment_applied'] = True  # NUEVO\n",
    "    processed_doc['metadata']['original_chunk_count'] = stats['original_chunks']\n",
    "    processed_doc['metadata']['final_chunk_count'] = stats['final_chunks']\n",
    "    processed_doc['metadata']['combined_chunk_count'] = stats['combined_chunks']\n",
    "    processed_doc['metadata']['semantic_enriched_count'] = stats['semantic_enrichment_applied']  # NUEVO\n",
    "    processed_doc['metadata']['combination_rate_percent'] = round(combination_rate, 1)\n",
    "    processed_doc['metadata']['enrichment_rate_percent'] = round(enrichment_rate, 1)  # NUEVO\n",
    "    processed_doc['metadata']['unique_headers_used'] = unique_headers_count\n",
    "    processed_doc['metadata']['avg_header_reuse'] = round(avg_reuse_per_header, 1)\n",
    "    processed_doc['metadata']['estimated_size_mb'] = round(size_mb, 2)\n",
    "    \n",
    "    return processed_doc, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos *_philatelic.json aplicando header backtracking + enriquecimiento sem√°ntico.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary con estad√≠sticas completas del procesamiento\n",
    "    \"\"\"\n",
    "    # Obtener todos los archivos philatelic usando la funci√≥n del m√≥dulo\n",
    "    philatelic_files = get_all_philatelic_files()\n",
    "    \n",
    "    if not philatelic_files:\n",
    "        print(\"‚ùå No se encontraron archivos *_philatelic.json en results/parsed_jsons/\")\n",
    "        return {\n",
    "            'total_files': 0,\n",
    "            'processed_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'errors': [],\n",
    "            'total_original_chunks': 0,\n",
    "            'total_final_chunks': 0,\n",
    "            'total_combinations': 0,\n",
    "            'total_semantic_enrichments': 0,\n",
    "            'total_size_mb': 0.0\n",
    "        }\n",
    "    \n",
    "    print(f\"üìÅ Encontrados {len(philatelic_files)} archivos para procesar\")\n",
    "    \n",
    "    # Estad√≠sticas globales\n",
    "    global_stats = {\n",
    "        'total_files': len(philatelic_files),\n",
    "        'processed_files': 0,\n",
    "        'failed_files': 0,\n",
    "        'errors': [],\n",
    "        'total_original_chunks': 0,\n",
    "        'total_final_chunks': 0,\n",
    "        'total_combinations': 0,\n",
    "        'total_semantic_enrichments': 0,\n",
    "        'total_size_mb': 0.0,\n",
    "        'file_details': [],\n",
    "        'size_warnings': [],\n",
    "        'efficiency_report': {}\n",
    "    }\n",
    "    \n",
    "    for i, file_path in enumerate(philatelic_files):\n",
    "        try:\n",
    "            doc_id = extract_doc_id_from_filename(file_path.name)\n",
    "            print(f\"\\n[{i+1}/{len(philatelic_files)}] Procesando {doc_id}...\")\n",
    "            \n",
    "            # Cargar archivo JSON\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            \n",
    "            original_chunks_count = len(doc_data.get('chunks', []))\n",
    "            \n",
    "            # Procesar el documento\n",
    "            processed_doc, file_stats = process_document_chunks(doc_data)\n",
    "            \n",
    "            # Generar nombre de archivo de salida\n",
    "            output_filename = f\"{doc_id}_final.json\"\n",
    "            output_path = FINAL_JSONS_DIR / output_filename\n",
    "            \n",
    "            # Verificar tama√±o antes de guardar\n",
    "            estimated_size_mb = file_stats.get('estimated_size_mb', 0)\n",
    "            if estimated_size_mb > 50:  # Warning para archivos grandes\n",
    "                global_stats['size_warnings'].append({\n",
    "                    'file': doc_id,\n",
    "                    'final_mb': estimated_size_mb,\n",
    "                    'reason': 'Large file size'\n",
    "                })\n",
    "            \n",
    "            # Guardar archivo procesado\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_doc, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Actualizar estad√≠sticas globales\n",
    "            global_stats['processed_files'] += 1\n",
    "            global_stats['total_original_chunks'] += file_stats['original_chunks']\n",
    "            global_stats['total_final_chunks'] += file_stats['final_chunks']\n",
    "            global_stats['total_combinations'] += file_stats['combined_chunks']\n",
    "            global_stats['total_semantic_enrichments'] += file_stats.get('semantic_enrichment_applied', 0)\n",
    "            global_stats['total_size_mb'] += estimated_size_mb\n",
    "            \n",
    "            # Detalles por archivo\n",
    "            global_stats['file_details'].append({\n",
    "                'doc_id': doc_id,\n",
    "                'original_chunks': file_stats['original_chunks'],\n",
    "                'final_chunks': file_stats['final_chunks'],\n",
    "                'combinations': file_stats['combined_chunks'],\n",
    "                'semantic_enrichments': file_stats.get('semantic_enrichment_applied', 0),\n",
    "                'file_size_mb': estimated_size_mb\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úÖ {doc_id}: {file_stats['combined_chunks']} combinaciones + {file_stats.get('semantic_enrichment_applied', 0)} enriquecimientos ‚Üí {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            global_stats['failed_files'] += 1\n",
    "            global_stats['errors'].append({\n",
    "                'file': file_path.name,\n",
    "                'error': error_msg\n",
    "            })\n",
    "            \n",
    "            print(f\"‚ùå Error procesando {file_path.name}: {error_msg[:60]}...\")\n",
    "            continue\n",
    "    \n",
    "    # Generar reporte de eficiencia\n",
    "    if global_stats['processed_files'] > 0:\n",
    "        avg_combinations = global_stats['total_combinations'] / global_stats['processed_files']\n",
    "        avg_enrichments = global_stats['total_semantic_enrichments'] / global_stats['processed_files']\n",
    "        avg_size = global_stats['total_size_mb'] / global_stats['processed_files']\n",
    "        \n",
    "        global_stats['efficiency_report'] = {\n",
    "            'avg_combinations_per_file': round(avg_combinations, 1),\n",
    "            'avg_enrichments_per_file': round(avg_enrichments, 1),\n",
    "            'avg_file_size_mb': round(avg_size, 1),\n",
    "            'memory_efficient_files': sum(1 for details in global_stats['file_details'] \n",
    "                                        if details['file_size_mb'] < 10),\n",
    "            'combination_success_rate': round((global_stats['total_combinations'] / \n",
    "                                             max(global_stats['total_original_chunks'], 1)) * 100, 1),\n",
    "            'enrichment_success_rate': round((global_stats['total_semantic_enrichments'] / \n",
    "                                            max(global_stats['total_original_chunks'], 1)) * 100, 1)\n",
    "        }\n",
    "    \n",
    "    return global_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TEST DE CHUNK REAL CON ENRIQUECIMIENTO COMPLETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ TEST CHUNK REAL - ENRIQUECIMIENTO SEM√ÅNTICO COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chunk real de tabla filat√©lica para testing\n",
    "test_table_chunk = {\n",
    "    \"chunk_id\": \"Oxcart253:025:3-3:0\",\n",
    "    \"chunk_type\": \"text\",\n",
    "    \"text\": \"0\\t1\\t2\\r\\nScott 147\\tGR20 Type G7 2c used perf 12x11.5 wove paper lithographed mint never hinged very fine\\t25.00\\r\\nMichel 23a\\tGR21-28 Type G13 mostly used (ex-Saenz collection) comb perf yellow gum\\t40.00\\r\\nYvert 45\\tGR29-36 Type G14 postally used (ex-Saenz collection) thin spot crease\\t40.00\\r\\n66\\tGR38-45 Type G8 mint lightly hinged (ex-Colonel Green collection) superb centering\\t20.00\\r\\n67\\tGR47 Type G9 block of 4 mint no gum imperforate (ex-Alvarez) 1885 coat of arms\\t5.00\\r\\n68\\tG47-54 Type G9 not used (ex-Colonel Green collection) watermark inverted\\t25.00\\r\\n70\\tGR pair position 24 & 25 mint hinged gum signed Peralta 1880s cathedral design\\t75.00\\r\\n71\\tGR pair position 27 & 28 mint never hinged original gum signed Peralta engraved\\t75.00\\r\\n72\\tG3-4 pair not used gum signed Peralta and Napier laid paper extremely fine $50 USD\\t75.00\\r\\n80\\tRevenue Guanacaste Overprints Collection lots 48 to 80 inverted overprint color error\\t869.-\\r\\n\",\n",
    "    \"grounding\": [{\"page\": 25, \"box\": None}],\n",
    "    \"metadata\": {\n",
    "        \"labels\": [\"tab\"],\n",
    "        \"reading_order_range\": [3, 3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Header simulado (como ser√≠a en el procesamiento real)\n",
    "test_header_chunk = {\n",
    "    \"chunk_id\": \"Oxcart253:025:2-2:0\",\n",
    "    \"chunk_type\": \"text\", \n",
    "    \"text\": \"COSTA RICA PHILATELIC AUCTION CATALOG 1885-1891 GUANACASTE OVERPRINT PERIOD\",\n",
    "    \"metadata\": {\n",
    "        \"labels\": [\"sec\"],\n",
    "        \"reading_order_range\": [2, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã CHUNK ORIGINAL (solo tabla):\")\n",
    "print(f\"Texto: {test_table_chunk['text'][:100]}...\")\n",
    "print(f\"Longitud: {len(test_table_chunk['text'])} caracteres\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç APLICANDO ENRIQUECIMIENTO SEM√ÅNTICO...\")\n",
    "\n",
    "# 1. ENRIQUECIMIENTO INDIVIDUAL del chunk tabla\n",
    "enriched_chunk = global_enricher.enrich_chunk_advanced_bilingual(test_table_chunk.copy())\n",
    "entities = enriched_chunk.get('metadata', {}).get('entities', {})\n",
    "\n",
    "print(f\"\\nüìä ENTIDADES EXTRA√çDAS:\")\n",
    "print(f\"Quality Score: {enriched_chunk.get('metadata', {}).get('quality_score', 0):.2f}\")\n",
    "\n",
    "# Mostrar solo las entidades importantes encontradas\n",
    "if entities.get('catalog'):\n",
    "    sistemas = ', '.join(f\"{c['system']} {c['number']}\" for c in entities['catalog'][:3])\n",
    "    print(f\"Cat√°logos: {len(entities['catalog'])}, sistemas ‚Üí {sistemas}\")\n",
    "if entities.get('condition'):\n",
    "    print(f\"Condiciones: {entities['condition']}\")\n",
    "if entities.get('perforation'):\n",
    "    print(f\"Perforaci√≥n: {entities['perforation']}\")\n",
    "if entities.get('varieties'):\n",
    "    print(f\"EFO: {len(entities['varieties'])} variedades ‚Üí {[v['label'] for v in entities['varieties']]}\")\n",
    "if entities.get('prices'):\n",
    "    valores = ', '.join(f\"{p['amount']} {p['currency']}\" for p in entities['prices'][:3])\n",
    "    print(f\"Precios: {len(entities['prices'])} valores ‚Üí {valores}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ COMBINACI√ìN CON HEADER + ENRIQUECIMIENTO AVANZADO\")\n",
    "\n",
    "# 2. COMBINACI√ìN CON HEADER usando ChunkCombiner (incluye enriquecimiento autom√°tico)\n",
    "combiner = ChunkCombiner(global_enricher)\n",
    "combined_chunk = combiner.combine_single_header_chunk(test_header_chunk, test_table_chunk)\n",
    "\n",
    "print(f\"\\nüìù TEXTO ORIGINAL (Header + Tabla):\")\n",
    "original_combined = f\"{test_header_chunk['text']}\\\\n\\\\n{test_table_chunk['text']}\"\n",
    "print(f\"Longitud: {len(original_combined)} caracteres\")\n",
    "\n",
    "print(f\"\\nüöÄ TEXTO FINAL ENRIQUECIDO:\")\n",
    "final_text = combined_chunk.get('text', '')\n",
    "print(f\"Longitud: {len(final_text)} caracteres\")\n",
    "print(f\"Incremento: {((len(final_text) - len(original_combined)) / len(original_combined) * 100):.0f}%\")\n",
    "\n",
    "# Mostrar el resultado final de manera legible\n",
    "print(f\"\\nüìñ RESULTADO FINAL:\")\n",
    "print(\"-\" * 40)\n",
    "# Solo mostrar primeras l√≠neas del texto enriquecido para ver la estructura\n",
    "lines = final_text.split('\\\\n')\n",
    "for i, line in enumerate(lines[:10]):  # Primeras 10 l√≠neas\n",
    "    if line.strip():\n",
    "        print(f\"{line.strip()}\")\n",
    "\n",
    "if len(lines) > 10:\n",
    "    print(f\"... [{len(lines)-10} l√≠neas adicionales de enriquecimiento]\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nüéâ RESUMEN:\")\n",
    "print(f\"‚úÖ Chunk original: {len(test_table_chunk['text'])} chars\")\n",
    "print(f\"‚úÖ Con header: {len(original_combined)} chars\") \n",
    "print(f\"‚úÖ Completamente enriquecido: {len(final_text)} chars\")\n",
    "print(f\"‚úÖ Mejora total: {((len(final_text) - len(test_table_chunk['text'])) / len(test_table_chunk['text']) * 100):.0f}%\")\n",
    "print(f\"‚úÖ Semantic enrichment aplicado: {combined_chunk.get('metadata', {}).get('semantic_enrichment_applied', False)}\")\n",
    "\n",
    "# Verificar qu√© tipos de enriquecimiento se aplicaron\n",
    "enrichment_applied = []\n",
    "if \"Cat√°logos internacionales:\" in final_text:\n",
    "    enrichment_applied.append(\"Cat√°logos\")\n",
    "if \"Especificaciones t√©cnicas:\" in final_text:\n",
    "    enrichment_applied.append(\"T√©cnicas\")\n",
    "if \"Condici√≥n:\" in final_text:\n",
    "    enrichment_applied.append(\"Condici√≥n\")\n",
    "if \"Precios:\" in final_text:\n",
    "    enrichment_applied.append(\"Precios\")\n",
    "if \"Guanacaste\" in final_text:\n",
    "    enrichment_applied.append(\"Contexto CR\")\n",
    "\n",
    "print(f\"‚úÖ Tipos de enriquecimiento aplicados: {', '.join(enrichment_applied) if enrichment_applied else 'B√°sico'}\")\n",
    "\n",
    "print(\"\\\\nüéØ ¬°Listo para RAG! El chunk ahora tiene contexto completo y metadata rica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Procesamiento Masivo Todos los PDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üöÄ INICIANDO PROCESAMIENTO MASIVO - NUEVA ESTRATEGIA PDF + ENRIQUECIMIENTO SEM√ÅNTICO\")\n",
    "print(\"TODOS LOS TEXTOS BUSCAR√ÅN SU HEADER/SEC/SUB_SEC ANTERIOR + PHILATELIC ENRICHMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Ejecutar procesamiento con manejo de errores robusto\n",
    "try:\n",
    "    results = process_all_documents()\n",
    "    processing_successful = True\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå ERROR DURANTE EL PROCESAMIENTO:\")\n",
    "    print(f\"Tipo: {type(e).__name__}\")\n",
    "    print(f\"Mensaje: {str(e)}\")\n",
    "    processing_successful = False\n",
    "    results = {\n",
    "        'total_files': 0,\n",
    "        'processed_files': 0,\n",
    "        'failed_files': 1,\n",
    "        'errors': [{'file': 'procesamiento_general', 'error': str(e)}],\n",
    "        'total_semantic_enrichments': 0\n",
    "    }\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RESUMEN FINAL DEL PROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if processing_successful:\n",
    "    print(f\"‚úÖ √âXITO: Procesamiento completado con nueva estrategia PDF + ENRIQUECIMIENTO SEM√ÅNTICO\")\n",
    "    \n",
    "    # Estad√≠sticas principales\n",
    "    print(f\"\\nüìÅ ARCHIVOS:\")\n",
    "    print(f\"  Total encontrados:     {results.get('total_files', 0)}\")\n",
    "    print(f\"  Procesados con √©xito:  {results.get('processed_files', 0)}\")\n",
    "    print(f\"  Fallidos:              {results.get('failed_files', 0)}\")\n",
    "    \n",
    "    print(f\"\\nüìä CHUNKS:\")\n",
    "    print(f\"  Chunks originales:     {results.get('total_original_chunks', 0):,}\")\n",
    "    print(f\"  Chunks finales:        {results.get('total_final_chunks', 0):,}\")\n",
    "    print(f\"  Combinaciones:         {results.get('total_combinations', 0):,}\")\n",
    "    print(f\"  Enriquecimientos:      {results.get('total_semantic_enrichments', 0):,}\")  # NUEVO\n",
    "    \n",
    "    # Calcular estad√≠sticas\n",
    "    if results.get('total_original_chunks', 0) > 0:\n",
    "        combination_rate = (results.get('total_combinations', 0) / results.get('total_original_chunks', 0)) * 100\n",
    "        enrichment_rate = (results.get('total_semantic_enrichments', 0) / results.get('total_original_chunks', 0)) * 100  # NUEVO\n",
    "        print(f\"  Tasa combinaci√≥n:      {combination_rate:.1f}%\")\n",
    "        print(f\"  Tasa enriquecimiento:  {enrichment_rate:.1f}%\")  # NUEVO\n",
    "    \n",
    "    print(f\"\\nüíæ TAMA√ëOS:\")\n",
    "    print(f\"  Tama√±o total:          {results.get('total_size_mb', 0):.1f} MB\")\n",
    "    \n",
    "    if results.get('processed_files', 0) > 0:\n",
    "        avg_size = results.get('total_size_mb', 0) / results.get('processed_files', 0)\n",
    "        print(f\"  Tama√±o promedio:       {avg_size:.1f} MB por archivo\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è RENDIMIENTO:\")\n",
    "    print(f\"  Tiempo total:          {processing_time:.1f} segundos\")\n",
    "    \n",
    "    if results.get('processed_files', 0) > 0:\n",
    "        avg_time = processing_time / results.get('processed_files', 0)\n",
    "        print(f\"  Tiempo promedio:       {avg_time:.1f}s por archivo\")\n",
    "    \n",
    "    # Reportar eficiencia con enriquecimiento sem√°ntico\n",
    "    efficiency_report = results.get('efficiency_report', {})\n",
    "    if efficiency_report:\n",
    "        print(f\"\\nüìà EFICIENCIA:\")\n",
    "        print(f\"  Archivos eficientes:     {efficiency_report.get('memory_efficient_files', 0)}\")\n",
    "        print(f\"  Combinaciones/archivo:   {efficiency_report.get('avg_combinations_per_file', 0)}\")\n",
    "        print(f\"  Enriquecimientos/archivo: {efficiency_report.get('avg_enrichments_per_file', 0)}\")  # NUEVO\n",
    "        print(f\"  Tasa √©xito combinaci√≥n:  {efficiency_report.get('combination_success_rate', 0):.1f}%\")\n",
    "        print(f\"  Tasa √©xito enriquecimiento: {efficiency_report.get('enrichment_success_rate', 0):.1f}%\")  # NUEVO\n",
    "    \n",
    "    # Mostrar archivos m√°s exitosos (combinando combinaciones + enriquecimientos)\n",
    "    file_details = results.get('file_details', [])\n",
    "    if file_details:\n",
    "        print(f\"\\nüèÜ TOP 5 ARCHIVOS CON MAYOR MEJORA TOTAL:\")\n",
    "        # Ordenar por la suma de combinaciones + enriquecimientos\n",
    "        top_files = sorted(file_details, \n",
    "                          key=lambda x: x.get('combinations', 0) + x.get('semantic_enrichments', 0), \n",
    "                          reverse=True)[:5]\n",
    "        \n",
    "        for i, file_info in enumerate(top_files, 1):\n",
    "            combinations = file_info.get('combinations', 0)\n",
    "            enrichments = file_info.get('semantic_enrichments', 0)\n",
    "            total_chunks = file_info.get('original_chunks', 0)\n",
    "            file_size = file_info.get('file_size_mb', 0)\n",
    "            total_improvements = combinations + enrichments\n",
    "            \n",
    "            if total_chunks > 0:\n",
    "                improvement_rate = (total_improvements / total_chunks) * 100\n",
    "                print(f\"  {i}. {file_info.get('doc_id', 'unknown'):10} - {combinations} comb + {enrichments} enr = {total_improvements} mejoras ({improvement_rate:4.1f}%) - {file_size:.1f}MB\")\n",
    "    \n",
    "    # Advertencias y errores\n",
    "    size_warnings = results.get('size_warnings', [])\n",
    "    if size_warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è ADVERTENCIAS DE TAMA√ëO ({len(size_warnings)}):\")\n",
    "        for warning in size_warnings[:3]:  # Solo primeras 3\n",
    "            print(f\"  - {warning.get('file', 'unknown')}: {warning.get('final_mb', 0):.1f}MB ({warning.get('reason', 'unknown')})\")\n",
    "    \n",
    "    errors = results.get('errors', [])\n",
    "    if errors:\n",
    "        print(f\"\\n‚ùå ERRORES ({len(errors)}):\")\n",
    "        for error in errors[:3]:  # Solo primeros 3\n",
    "            print(f\"  - {error.get('file', 'unknown')}: {str(error.get('error', 'unknown'))[:60]}...\")\n",
    "    \n",
    "    print(f\"\\nüìÇ ARCHIVOS DE SALIDA:\")\n",
    "    print(f\"  Directorio: {FINAL_JSONS_DIR}\")\n",
    "    print(f\"  Formato: OXCART##_final.json\")\n",
    "    print(f\"  ‚úÖ Listos para indexaci√≥n en Weaviate\")\n",
    "    \n",
    "    print(f\"\\nüéØ NUEVA ESTRATEGIA PDF + ENRIQUECIMIENTO SEM√ÅNTICO - RESULTADOS:\")\n",
    "    print(f\"‚úÖ Chunks como 'POSITION 2: There are two blue dots...'\")\n",
    "    print(f\"‚úÖ AHORA tienen su contexto de header/secci√≥n anterior\")\n",
    "    print(f\"‚úÖ PLUS: Enriquecimiento sem√°ntico filat√©lico avanzado aplicado\")\n",
    "    print(f\"‚úÖ Detecci√≥n de cat√°logos internacionales (Scott, Michel, Yvert, etc.)\")\n",
    "    print(f\"‚úÖ Clasificaci√≥n EFO (Errores, variedades, oddities)\")\n",
    "    print(f\"‚úÖ Especificaciones t√©cnicas (perforaci√≥n, papel, impresi√≥n)\")\n",
    "    print(f\"‚úÖ Evaluaci√≥n de condici√≥n (mint/used, centering, defects)\")\n",
    "    print(f\"‚úÖ Contexto Costa Rica (per√≠odo Guanacaste, historia)\")\n",
    "    print(f\"‚úÖ Contexto sem√°ntico completo para RAG queries\")\n",
    "    print(f\"‚úÖ Tama√±os controlados (no m√°s archivos enormes)\")\n",
    "    print(f\"‚úÖ Deduplicaci√≥n inteligente aplicada\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå FALLO: El procesamiento no se pudo completar\")\n",
    "    print(f\"‚è±Ô∏è Tiempo antes del fallo: {processing_time:.1f} segundos\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "if processing_successful and results.get('processed_files', 0) > 0:\n",
    "    total_improvements = results.get('total_combinations', 0) + results.get('total_semantic_enrichments', 0)\n",
    "    print(f\"üéâ ¬°PROCESAMIENTO COMPLETADO CON √âXITO!\")\n",
    "    print(f\"La nueva estrategia PDF + ENRIQUECIMIENTO SEM√ÅNTICO funcion√≥ perfectamente.\")\n",
    "    print(f\"{total_improvements:,} chunks totales mejorados ({results.get('total_combinations', 0):,} combinaciones + {results.get('total_semantic_enrichments', 0):,} enriquecimientos).\")\n",
    "    print(f\"Los textos finales son significativamente m√°s ricos para embeddings RAG.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Procesamiento completado con limitaciones.\")\n",
    "    print(f\"Revisar errores mostrados arriba.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
