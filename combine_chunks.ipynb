{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Enriquecimiento Filatelico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS Y CONFIGURACIÃ“N - SISTEMA BILINGÃœE\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "# Import our new bilingual philatelic logic module\n",
    "from philatelic_chunk_logic import *\n",
    "\n",
    "# Initialize main enricher\n",
    "global_enricher = SemanticEnricher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Chunks - Mejora de Contexto SemÃ¡ntico\n",
    "\n",
    "LÃ³gica de backtracking para combinar chunks problemÃ¡ticos donde pÃ¡rrafos cortos (`para`) pierden contexto al estar separados de sus headers/secciones (`sec`) correspondientes.\n",
    "\n",
    "## Objetivo\n",
    "- Mejorar la calidad de los chunks para indexaciÃ³n en Weaviate\n",
    "- Mantener contexto semÃ¡ntico combinando headers con pÃ¡rrafos relacionados\n",
    "- Procesar todos los PDFs de `results/parsed_jsons/` y generar versiones mejoradas en `results/final_jsons/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n de directorios\n",
    "BASE_DIR = Path('.')\n",
    "PARSED_JSONS_DIR = BASE_DIR / 'results' / 'parsed_jsons'\n",
    "MARKDOWN_DIR = BASE_DIR / 'results' / 'markdown'\n",
    "FINAL_JSONS_DIR = BASE_DIR / 'results' / 'final_jsons'\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "FINAL_JSONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Directorios configurados:\")\n",
    "print(f\"- Input JSON: {PARSED_JSONS_DIR}\")\n",
    "print(f\"- Input Markdown: {MARKDOWN_DIR}\")\n",
    "print(f\"- Output Final: {FINAL_JSONS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FunciÃ³n Principal de CombinaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunks(sec_chunk: Dict, para_chunk: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Combina un chunk de secciÃ³n (sec) con un chunk de pÃ¡rrafo (para).\n",
    "    \n",
    "    El chunk resultante tendrÃ¡:\n",
    "    - Texto: sec_text + \"\\n\\n\" + para_text\n",
    "    - Metadata combinada inteligentemente\n",
    "    - Grounding de ambos chunks\n",
    "    \"\"\"\n",
    "    # Crear una copia profunda del chunk de secciÃ³n como base\n",
    "    combined_chunk = copy.deepcopy(sec_chunk)\n",
    "    \n",
    "    # Combinar textos\n",
    "    sec_text = sec_chunk.get('text', '').strip()\n",
    "    para_text = para_chunk.get('text', '').strip()\n",
    "    \n",
    "    combined_text = f\"{sec_text}\\n\\n{para_text}\" if sec_text and para_text else sec_text + para_text\n",
    "    combined_chunk['text'] = combined_text\n",
    "    \n",
    "    # Actualizar chunk_id para reflejar la combinaciÃ³n\n",
    "    sec_id = sec_chunk.get('chunk_id', '')\n",
    "    para_id = para_chunk.get('chunk_id', '')\n",
    "    combined_chunk['chunk_id'] = f\"{sec_id}+{para_id.split(':')[-1]}\"  # Formato mÃ¡s limpio\n",
    "    \n",
    "    # Combinar grounding (coordenadas de ubicaciÃ³n)\n",
    "    sec_grounding = sec_chunk.get('grounding', [])\n",
    "    para_grounding = para_chunk.get('grounding', [])\n",
    "    combined_chunk['grounding'] = sec_grounding + para_grounding\n",
    "    \n",
    "    # Combinar metadata\n",
    "    if 'metadata' in combined_chunk and 'metadata' in para_chunk:\n",
    "        # Combinar labels manteniendo 'sec' como principal\n",
    "        sec_labels = set(combined_chunk['metadata'].get('labels', []))\n",
    "        para_labels = set(para_chunk['metadata'].get('labels', []))\n",
    "        \n",
    "        # El chunk combinado mantiene 'sec' pero aÃ±ade informaciÃ³n de que incluye 'para'\n",
    "        combined_labels = list(sec_labels | para_labels)\n",
    "        combined_chunk['metadata']['labels'] = combined_labels\n",
    "        \n",
    "        # Combinar reading_order_range\n",
    "        sec_range = combined_chunk['metadata'].get('reading_order_range', [0, 0])\n",
    "        para_range = para_chunk['metadata'].get('reading_order_range', [0, 0])\n",
    "        \n",
    "        combined_range = [\n",
    "            min(sec_range[0], para_range[0]),\n",
    "            max(sec_range[1], para_range[1])\n",
    "        ]\n",
    "        combined_chunk['metadata']['reading_order_range'] = combined_range\n",
    "        \n",
    "        # Actualizar combined_chunks counter\n",
    "        combined_chunk['metadata']['combined_chunks'] = (\n",
    "            combined_chunk['metadata'].get('combined_chunks', 1) + \n",
    "            para_chunk['metadata'].get('combined_chunks', 1)\n",
    "        )\n",
    "        \n",
    "        # Promedio de quality_score si existe\n",
    "        sec_quality = combined_chunk['metadata'].get('quality_score', 0.5)\n",
    "        para_quality = para_chunk['metadata'].get('quality_score', 0.5)\n",
    "        combined_chunk['metadata']['quality_score'] = (sec_quality + para_quality) / 2\n",
    "    \n",
    "    return combined_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Procesamiento de Documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document_chunks(doc_data: Dict) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Procesa los chunks aplicando la LÃ“GICA CORREGIDA DE REUTILIZACIÃ“N con ENRIQUECIMIENTO SEMÃNTICO:\n",
    "    - TODOS los chunks de texto buscan su header anterior\n",
    "    - Headers pueden ser REUTILIZADOS para mÃºltiples chunks\n",
    "    - Cada chunk simplemente busca el header mÃ¡s cercano disponible\n",
    "    - NO hay concepto de \"headers usados\" - todos son reutilizables\n",
    "    - NUEVO: Integra enriquecimiento semÃ¡ntico usando philatelic_chunk_logic.py\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[Dict, Dict]: (processed_document, stats)\n",
    "    \"\"\"\n",
    "    chunks = doc_data.get('chunks', [])\n",
    "    processed_chunks = []\n",
    "    \n",
    "    # Inicializar ChunkCombiner con el enricher global\n",
    "    from philatelic_chunk_logic import ChunkCombiner\n",
    "    combiner = ChunkCombiner(global_enricher)\n",
    "    \n",
    "    stats = {\n",
    "        'original_chunks': len(chunks),\n",
    "        'combined_chunks': 0,\n",
    "        'final_chunks': 0,\n",
    "        'combinations': [],\n",
    "        'no_header_found': 0,\n",
    "        'estimated_size_bytes': 0,\n",
    "        'largest_chunk_size': 0,\n",
    "        'header_reuse_stats': {},  # EstadÃ­sticas de reutilizaciÃ³n de headers\n",
    "        'unique_headers_used': set(),  # Para tracking informativo\n",
    "        'semantic_enrichment_applied': 0  # NUEVO: contador de enriquecimientos semÃ¡nticos\n",
    "    }\n",
    "    \n",
    "    print(f\"ðŸ“„ Procesando {len(chunks)} chunks con lÃ³gica de REUTILIZACIÃ“N de headers + ENRIQUECIMIENTO SEMÃNTICO...\")\n",
    "    \n",
    "    for i, current_chunk in enumerate(chunks):\n",
    "        try:\n",
    "            # Verificar estructura del chunk\n",
    "            if not isinstance(current_chunk, dict):\n",
    "                print(f\"    âš ï¸ Chunk {i} invÃ¡lido: {type(current_chunk)}\")\n",
    "                processed_chunks.append(current_chunk)\n",
    "                continue\n",
    "            \n",
    "            current_labels = get_chunk_labels(current_chunk)\n",
    "            current_text = current_chunk.get('text', '')\n",
    "            \n",
    "            # NUEVA LÃ“GICA CON CLASES INTEGRADAS: Usar ChunkCombiner\n",
    "            should_combine, header_index = combiner.should_combine_chunks(current_chunk, processed_chunks)\n",
    "            \n",
    "            if should_combine and header_index is not None:\n",
    "                # Combinar con el header encontrado usando enriquecimiento semÃ¡ntico\n",
    "                header_chunk = processed_chunks[header_index]\n",
    "                \n",
    "                try:\n",
    "                    # NUEVO: Usar ChunkCombiner que integra enriquecimiento semÃ¡ntico automÃ¡ticamente\n",
    "                    combined = combiner.combine_single_header_chunk(header_chunk, current_chunk)\n",
    "                    \n",
    "                    # Verificar tamaÃ±o del resultado\n",
    "                    chunk_size = estimate_chunk_size_bytes(combined)\n",
    "                    stats['largest_chunk_size'] = max(stats['largest_chunk_size'], chunk_size)\n",
    "                    \n",
    "                    # LÃ­mite de seguridad: 100KB por chunk\n",
    "                    MAX_CHUNK_SIZE = 100 * 1024  # 100KB\n",
    "                    if chunk_size > MAX_CHUNK_SIZE:\n",
    "                        print(f\"    âš ï¸ Chunk {i}: CombinaciÃ³n muy grande ({chunk_size:,} bytes), usando original\")\n",
    "                        processed_chunks.append(current_chunk)\n",
    "                        continue\n",
    "                    \n",
    "                    stats['estimated_size_bytes'] += chunk_size\n",
    "                    \n",
    "                    # Verificar si se aplicÃ³ enriquecimiento semÃ¡ntico\n",
    "                    if combined.get('metadata', {}).get('semantic_enrichment_applied', False):\n",
    "                        stats['semantic_enrichment_applied'] += 1\n",
    "                    \n",
    "                except Exception as combine_error:\n",
    "                    print(f\"    âŒ Error combinando chunk {i}: {str(combine_error)[:50]}...\")\n",
    "                    processed_chunks.append(current_chunk)\n",
    "                    continue\n",
    "                \n",
    "                # Ã‰XITO: Registrar estadÃ­sticas (pero NO marcar header como \"usado\")\n",
    "                stats['combined_chunks'] += 1\n",
    "                \n",
    "                # EstadÃ­sticas de reutilizaciÃ³n de headers\n",
    "                header_id = header_chunk.get('chunk_id', 'unknown')\n",
    "                header_labels = get_chunk_labels(header_chunk)\n",
    "                header_text = header_chunk.get('text', '')[:30] + '...'\n",
    "                \n",
    "                # Contar cuÃ¡ntas veces se reutiliza cada header\n",
    "                if header_id not in stats['header_reuse_stats']:\n",
    "                    stats['header_reuse_stats'][header_id] = {\n",
    "                        'labels': header_labels,\n",
    "                        'text_preview': header_text,\n",
    "                        'reuse_count': 0,\n",
    "                        'combined_with': []\n",
    "                    }\n",
    "                \n",
    "                stats['header_reuse_stats'][header_id]['reuse_count'] += 1\n",
    "                stats['header_reuse_stats'][header_id]['combined_with'].append({\n",
    "                    'chunk_id': current_chunk.get('chunk_id', 'unknown'),\n",
    "                    'labels': current_labels\n",
    "                })\n",
    "                \n",
    "                # Tracking Ãºnico de headers (para estadÃ­sticas)\n",
    "                stats['unique_headers_used'].add(header_id)\n",
    "                \n",
    "                # Registrar ejemplo de combinaciÃ³n (simplificado)\n",
    "                stats['combinations'].append({\n",
    "                    'content_id': current_chunk.get('chunk_id', 'unknown')[:15],\n",
    "                    'content_labels': ', '.join(current_labels),\n",
    "                    'header_labels': ', '.join(header_labels),\n",
    "                    'header_preview': header_text,\n",
    "                    'combined_size_kb': round(chunk_size / 1024, 1),\n",
    "                    'header_reuse_count': stats['header_reuse_stats'][header_id]['reuse_count'],\n",
    "                    'semantic_enriched': combined.get('metadata', {}).get('semantic_enrichment_applied', False)  # NUEVO\n",
    "                })\n",
    "                \n",
    "                # AÃ±adir chunk combinado\n",
    "                processed_chunks.append(combined)\n",
    "                \n",
    "                enrichment_status = \"ðŸŽ¯\" if combined.get('metadata', {}).get('semantic_enrichment_applied', False) else \"ðŸ“\"\n",
    "                print(f\"    âœ… Chunk {i} ({', '.join(current_labels)}): combinado con {', '.join(header_labels)} (reutilizaciÃ³n #{stats['header_reuse_stats'][header_id]['reuse_count']}) {enrichment_status}\")\n",
    "                \n",
    "            else:\n",
    "                # No se pudo combinar - aplicar enriquecimiento semÃ¡ntico individual\n",
    "                if any(label in ['para', 'marginalia', 'tab'] for label in current_labels):\n",
    "                    try:\n",
    "                        # NUEVO: Aplicar enriquecimiento semÃ¡ntico aÃºn sin header\n",
    "                        enriched_chunk = global_enricher.enrich_chunk_advanced_bilingual(current_chunk.copy())\n",
    "                        if enriched_chunk.get('metadata', {}).get('quality_score', 0) > current_chunk.get('metadata', {}).get('quality_score', 0):\n",
    "                            current_chunk = enriched_chunk\n",
    "                            stats['semantic_enrichment_applied'] += 1\n",
    "                            print(f\"    ðŸŽ¯ Chunk {i} ({', '.join(current_labels)}): enriquecimiento semÃ¡ntico individual aplicado\")\n",
    "                    except Exception as enrich_error:\n",
    "                        print(f\"    âš ï¸ Error enriqueciendo chunk {i}: {str(enrich_error)[:30]}...\")\n",
    "                    \n",
    "                    stats['no_header_found'] += 1\n",
    "                    print(f\"    ðŸ“ Chunk {i} ({', '.join(current_labels)}): sin header disponible\")\n",
    "                \n",
    "                # AÃ±adir chunk sin combinar (pero posiblemente enriquecido)\n",
    "                chunk_size = estimate_chunk_size_bytes(current_chunk)\n",
    "                stats['estimated_size_bytes'] += chunk_size\n",
    "                stats['largest_chunk_size'] = max(stats['largest_chunk_size'], chunk_size)\n",
    "                processed_chunks.append(current_chunk)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Error general\n",
    "            error_info = f\"{type(e).__name__}: {str(e)[:50]}...\"\n",
    "            print(f\"    âŒ Error en chunk {i}: {error_info}\")\n",
    "            \n",
    "            processed_chunks.append(current_chunk)\n",
    "    \n",
    "    stats['final_chunks'] = len(processed_chunks)\n",
    "    \n",
    "    # Verificar eficiencia del algoritmo\n",
    "    size_mb = stats['estimated_size_bytes'] / (1024 * 1024)\n",
    "    combination_rate = (stats['combined_chunks'] / stats['original_chunks']) * 100 if stats['original_chunks'] > 0 else 0\n",
    "    enrichment_rate = (stats['semantic_enrichment_applied'] / stats['original_chunks']) * 100 if stats['original_chunks'] > 0 else 0\n",
    "    \n",
    "    # EstadÃ­sticas de reutilizaciÃ³n\n",
    "    total_reuses = sum(header_stats['reuse_count'] for header_stats in stats['header_reuse_stats'].values())\n",
    "    unique_headers_count = len(stats['unique_headers_used'])\n",
    "    avg_reuse_per_header = total_reuses / unique_headers_count if unique_headers_count > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š RESULTADOS:\")\n",
    "    print(f\"  Chunks combinados: {stats['combined_chunks']}/{stats['original_chunks']} ({combination_rate:.1f}%)\")\n",
    "    print(f\"  Enriquecimiento semÃ¡ntico: {stats['semantic_enrichment_applied']}/{stats['original_chunks']} ({enrichment_rate:.1f}%)\")  # NUEVO\n",
    "    print(f\"  Headers Ãºnicos utilizados: {unique_headers_count}\")\n",
    "    print(f\"  Promedio reutilizaciÃ³n por header: {avg_reuse_per_header:.1f}\")\n",
    "    print(f\"  Sin header encontrado: {stats['no_header_found']}\")\n",
    "    print(f\"  TamaÃ±o estimado: {size_mb:.2f} MB\")\n",
    "    print(f\"  Chunk mÃ¡s grande: {stats['largest_chunk_size']//1024}KB\")\n",
    "    \n",
    "    # Mostrar headers mÃ¡s reutilizados (top 3)\n",
    "    most_reused = sorted(stats['header_reuse_stats'].items(), \n",
    "                        key=lambda x: x[1]['reuse_count'], reverse=True)[:3]\n",
    "    \n",
    "    if most_reused:\n",
    "        print(f\"  Headers mÃ¡s reutilizados:\")\n",
    "        for header_id, header_info in most_reused:\n",
    "            reuse_count = header_info['reuse_count']\n",
    "            labels = ', '.join(header_info['labels'])\n",
    "            preview = header_info['text_preview']\n",
    "            print(f\"    - {labels}: \\\"{preview}\\\" (usado {reuse_count} veces)\")\n",
    "    \n",
    "    # Crear documento procesado\n",
    "    processed_doc = copy.deepcopy(doc_data)\n",
    "    processed_doc['chunks'] = processed_chunks\n",
    "    \n",
    "    # Metadata del documento\n",
    "    if 'metadata' not in processed_doc:\n",
    "        processed_doc['metadata'] = {}\n",
    "    \n",
    "    processed_doc['metadata']['processing_date'] = datetime.now().isoformat()\n",
    "    processed_doc['metadata']['header_reuse_strategy_applied'] = True\n",
    "    processed_doc['metadata']['semantic_enrichment_applied'] = True  # NUEVO\n",
    "    processed_doc['metadata']['original_chunk_count'] = stats['original_chunks']\n",
    "    processed_doc['metadata']['final_chunk_count'] = stats['final_chunks']\n",
    "    processed_doc['metadata']['combined_chunk_count'] = stats['combined_chunks']\n",
    "    processed_doc['metadata']['semantic_enriched_count'] = stats['semantic_enrichment_applied']  # NUEVO\n",
    "    processed_doc['metadata']['combination_rate_percent'] = round(combination_rate, 1)\n",
    "    processed_doc['metadata']['enrichment_rate_percent'] = round(enrichment_rate, 1)  # NUEVO\n",
    "    processed_doc['metadata']['unique_headers_used'] = unique_headers_count\n",
    "    processed_doc['metadata']['avg_header_reuse'] = round(avg_reuse_per_header, 1)\n",
    "    processed_doc['metadata']['estimated_size_mb'] = round(size_mb, 2)\n",
    "    \n",
    "    return processed_doc, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos *_philatelic.json aplicando header backtracking + enriquecimiento semÃ¡ntico.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary con estadÃ­sticas completas del procesamiento\n",
    "    \"\"\"\n",
    "    # Obtener todos los archivos philatelic usando la funciÃ³n del mÃ³dulo\n",
    "    philatelic_files = get_all_philatelic_files()\n",
    "    \n",
    "    if not philatelic_files:\n",
    "        print(\"âŒ No se encontraron archivos *_philatelic.json en results/parsed_jsons/\")\n",
    "        return {\n",
    "            'total_files': 0,\n",
    "            'processed_files': 0,\n",
    "            'failed_files': 0,\n",
    "            'errors': [],\n",
    "            'total_original_chunks': 0,\n",
    "            'total_final_chunks': 0,\n",
    "            'total_combinations': 0,\n",
    "            'total_semantic_enrichments': 0,\n",
    "            'total_size_mb': 0.0\n",
    "        }\n",
    "    \n",
    "    print(f\"ðŸ“ Encontrados {len(philatelic_files)} archivos para procesar\")\n",
    "    \n",
    "    # EstadÃ­sticas globales\n",
    "    global_stats = {\n",
    "        'total_files': len(philatelic_files),\n",
    "        'processed_files': 0,\n",
    "        'failed_files': 0,\n",
    "        'errors': [],\n",
    "        'total_original_chunks': 0,\n",
    "        'total_final_chunks': 0,\n",
    "        'total_combinations': 0,\n",
    "        'total_semantic_enrichments': 0,\n",
    "        'total_size_mb': 0.0,\n",
    "        'file_details': [],\n",
    "        'size_warnings': [],\n",
    "        'efficiency_report': {}\n",
    "    }\n",
    "    \n",
    "    for i, file_path in enumerate(philatelic_files):\n",
    "        try:\n",
    "            doc_id = extract_doc_id_from_filename(file_path.name)\n",
    "            print(f\"\\n[{i+1}/{len(philatelic_files)}] Procesando {doc_id}...\")\n",
    "            \n",
    "            # Cargar archivo JSON\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            \n",
    "            original_chunks_count = len(doc_data.get('chunks', []))\n",
    "            \n",
    "            # Procesar el documento\n",
    "            processed_doc, file_stats = process_document_chunks(doc_data)\n",
    "            \n",
    "            # Generar nombre de archivo de salida\n",
    "            output_filename = f\"{doc_id}_final.json\"\n",
    "            output_path = FINAL_JSONS_DIR / output_filename\n",
    "            \n",
    "            # Verificar tamaÃ±o antes de guardar\n",
    "            estimated_size_mb = file_stats.get('estimated_size_mb', 0)\n",
    "            if estimated_size_mb > 50:  # Warning para archivos grandes\n",
    "                global_stats['size_warnings'].append({\n",
    "                    'file': doc_id,\n",
    "                    'final_mb': estimated_size_mb,\n",
    "                    'reason': 'Large file size'\n",
    "                })\n",
    "            \n",
    "            # Guardar archivo procesado\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(processed_doc, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Actualizar estadÃ­sticas globales\n",
    "            global_stats['processed_files'] += 1\n",
    "            global_stats['total_original_chunks'] += file_stats['original_chunks']\n",
    "            global_stats['total_final_chunks'] += file_stats['final_chunks']\n",
    "            global_stats['total_combinations'] += file_stats['combined_chunks']\n",
    "            global_stats['total_semantic_enrichments'] += file_stats.get('semantic_enrichment_applied', 0)\n",
    "            global_stats['total_size_mb'] += estimated_size_mb\n",
    "            \n",
    "            # Detalles por archivo\n",
    "            global_stats['file_details'].append({\n",
    "                'doc_id': doc_id,\n",
    "                'original_chunks': file_stats['original_chunks'],\n",
    "                'final_chunks': file_stats['final_chunks'],\n",
    "                'combinations': file_stats['combined_chunks'],\n",
    "                'semantic_enrichments': file_stats.get('semantic_enrichment_applied', 0),\n",
    "                'file_size_mb': estimated_size_mb\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… {doc_id}: {file_stats['combined_chunks']} combinaciones + {file_stats.get('semantic_enrichment_applied', 0)} enriquecimientos â†’ {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            global_stats['failed_files'] += 1\n",
    "            global_stats['errors'].append({\n",
    "                'file': file_path.name,\n",
    "                'error': error_msg\n",
    "            })\n",
    "            \n",
    "            print(f\"âŒ Error procesando {file_path.name}: {error_msg[:60]}...\")\n",
    "            continue\n",
    "    \n",
    "    # Generar reporte de eficiencia\n",
    "    if global_stats['processed_files'] > 0:\n",
    "        avg_combinations = global_stats['total_combinations'] / global_stats['processed_files']\n",
    "        avg_enrichments = global_stats['total_semantic_enrichments'] / global_stats['processed_files']\n",
    "        avg_size = global_stats['total_size_mb'] / global_stats['processed_files']\n",
    "        \n",
    "        global_stats['efficiency_report'] = {\n",
    "            'avg_combinations_per_file': round(avg_combinations, 1),\n",
    "            'avg_enrichments_per_file': round(avg_enrichments, 1),\n",
    "            'avg_file_size_mb': round(avg_size, 1),\n",
    "            'memory_efficient_files': sum(1 for details in global_stats['file_details'] \n",
    "                                        if details['file_size_mb'] < 10),\n",
    "            'combination_success_rate': round((global_stats['total_combinations'] / \n",
    "                                             max(global_stats['total_original_chunks'], 1)) * 100, 1),\n",
    "            'enrichment_success_rate': round((global_stats['total_semantic_enrichments'] / \n",
    "                                            max(global_stats['total_original_chunks'], 1)) * 100, 1)\n",
    "        }\n",
    "    \n",
    "    return global_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TEST DE CHUNK REAL CON ENRIQUECIMIENTO COMPLETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ TEST CHUNK REAL - ENRIQUECIMIENTO SEMÃNTICO COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Chunk real de tabla filatÃ©lica para testing\n",
    "test_table_chunk = {\n",
    "    \"chunk_id\": \"Oxcart253:025:3-3:0\",\n",
    "    \"chunk_type\": \"text\",\n",
    "    \"text\": \"0\\t1\\t2\\r\\nScott 147\\tGR20 Type G7 2c used perf 12x11.5 wove paper lithographed mint never hinged very fine\\t25.00\\r\\nMichel 23a\\tGR21-28 Type G13 mostly used (ex-Saenz collection) comb perf yellow gum\\t40.00\\r\\nYvert 45\\tGR29-36 Type G14 postally used (ex-Saenz collection) thin spot crease\\t40.00\\r\\n66\\tGR38-45 Type G8 mint lightly hinged (ex-Colonel Green collection) superb centering\\t20.00\\r\\n67\\tGR47 Type G9 block of 4 mint no gum imperforate (ex-Alvarez) 1885 coat of arms\\t5.00\\r\\n68\\tG47-54 Type G9 not used (ex-Colonel Green collection) watermark inverted\\t25.00\\r\\n70\\tGR pair position 24 & 25 mint hinged gum signed Peralta 1880s cathedral design\\t75.00\\r\\n71\\tGR pair position 27 & 28 mint never hinged original gum signed Peralta engraved\\t75.00\\r\\n72\\tG3-4 pair not used gum signed Peralta and Napier laid paper extremely fine $50 USD\\t75.00\\r\\n80\\tRevenue Guanacaste Overprints Collection lots 48 to 80 inverted overprint color error\\t869.-\\r\\n\",\n",
    "    \"grounding\": [{\"page\": 25, \"box\": None}],\n",
    "    \"metadata\": {\n",
    "        \"labels\": [\"tab\"],\n",
    "        \"reading_order_range\": [3, 3]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Header simulado (como serÃ­a en el procesamiento real)\n",
    "test_header_chunk = {\n",
    "    \"chunk_id\": \"Oxcart253:025:2-2:0\",\n",
    "    \"chunk_type\": \"text\", \n",
    "    \"text\": \"COSTA RICA PHILATELIC AUCTION CATALOG 1885-1891 GUANACASTE OVERPRINT PERIOD\",\n",
    "    \"metadata\": {\n",
    "        \"labels\": [\"sec\"],\n",
    "        \"reading_order_range\": [2, 2]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ“‹ CHUNK ORIGINAL (solo tabla):\")\n",
    "print(f\"Texto: {test_table_chunk['text'][:100]}...\")\n",
    "print(f\"Longitud: {len(test_table_chunk['text'])} caracteres\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ” APLICANDO ENRIQUECIMIENTO SEMÃNTICO...\")\n",
    "\n",
    "# 1. ENRIQUECIMIENTO INDIVIDUAL del chunk tabla\n",
    "enriched_chunk = global_enricher.enrich_chunk_advanced_bilingual(test_table_chunk.copy())\n",
    "entities = enriched_chunk.get('metadata', {}).get('entities', {})\n",
    "\n",
    "print(f\"\\nðŸ“Š ENTIDADES EXTRAÃDAS:\")\n",
    "print(f\"Quality Score: {enriched_chunk.get('metadata', {}).get('quality_score', 0):.2f}\")\n",
    "\n",
    "# Mostrar solo las entidades importantes encontradas\n",
    "if entities.get('catalog'):\n",
    "    sistemas = ', '.join(f\"{c['system']} {c['number']}\" for c in entities['catalog'][:3])\n",
    "    print(f\"CatÃ¡logos: {len(entities['catalog'])}, sistemas â†’ {sistemas}\")\n",
    "if entities.get('condition'):\n",
    "    print(f\"Condiciones: {entities['condition']}\")\n",
    "if entities.get('perforation'):\n",
    "    print(f\"PerforaciÃ³n: {entities['perforation']}\")\n",
    "if entities.get('varieties'):\n",
    "    print(f\"EFO: {len(entities['varieties'])} variedades â†’ {[v['label'] for v in entities['varieties']]}\")\n",
    "if entities.get('prices'):\n",
    "    valores = ', '.join(f\"{p['amount']} {p['currency']}\" for p in entities['prices'][:3])\n",
    "    print(f\"Precios: {len(entities['prices'])} valores â†’ {valores}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ¯ COMBINACIÃ“N CON HEADER + ENRIQUECIMIENTO AVANZADO\")\n",
    "\n",
    "# 2. COMBINACIÃ“N CON HEADER usando ChunkCombiner (incluye enriquecimiento automÃ¡tico)\n",
    "combiner = ChunkCombiner(global_enricher)\n",
    "combined_chunk = combiner.combine_single_header_chunk(test_header_chunk, test_table_chunk)\n",
    "\n",
    "print(f\"\\nðŸ“ TEXTO ORIGINAL (Header + Tabla):\")\n",
    "original_combined = f\"{test_header_chunk['text']}\\\\n\\\\n{test_table_chunk['text']}\"\n",
    "print(f\"Longitud: {len(original_combined)} caracteres\")\n",
    "\n",
    "print(f\"\\nðŸš€ TEXTO FINAL ENRIQUECIDO:\")\n",
    "final_text = combined_chunk.get('text', '')\n",
    "print(f\"Longitud: {len(final_text)} caracteres\")\n",
    "print(f\"Incremento: {((len(final_text) - len(original_combined)) / len(original_combined) * 100):.0f}%\")\n",
    "\n",
    "# Mostrar el resultado final de manera legible\n",
    "print(f\"\\nðŸ“– RESULTADO FINAL:\")\n",
    "print(\"-\" * 40)\n",
    "# Solo mostrar primeras lÃ­neas del texto enriquecido para ver la estructura\n",
    "lines = final_text.split('\\\\n')\n",
    "for i, line in enumerate(lines[:10]):  # Primeras 10 lÃ­neas\n",
    "    if line.strip():\n",
    "        print(f\"{line.strip()}\")\n",
    "\n",
    "if len(lines) > 10:\n",
    "    print(f\"... [{len(lines)-10} lÃ­neas adicionales de enriquecimiento]\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(f\"\\nðŸŽ‰ RESUMEN:\")\n",
    "print(f\"âœ… Chunk original: {len(test_table_chunk['text'])} chars\")\n",
    "print(f\"âœ… Con header: {len(original_combined)} chars\") \n",
    "print(f\"âœ… Completamente enriquecido: {len(final_text)} chars\")\n",
    "print(f\"âœ… Mejora total: {((len(final_text) - len(test_table_chunk['text'])) / len(test_table_chunk['text']) * 100):.0f}%\")\n",
    "print(f\"âœ… Semantic enrichment aplicado: {combined_chunk.get('metadata', {}).get('semantic_enrichment_applied', False)}\")\n",
    "\n",
    "# Verificar quÃ© tipos de enriquecimiento se aplicaron\n",
    "enrichment_applied = []\n",
    "if \"CatÃ¡logos internacionales:\" in final_text:\n",
    "    enrichment_applied.append(\"CatÃ¡logos\")\n",
    "if \"Especificaciones tÃ©cnicas:\" in final_text:\n",
    "    enrichment_applied.append(\"TÃ©cnicas\")\n",
    "if \"CondiciÃ³n:\" in final_text:\n",
    "    enrichment_applied.append(\"CondiciÃ³n\")\n",
    "if \"Precios:\" in final_text:\n",
    "    enrichment_applied.append(\"Precios\")\n",
    "if \"Guanacaste\" in final_text:\n",
    "    enrichment_applied.append(\"Contexto CR\")\n",
    "\n",
    "print(f\"âœ… Tipos de enriquecimiento aplicados: {', '.join(enrichment_applied) if enrichment_applied else 'BÃ¡sico'}\")\n",
    "\n",
    "print(\"\\\\nðŸŽ¯ Â¡Listo para RAG! El chunk ahora tiene contexto completo y metadata rica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Procesamiento Masivo Todos los PDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"ðŸš€ INICIANDO PROCESAMIENTO MASIVO - NUEVA ESTRATEGIA PDF + ENRIQUECIMIENTO SEMÃNTICO\")\n",
    "print(\"TODOS LOS TEXTOS BUSCARÃN SU HEADER/SEC/SUB_SEC ANTERIOR + PHILATELIC ENRICHMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Ejecutar procesamiento con manejo de errores robusto\n",
    "try:\n",
    "    results = process_all_documents()\n",
    "    processing_successful = True\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR DURANTE EL PROCESAMIENTO:\")\n",
    "    print(f\"Tipo: {type(e).__name__}\")\n",
    "    print(f\"Mensaje: {str(e)}\")\n",
    "    processing_successful = False\n",
    "    results = {\n",
    "        'total_files': 0,\n",
    "        'processed_files': 0,\n",
    "        'failed_files': 1,\n",
    "        'errors': [{'file': 'procesamiento_general', 'error': str(e)}],\n",
    "        'total_semantic_enrichments': 0\n",
    "    }\n",
    "\n",
    "end_time = datetime.now()\n",
    "processing_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ“Š RESUMEN FINAL DEL PROCESAMIENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if processing_successful:\n",
    "    print(f\"âœ… Ã‰XITO: Procesamiento completado con nueva estrategia PDF + ENRIQUECIMIENTO SEMÃNTICO\")\n",
    "    \n",
    "    # EstadÃ­sticas principales\n",
    "    print(f\"\\nðŸ“ ARCHIVOS:\")\n",
    "    print(f\"  Total encontrados:     {results.get('total_files', 0)}\")\n",
    "    print(f\"  Procesados con Ã©xito:  {results.get('processed_files', 0)}\")\n",
    "    print(f\"  Fallidos:              {results.get('failed_files', 0)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š CHUNKS:\")\n",
    "    print(f\"  Chunks originales:     {results.get('total_original_chunks', 0):,}\")\n",
    "    print(f\"  Chunks finales:        {results.get('total_final_chunks', 0):,}\")\n",
    "    print(f\"  Combinaciones:         {results.get('total_combinations', 0):,}\")\n",
    "    print(f\"  Enriquecimientos:      {results.get('total_semantic_enrichments', 0):,}\")  # NUEVO\n",
    "    \n",
    "    # Calcular estadÃ­sticas\n",
    "    if results.get('total_original_chunks', 0) > 0:\n",
    "        combination_rate = (results.get('total_combinations', 0) / results.get('total_original_chunks', 0)) * 100\n",
    "        enrichment_rate = (results.get('total_semantic_enrichments', 0) / results.get('total_original_chunks', 0)) * 100  # NUEVO\n",
    "        print(f\"  Tasa combinaciÃ³n:      {combination_rate:.1f}%\")\n",
    "        print(f\"  Tasa enriquecimiento:  {enrichment_rate:.1f}%\")  # NUEVO\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ TAMAÃ‘OS:\")\n",
    "    print(f\"  TamaÃ±o total:          {results.get('total_size_mb', 0):.1f} MB\")\n",
    "    \n",
    "    if results.get('processed_files', 0) > 0:\n",
    "        avg_size = results.get('total_size_mb', 0) / results.get('processed_files', 0)\n",
    "        print(f\"  TamaÃ±o promedio:       {avg_size:.1f} MB por archivo\")\n",
    "    \n",
    "    print(f\"\\nâ±ï¸ RENDIMIENTO:\")\n",
    "    print(f\"  Tiempo total:          {processing_time:.1f} segundos\")\n",
    "    \n",
    "    if results.get('processed_files', 0) > 0:\n",
    "        avg_time = processing_time / results.get('processed_files', 0)\n",
    "        print(f\"  Tiempo promedio:       {avg_time:.1f}s por archivo\")\n",
    "    \n",
    "    # Reportar eficiencia con enriquecimiento semÃ¡ntico\n",
    "    efficiency_report = results.get('efficiency_report', {})\n",
    "    if efficiency_report:\n",
    "        print(f\"\\nðŸ“ˆ EFICIENCIA:\")\n",
    "        print(f\"  Archivos eficientes:     {efficiency_report.get('memory_efficient_files', 0)}\")\n",
    "        print(f\"  Combinaciones/archivo:   {efficiency_report.get('avg_combinations_per_file', 0)}\")\n",
    "        print(f\"  Enriquecimientos/archivo: {efficiency_report.get('avg_enrichments_per_file', 0)}\")  # NUEVO\n",
    "        print(f\"  Tasa Ã©xito combinaciÃ³n:  {efficiency_report.get('combination_success_rate', 0):.1f}%\")\n",
    "        print(f\"  Tasa Ã©xito enriquecimiento: {efficiency_report.get('enrichment_success_rate', 0):.1f}%\")  # NUEVO\n",
    "    \n",
    "    # Mostrar archivos mÃ¡s exitosos (combinando combinaciones + enriquecimientos)\n",
    "    file_details = results.get('file_details', [])\n",
    "    if file_details:\n",
    "        print(f\"\\nðŸ† TOP 5 ARCHIVOS CON MAYOR MEJORA TOTAL:\")\n",
    "        # Ordenar por la suma de combinaciones + enriquecimientos\n",
    "        top_files = sorted(file_details, \n",
    "                          key=lambda x: x.get('combinations', 0) + x.get('semantic_enrichments', 0), \n",
    "                          reverse=True)[:5]\n",
    "        \n",
    "        for i, file_info in enumerate(top_files, 1):\n",
    "            combinations = file_info.get('combinations', 0)\n",
    "            enrichments = file_info.get('semantic_enrichments', 0)\n",
    "            total_chunks = file_info.get('original_chunks', 0)\n",
    "            file_size = file_info.get('file_size_mb', 0)\n",
    "            total_improvements = combinations + enrichments\n",
    "            \n",
    "            if total_chunks > 0:\n",
    "                improvement_rate = (total_improvements / total_chunks) * 100\n",
    "                print(f\"  {i}. {file_info.get('doc_id', 'unknown'):10} - {combinations} comb + {enrichments} enr = {total_improvements} mejoras ({improvement_rate:4.1f}%) - {file_size:.1f}MB\")\n",
    "    \n",
    "    # Advertencias y errores\n",
    "    size_warnings = results.get('size_warnings', [])\n",
    "    if size_warnings:\n",
    "        print(f\"\\nâš ï¸ ADVERTENCIAS DE TAMAÃ‘O ({len(size_warnings)}):\")\n",
    "        for warning in size_warnings[:3]:  # Solo primeras 3\n",
    "            print(f\"  - {warning.get('file', 'unknown')}: {warning.get('final_mb', 0):.1f}MB ({warning.get('reason', 'unknown')})\")\n",
    "    \n",
    "    errors = results.get('errors', [])\n",
    "    if errors:\n",
    "        print(f\"\\nâŒ ERRORES ({len(errors)}):\")\n",
    "        for error in errors[:3]:  # Solo primeros 3\n",
    "            print(f\"  - {error.get('file', 'unknown')}: {str(error.get('error', 'unknown'))[:60]}...\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‚ ARCHIVOS DE SALIDA:\")\n",
    "    print(f\"  Directorio: {FINAL_JSONS_DIR}\")\n",
    "    print(f\"  Formato: OXCART##_final.json\")\n",
    "    print(f\"  âœ… Listos para indexaciÃ³n en Weaviate\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ NUEVA ESTRATEGIA PDF + ENRIQUECIMIENTO SEMÃNTICO - RESULTADOS:\")\n",
    "    print(f\"âœ… Chunks como 'POSITION 2: There are two blue dots...'\")\n",
    "    print(f\"âœ… AHORA tienen su contexto de header/secciÃ³n anterior\")\n",
    "    print(f\"âœ… PLUS: Enriquecimiento semÃ¡ntico filatÃ©lico avanzado aplicado\")\n",
    "    print(f\"âœ… DetecciÃ³n de catÃ¡logos internacionales (Scott, Michel, Yvert, etc.)\")\n",
    "    print(f\"âœ… ClasificaciÃ³n EFO (Errores, variedades, oddities)\")\n",
    "    print(f\"âœ… Especificaciones tÃ©cnicas (perforaciÃ³n, papel, impresiÃ³n)\")\n",
    "    print(f\"âœ… EvaluaciÃ³n de condiciÃ³n (mint/used, centering, defects)\")\n",
    "    print(f\"âœ… Contexto Costa Rica (perÃ­odo Guanacaste, historia)\")\n",
    "    print(f\"âœ… Contexto semÃ¡ntico completo para RAG queries\")\n",
    "    print(f\"âœ… TamaÃ±os controlados (no mÃ¡s archivos enormes)\")\n",
    "    print(f\"âœ… DeduplicaciÃ³n inteligente aplicada\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ FALLO: El procesamiento no se pudo completar\")\n",
    "    print(f\"â±ï¸ Tiempo antes del fallo: {processing_time:.1f} segundos\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "if processing_successful and results.get('processed_files', 0) > 0:\n",
    "    total_improvements = results.get('total_combinations', 0) + results.get('total_semantic_enrichments', 0)\n",
    "    print(f\"ðŸŽ‰ Â¡PROCESAMIENTO COMPLETADO CON Ã‰XITO!\")\n",
    "    print(f\"La nueva estrategia PDF + ENRIQUECIMIENTO SEMÃNTICO funcionÃ³ perfectamente.\")\n",
    "    print(f\"{total_improvements:,} chunks totales mejorados ({results.get('total_combinations', 0):,} combinaciones + {results.get('total_semantic_enrichments', 0):,} enriquecimientos).\")\n",
    "    print(f\"Los textos finales son significativamente mÃ¡s ricos para embeddings RAG.\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Procesamiento completado con limitaciones.\")\n",
    "    print(f\"Revisar errores mostrados arriba.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# ACTUALIZACIÃ“N PARA GARANTIZAR TEXT_ORIGINAL EN TODOS LOS CHUNKS\n# ============================================================================\n\ndef ensure_text_original_in_chunks(chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Garantiza que todos los chunks tengan text_original para Weaviate.\n    \n    Args:\n        chunks: Lista de chunks procesados\n        \n    Returns:\n        Lista de chunks con text_original garantizado\n    \"\"\"\n    processed_chunks = []\n    \n    for chunk in chunks:\n        # Crear copia del chunk\n        updated_chunk = copy.deepcopy(chunk)\n        \n        # Asegurar que tenga text_original\n        if 'text_original' not in updated_chunk or not updated_chunk['text_original']:\n            updated_chunk['text_original'] = updated_chunk.get('text', '')\n        \n        processed_chunks.append(updated_chunk)\n    \n    return processed_chunks\n\ndef process_document_chunks_with_text_original(doc_data: Dict) -> Tuple[Dict, Dict]:\n    \"\"\"\n    VersiÃ³n actualizada que garantiza text_original en todos los chunks.\n    \"\"\"\n    # Usar la funciÃ³n existente\n    processed_doc, stats = process_document_chunks(doc_data)\n    \n    # Asegurar text_original en todos los chunks\n    processed_doc['chunks'] = ensure_text_original_in_chunks(processed_doc['chunks'])\n    \n    return processed_doc, stats\n\nprint(\"âœ… Funciones actualizadas para garantizar text_original en todos los chunks\")\nprint(\"âœ… Los chunks ahora estÃ¡n listos para indexaciÃ³n limpia en Weaviate\")\nprint(\"âœ… text_original = texto para mostrar usuario\")  \nprint(\"âœ… text = texto enriquecido para embeddings\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}