{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1454574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Costa Rica Philatelic Knowledge Graph Extractor - EXPERT VERSION\n",
    "Handles all Costa Rican denomination systems and catalog syntax\n",
    "By: Philately & Regex Expert\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from collections import defaultdict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd59eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install networkx matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class PhilatelicNode:\n",
    "    \"\"\"Represents a philatelic entity node\"\"\"\n",
    "    catalog_number: str\n",
    "    node_type: str  # stamp, proof, specimen, essay, variety, error\n",
    "    sub_type: Optional[str] = None  # die_proof, plate_proof, color_variety, etc.\n",
    "    denomination: Optional[str] = None\n",
    "    color: Optional[str] = None\n",
    "    year: Optional[str] = None\n",
    "    issue_name: Optional[str] = None  # From most recent <sec> element\n",
    "    quantity: Optional[str] = None\n",
    "    page_number: int = 0\n",
    "    reading_order: int = 0\n",
    "    raw_text: str = \"\"\n",
    "    context_before: List[str] = field(default_factory=list)\n",
    "    context_after: List[str] = field(default_factory=list)\n",
    "    attributes: List[str] = field(default_factory=list)\n",
    "    reference_numbers: List[str] = field(default_factory=list)\n",
    "    base_stamp: Optional[str] = None  # For proofs/varieties: which stamp they belong to\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {k: v for k, v in asdict(self).items() if v not in [None, [], \"\"]}\n",
    "\n",
    "class CostaRicaCatalogParser:\n",
    "    \"\"\"Expert parser for Costa Rica philatelic catalog with full denomination support\"\"\"\n",
    "    \n",
    "    # COMPREHENSIVE DENOMINATION REGEX\n",
    "    # Handles: centavos(c), Colones(C/col), pesos(p), reales(r), fractions(½, 1/2)\n",
    "    # Full words: centimos, centavos, colones, pesos, reales\n",
    "    DENOMINATION = re.compile(\n",
    "        r'\\b('  # Captura grupo 1: el valor\n",
    "            r'\\d+(?:[/.]?\\d+)?'  # Números normales: 15, 1.5, 1/2\n",
    "            r'|½|¼|¾'            # O símbolos de fracción standalone\n",
    "        r')\\s*'\n",
    "        r'('  # Captura grupo 2: la unidad\n",
    "            r'c|C|col|colón|colones|p|peso|pesos|r|real|reales|'\n",
    "            r'centimo|centimos|céntimo|céntimos|centavo|centavos|céntavo|céntavos'\n",
    "        r')\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # Surcharge pattern: \"1c on ½ real\", \"5c on 2 reales\", etc.\n",
    "    SURCHARGE = re.compile(\n",
    "        r'(\\d+)\\s*c\\s+on\\s+([½¼¾\\d/]+)\\s*(real|reales)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # More precise: only match denomination after catalog number, not standalone\n",
    "    DENOMINATION_AFTER_CATALOG = re.compile(\n",
    "        r'(?:DP|PP|S|E|I|OP|^|\\s)(\\d+[a-z]?)\\s+'  # Catalog number\n",
    "        r'(\\d+(?:[/.]?\\d+|½|¼)?)\\s*'  # Denomination value\n",
    "        r'(c|C|col|colón|colones|p|peso|pesos|r|real|reales|'\n",
    "        r'centimo|centimos|céntimo|céntimos|centavo|centavos)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    # Color patterns - comprehensive\n",
    "    COLOR_MODIFIERS = r'(deep|light|dark|bright|pale|dull|bright)'\n",
    "    COLOR_NAMES = r'(black|violet|blue|red|green|orange|brown|yellow|gray|grey|' \\\n",
    "                  r'scarlet|ultramarine|carmine|olive|purple|pink|rose|magenta|' \\\n",
    "                  r'turquoise|indigo|vermillion|sepia|slate|cobalt|crimson)'\n",
    "    COLORS = re.compile(rf'\\b{COLOR_MODIFIERS}?\\s*{COLOR_NAMES}\\b', re.IGNORECASE)\n",
    "    \n",
    "    YEAR = re.compile(r'\\b(18\\d{2}|19\\d{2}|20\\d{2})\\b')\n",
    "    \n",
    "    # Quantity: numbers with commas, but NOT single/double digit numbers\n",
    "    # Must have comma OR be 4+ digits OR have \"printed\"/\"issued\" keyword\n",
    "    QUANTITY = re.compile(\n",
    "        r'\\b(\\d{1,3}(?:,\\d{3})+)\\b|'  # Try comma pattern FIRST\n",
    "        r'(?:printed|issued|quantity)[:\\s]+(\\d{1,3}(?:,\\d{3})+)\\b',  # ← Añadir soporte para comas aquí también\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    \n",
    "    REFERENCE = re.compile(r'#([\\w-]+)')\n",
    "    \n",
    "    ATTRIBUTES = [\n",
    "        'imperf', 'imperforate', 'inverted', 'double perf', 'triple perf',\n",
    "        'tete beche', 'tete-beche', 'téte-béche', 'shifted', 'gutter pair', \n",
    "        'sunk on card', 'no numeral', 'omitted center', 'inverted center', \n",
    "        'double op', 'horizontal pair', 'vertical pair', 'right margin', \n",
    "        'left margin', 'lower margin', 'upper margin', 'inverted op',\n",
    "        'misplaced', 'offset', 'printed on both sides', 'blind perf',\n",
    "        'photographic proof'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_issue = None  # Most recent <sec> element\n",
    "        self.current_sec = None          \n",
    "        self.current_sub_sec = None      \n",
    "        self.current_sub_sub_sec = None  \n",
    "        self.current_section = None  # Proof, Regular issue, etc.\n",
    "        self.current_year = None\n",
    "        self.stamps_seen = set()\n",
    "    \n",
    "    def clean_latex(self, text: str) -> str:\n",
    "        \"\"\"Remove LaTeX formatting and extract content\"\"\"\n",
    "        if '\\\\begin{array}' in text:\n",
    "            match = re.search(r'\\\\begin\\{array\\}[^{]*\\{(.*?)\\\\end\\{array\\}', text, re.DOTALL)\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                rows = content.split('\\\\\\\\')\n",
    "                cleaned_rows = []\n",
    "                for row in rows:\n",
    "                    row = re.sub(r'\\\\text\\s*\\{\\s*([^}]*)\\s*\\}', r'\\1', row)\n",
    "                    row = re.sub(r'\\\\mathrm\\s*\\{\\s*([^}]*)\\s*\\}', r'\\1', row)\n",
    "                    row = re.sub(r'&', ' ', row)\n",
    "                    row = re.sub(r'\\\\[a-zA-Z]+', '', row)\n",
    "                    row = re.sub(r'[{}$\\\\]', '', row)\n",
    "                    row = re.sub(r'\\s+', ' ', row).strip()\n",
    "                    if row:\n",
    "                        cleaned_rows.append(row)\n",
    "                return '\\n'.join(cleaned_rows)\n",
    "        \n",
    "        text = re.sub(r'\\$\\^?\\{(\\w+)\\}\\$', r'\\1', text)\n",
    "        text = re.sub(r'\\\\text\\s*\\{\\s*([^}]*)\\s*\\}', r'\\1', text)\n",
    "        text = re.sub(r'\\\\mathrm\\s*\\{\\s*([^}]*)\\s*\\}', r'\\1', text)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+\\{([^}]*)\\}', r'\\1', text)\n",
    "        text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "        text = re.sub(r'[{}$]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def normalize_denomination(self, value: str, unit: str) -> str:\n",
    "        \"\"\"Normalize denomination to standard format\"\"\"\n",
    "        \n",
    "        # CRITICAL: Check for capital C BEFORE any conversion\n",
    "        if unit == 'C':  # Capital C = Colones\n",
    "            unit_normalized = 'col'\n",
    "        elif unit.lower() in ['c', 'centimo', 'centimos', 'céntimo', 'céntimos', \n",
    "                            'centavo', 'centavos', 'céntavo', 'céntavos']:\n",
    "            unit_normalized = 'c'\n",
    "        elif unit.lower() in ['col', 'colón', 'colones']:\n",
    "            unit_normalized = 'col'\n",
    "        elif unit.lower() in ['p', 'peso', 'pesos']:\n",
    "            unit_normalized = 'p'\n",
    "        elif unit.lower() in ['r', 'real', 'reales']:\n",
    "            unit_normalized = 'r'\n",
    "        else:\n",
    "            unit_normalized = unit.lower()\n",
    "        \n",
    "        # Handle fractions\n",
    "        if value == '½':\n",
    "            value_normalized = '0.5'\n",
    "        elif value == '¼':\n",
    "            value_normalized = '0.25'\n",
    "        elif value == '¾':\n",
    "            value_normalized = '0.75'\n",
    "        elif '/' in value:\n",
    "            try:\n",
    "                parts = value.split('/')\n",
    "                result = float(parts[0]) / float(parts[1])\n",
    "                value_normalized = str(result)  # str(float) automáticamente pone 0.25\n",
    "            except:\n",
    "                value_normalized = value\n",
    "        else:\n",
    "            value_normalized = value\n",
    "        \n",
    "        return f\"{value_normalized}{unit_normalized}\"\n",
    "    \n",
    "    def extract_base_stamp_number(self, catalog_num: str) -> Optional[str]:\n",
    "        \"\"\"Extract base stamp number from proof/specimen/variety\"\"\"\n",
    "        match = re.match(r'[A-Z]*(\\d+)[a-zA-Z]*$', catalog_num, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def extract_stamp_color(self, line: str, catalog_num: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract color of stamp, avoiding colors of overprints, frames, values.\n",
    "        Expert strategy: Get color between denomination and first delimiter.\n",
    "        \"\"\"\n",
    "        # Find text segment after catalog and denomination\n",
    "        # Pattern: [catalog] [denom] [COLOR SHOULD BE HERE] [, or 'op' or other delimiter]\n",
    "        \n",
    "        # Escape special regex chars in catalog_num\n",
    "        cat_escaped = re.escape(catalog_num)\n",
    "        \n",
    "        # Try to find: catalog_num + any_denomination + color_text\n",
    "        pattern = rf'{cat_escaped}\\s+\\d+(?:[/.]?\\d+)?\\s*(?:c|C|col|p|r|centimos?|centavos?|colones?|pesos?|reales?)\\s+([^,]+?)(?:,|\\bop\\b|\\bimperf|\\bperf|\\bframe\\b|\\bvalue\\b|$)'\n",
    "        match = re.search(pattern, line, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            color_section = match.group(1).strip()\n",
    "            \n",
    "            # Stop at certain keywords\n",
    "            stop_words = ['op', 'specimen', 'frame', 'value', 'imperf', 'perf', \n",
    "                         'inverted', 'double', 'triple', 'sunk', 'with', 'hole',\n",
    "                         'margin', 'pair', '#', 'quantity', 'printed']\n",
    "            \n",
    "            color_parts = []\n",
    "            for word in color_section.split():\n",
    "                if word.lower() in stop_words:\n",
    "                    break\n",
    "                color_parts.append(word)\n",
    "            \n",
    "            if color_parts:\n",
    "                color_text = ' '.join(color_parts).strip()\n",
    "                # Verify it contains actual color\n",
    "                if self.COLORS.search(color_text):\n",
    "                    return color_text\n",
    "        \n",
    "        # Fallback: Get first color before comma or 'op'\n",
    "        text_before_delim = re.split(r',|\\bop\\b', line, maxsplit=1)[0]\n",
    "        \n",
    "        # But after the denomination\n",
    "        denom_match = re.search(r'\\d+(?:[/.]?\\d+)?\\s*(?:c|C|col|p|r|centim|centav|colon|peso|real)', \n",
    "                               text_before_delim, re.IGNORECASE)\n",
    "        if denom_match:\n",
    "            text_after_denom = text_before_delim[denom_match.end():].strip()\n",
    "            colors_matches = self.COLORS.findall(text_after_denom)\n",
    "            if colors_matches:\n",
    "                if isinstance(colors_matches[0], tuple):\n",
    "                    color_parts = [p.strip() for p in colors_matches[0] if p.strip()]\n",
    "                    return ' '.join(color_parts) if color_parts else None\n",
    "                return colors_matches[0]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def extract_attributes(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract philatelic attributes including overprint colors\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        found = []\n",
    "        \n",
    "        for attr in self.ATTRIBUTES:\n",
    "            if attr in text_lower:\n",
    "                found.append(attr)\n",
    "        \n",
    "        # Special: overprint colors (store as attribute, not as stamp color)\n",
    "        if 'op' in text_lower or 'overprint' in text_lower:\n",
    "            # Pattern: \"blue op\" or \"op in red\" or \"red overprint\"\n",
    "            op_color_patterns = [\n",
    "                r'(black|blue|red|green|violet|brown|orange|yellow)\\s+op\\b',\n",
    "                r'\\bop\\b.*?\\bin\\s+(black|blue|red|green|violet|brown|orange|yellow)',\n",
    "                r'(black|blue|red|green|violet|brown|orange|yellow)\\s+overprint'\n",
    "            ]\n",
    "            \n",
    "            for pattern in op_color_patterns:\n",
    "                matches = re.finditer(pattern, text_lower)\n",
    "                for match in matches:\n",
    "                    color = match.group(1)\n",
    "                    attr_name = f'overprint_{color}'\n",
    "                    if attr_name not in found:\n",
    "                        found.append(attr_name)\n",
    "        \n",
    "        return found\n",
    "    \n",
    "    def parse_catalog_entry(self, line: str) -> List[Dict]:\n",
    "        \"\"\"Parse line and extract catalog entries with proper denomination handling\"\"\"\n",
    "        entries = []\n",
    "        \n",
    "        # OCR CORRECTION\n",
    "        line = re.sub(r'\\b(DP|PP|S)I(\\d)', r'\\g<1>1\\2', line)\n",
    "        line = re.sub(r'\\b(DP|PP|S)I([a-z])', r'\\g<1>1\\2', line)\n",
    "        line = re.sub(r'\\b(S|PP|DP)l([a-z])', r'\\g<1>1\\2', line)\n",
    "        \n",
    "        # FILTERS\n",
    "        if re.search(r'Decree[s]?\\s*#\\s*\\d+', line, re.IGNORECASE):\n",
    "            return entries\n",
    "        if re.search(r'\\(Ref\\s+[A-Z]+\\s+\\d+', line, re.IGNORECASE):\n",
    "            return entries\n",
    "        if re.search(r'Accord\\s+\\d+', line, re.IGNORECASE):\n",
    "            return entries\n",
    "        \n",
    "        # Filter OCR corruption (2011 repeated 3+ times)\n",
    "        match = re.search(r'\\b(\\d+[a-zA-Z]*)\\b', line)\n",
    "        if match:\n",
    "            first_cat = match.group(1)\n",
    "            count = len(re.findall(rf'\\b{re.escape(first_cat)}\\b', line))\n",
    "            if count >= 3:\n",
    "                return entries\n",
    "        \n",
    "        # NEW: Handle bare numbers separated by newlines (e.g., \"5\\n\\n\\n6\\n\\n\\n7\")\n",
    "        bare_numbers = re.findall(r'^(\\d+)$', line, re.MULTILINE)\n",
    "        if bare_numbers and len(bare_numbers) >= 2:\n",
    "            for num in bare_numbers:\n",
    "                if int(num) <= 500:  # Reasonable stamp number\n",
    "                    entries.append({\n",
    "                        'catalog_number': num,\n",
    "                        'node_type': 'stamp',\n",
    "                        'sub_type': 'regular_issue',\n",
    "                        'base_stamp': None\n",
    "                    })\n",
    "            return entries\n",
    "        \n",
    "        # NEW: Handle concatenated M entries (M12M12a1c green...)\n",
    "        # Pattern: M## followed immediately by M##a or more M entries\n",
    "        m_pattern = r'\\b(M\\d+[a-z]?)\\b'\n",
    "        m_matches = list(re.finditer(m_pattern, line, re.IGNORECASE))\n",
    "        if len(m_matches) >= 2:\n",
    "            for m_match in m_matches:\n",
    "                cat_num = m_match.group(1).upper()\n",
    "                base = self.extract_base_stamp_number(cat_num)\n",
    "                if cat_num not in [e['catalog_number'] for e in entries]:\n",
    "                    entries.append({\n",
    "                        'catalog_number': cat_num,\n",
    "                        'node_type': 'specimen',\n",
    "                        'sub_type': 'specimen',\n",
    "                        'base_stamp': base\n",
    "                    })\n",
    "            # If we found M entries, return early to avoid duplicate processing\n",
    "            if entries:\n",
    "                return entries\n",
    "        \n",
    "        # NEW: Handle varieties with space after number (LaTeX cleaned: \"7 a surcharge...\")\n",
    "        # This handles the pattern after clean_latex converts \"${ }^{7} \\\\mathrm{a}\" to \"7 a\"\n",
    "        variety_space_pattern = r'\\b(\\d+)\\s+([a-z])\\b'\n",
    "        for match in re.finditer(variety_space_pattern, line):\n",
    "            num = match.group(1)\n",
    "            letter = match.group(2)\n",
    "            cat_num = f\"{num}{letter}\"\n",
    "            # Skip if this looks like a dimension or other non-catalog pattern\n",
    "            if int(num) > 2000:\n",
    "                continue\n",
    "            if cat_num not in [e['catalog_number'] for e in entries]:\n",
    "                entries.append({\n",
    "                    'catalog_number': cat_num,\n",
    "                    'node_type': 'variety',\n",
    "                    'sub_type': 'variety',\n",
    "                    'base_stamp': num\n",
    "                })\n",
    "        \n",
    "        # Die Proof: DP###\n",
    "        for match in re.finditer(r'\\bDP(\\d+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"DP{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'proof',\n",
    "                'sub_type': 'die_proof',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Plate Proof: PP###\n",
    "        for match in re.finditer(r'\\bPP(\\d+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"PP{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'proof',\n",
    "                'sub_type': 'plate_proof',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Color Proof: CP###\n",
    "        for match in re.finditer(r'\\bCP(\\d+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"CP{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'proof',\n",
    "                'sub_type': 'color_proof',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Specimen: S### (but not in concatenated M pattern already handled)\n",
    "        for match in re.finditer(r'\\bS(\\d+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"S{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'specimen',\n",
    "                'sub_type': 'specimen',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Essay: E###\n",
    "        for match in re.finditer(r'\\bE([I\\d]+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"E{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'essay',\n",
    "                'sub_type': 'essay',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Imperforate: I###\n",
    "        for match in re.finditer(r'\\bI(\\d+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"I{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'variety',\n",
    "                'sub_type': 'imperforate',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Overprint Proof: OP###\n",
    "        for match in re.finditer(r'\\bOP(\\d+[a-zA-Z]*)\\b', line, re.IGNORECASE):\n",
    "            cat_num = f\"OP{match.group(1)}\"\n",
    "            base = self.extract_base_stamp_number(cat_num)\n",
    "            entries.append({\n",
    "                'catalog_number': cat_num,\n",
    "                'node_type': 'proof',\n",
    "                'sub_type': 'overprint_proof',\n",
    "                'base_stamp': base\n",
    "            })\n",
    "        \n",
    "        # Base stamp with denomination\n",
    "        for match in re.finditer(\n",
    "            r'(?:^|\\s)(\\d+[a-zA-Z]*)\\s+(\\d+(?:[/.]?\\d+|½|¼)?)\\s*'\n",
    "            r'(c|C|col|colón|colones|p|peso|pesos|r|real|reales|'\n",
    "            r'centim|céntim|centav|céntav)\\b',\n",
    "            line, re.IGNORECASE\n",
    "        ):\n",
    "            cat_num = match.group(1).strip()\n",
    "            denom_value = match.group(2)\n",
    "            \n",
    "            try:\n",
    "                if int(denom_value.split('.')[0].split('/')[0]) > 500:\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if any(e['catalog_number'] == cat_num for e in entries):\n",
    "                continue\n",
    "            \n",
    "            if re.match(r'\\d+[a-zA-Z]+$', cat_num, re.IGNORECASE):\n",
    "                base = self.extract_base_stamp_number(cat_num)\n",
    "                entries.append({\n",
    "                    'catalog_number': cat_num,\n",
    "                    'node_type': 'variety',\n",
    "                    'sub_type': 'variety',\n",
    "                    'base_stamp': base\n",
    "                })\n",
    "            else:\n",
    "                entries.append({\n",
    "                    'catalog_number': cat_num,\n",
    "                    'node_type': 'stamp',\n",
    "                    'sub_type': 'regular_issue',\n",
    "                    'base_stamp': None\n",
    "                })\n",
    "        \n",
    "        return entries\n",
    "    def extract_denominations(self, line: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Extract denominations with their normalized values.\"\"\"\n",
    "        denoms = []\n",
    "        \n",
    "        for match in self.DENOMINATION.finditer(line):\n",
    "            value = match.group(1)\n",
    "            unit = match.group(2)\n",
    "            \n",
    "            # ONLY skip if it's EXACTLY a 2-character catalog pattern at word boundary\n",
    "            # AND appears at the START of text (catalog numbers come first)\n",
    "            matched_text = match.group(0)\n",
    "            \n",
    "            # Pattern: \"1c\" or \"2a\" appearing isolated (not \"5c\" in middle of sentence)\n",
    "            if re.match(r'^[1-9][a-z]$', matched_text):\n",
    "                # Check if this appears at the very start or after whitespace only\n",
    "                match_pos = match.start()\n",
    "                prefix = line[:match_pos].strip()\n",
    "                \n",
    "                # If there's meaningful text before this match, it's likely a real denomination\n",
    "                if prefix and not re.match(r'^\\d+$', prefix):\n",
    "                    # Text before suggests this is mid-sentence denomination, keep it\n",
    "                    pass\n",
    "                elif not prefix or re.match(r'^\\d+$', prefix):\n",
    "                    # This is at start or after just a number - might be catalog number\n",
    "                    # Skip ONLY if followed by space and descriptive text (not another number)\n",
    "                    suffix = line[match.end():match.end()+20].strip()\n",
    "                    if suffix and re.match(r'^[a-zA-Z]', suffix):\n",
    "                        # Followed by letters = likely catalog number like \"1c double\"\n",
    "                        continue\n",
    "            \n",
    "            # Skip quantities\n",
    "            try:\n",
    "                num_val = float(value.replace('½', '.5').replace('¼', '.25').split('/')[0])\n",
    "                if num_val > 500:\n",
    "                    continue\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            denoms.append((value, unit))\n",
    "        \n",
    "        return denoms\n",
    "    \n",
    "    def parse_element(self, element: Dict, context_before: List[str], \n",
    "                 context_after: List[str], page_number: int) -> List[PhilatelicNode]:\n",
    "        \"\"\"Parse element and extract nodes - EXPERT VERSION\"\"\"\n",
    "        text = element.get('text', '')\n",
    "        label = element.get('label', '')\n",
    "        reading_order = element.get('reading_order', 0)\n",
    "        \n",
    "        if label in ['header', 'foot', 'fig']:\n",
    "            return []\n",
    "        \n",
    "        # CRITICAL: <sec>, <sub_sec>, <sub_sub_sec> define hierarchy\n",
    "        # Priority: sub_sub_sec > sub_sec > sec\n",
    "        if label == 'sec':\n",
    "            self.current_sec = text\n",
    "            if not hasattr(self, 'current_sub_sec') or not self.current_sub_sec:\n",
    "                self.current_issue = text\n",
    "            year_match = self.YEAR.search(text)\n",
    "            if year_match:\n",
    "                self.current_year = year_match.group(1)\n",
    "            return []\n",
    "\n",
    "        if label == 'sub_sec':\n",
    "            self.current_sub_sec = text\n",
    "            if not hasattr(self, 'current_sub_sub_sec') or not self.current_sub_sub_sec:\n",
    "                self.current_issue = text\n",
    "            return []\n",
    "\n",
    "        if label == 'sub_sub_sec':\n",
    "            self.current_sub_sub_sec = text\n",
    "            self.current_issue = text\n",
    "            return []\n",
    "        \n",
    "        # Section markers\n",
    "        text_lower = text.lower().strip()\n",
    "        if any(section in text_lower for section in ['proof', 'proofs', 'regular issue', 'essays', 'essay', 'plate proof', 'die proof', 'specimen', 'photographic proof']):\n",
    "            for section in ['proof', 'proofs', 'regular issue', 'essays', 'essay', 'plate proof', 'die proof', 'specimen', 'regular issues', 'photographic proof', 'photographic proofs']:\n",
    "                if section in text_lower:\n",
    "                    self.current_section = section\n",
    "                    break\n",
    "        \n",
    "        nodes = []\n",
    "        \n",
    "        # Handle tables\n",
    "        if label == 'tab':\n",
    "            nodes.extend(self._parse_table(text, page_number, reading_order, context_before))\n",
    "            return nodes\n",
    "        \n",
    "        # Check if paragraph contains catalog entries\n",
    "        # Special handling for bare catalog numbers separated by newlines\n",
    "        if label == 'para':\n",
    "            lines_temp = [l.strip() for l in text.split('\\n') if l.strip()]\n",
    "            bare_num_pattern = r'^\\d+$'\n",
    "            \n",
    "            # Check if ALL lines are just bare numbers\n",
    "            if len(lines_temp) >= 2 and all(re.match(bare_num_pattern, l) for l in lines_temp):\n",
    "                for num in lines_temp:\n",
    "                    if 1 <= int(num) <= 500:\n",
    "                        node = PhilatelicNode(\n",
    "                            catalog_number=num,\n",
    "                            node_type='stamp',\n",
    "                            sub_type='regular_issue',\n",
    "                            denomination=None,\n",
    "                            color=None,\n",
    "                            year=self.current_year,\n",
    "                            issue_name=self.current_issue,\n",
    "                            quantity=None,\n",
    "                            page_number=page_number,\n",
    "                            reading_order=reading_order,\n",
    "                            raw_text=text,\n",
    "                            context_before=context_before[-3:],\n",
    "                            context_after=context_after[:1],\n",
    "                            attributes=[],\n",
    "                            reference_numbers=[],\n",
    "                            base_stamp=None\n",
    "                        )\n",
    "                        nodes.append(node)\n",
    "                return nodes  # Only return for THIS paragraph\n",
    "    \n",
    "            # If not bare numbers, check for paragraph entries\n",
    "            if re.search(r'(?:^|\\n)\\s*[A-Z]*\\d+[a-z]*\\s+', text):\n",
    "                nodes.extend(self._parse_paragraph_entries(\n",
    "                    text, page_number, reading_order, context_before\n",
    "                ))\n",
    "                if nodes:\n",
    "                    return nodes\n",
    "        \n",
    "        # Clean and process remaining content\n",
    "        clean_text = self.clean_latex(text)\n",
    "        \n",
    "        # DEBUG: Print cleaned text for specific problematic elements\n",
    "        if page_number in [10, 11] and label == 'para' and reading_order in [17, 10, 11]:\n",
    "            print(f\"\\n=== DEBUG PAGE {page_number}, ORDER {reading_order} ===\")\n",
    "            print(f\"ORIGINAL: {text[:200]}\")\n",
    "            print(f\"CLEANED:  {clean_text[:200]}\")\n",
    "            print(f\"===\\n\")\n",
    "        \n",
    "        lines = [line.strip() for line in clean_text.split('\\n') if line.strip()]\n",
    "        \n",
    "        for line in lines:\n",
    "            entries = self.parse_catalog_entry(line)\n",
    "            \n",
    "            if not entries:\n",
    "                continue\n",
    "            \n",
    "            # Extract denominations\n",
    "            denoms_raw = self.extract_denominations(line)\n",
    "            denominations = [self.normalize_denomination(v, u) for v, u in denoms_raw]\n",
    "            \n",
    "            # Extract colors\n",
    "            colors = []\n",
    "            for entry in entries:\n",
    "                cat_num = entry['catalog_number']\n",
    "                color = self.extract_stamp_color(line, cat_num)\n",
    "                colors.append(color)\n",
    "            \n",
    "            # Fallback color extraction\n",
    "            if not any(colors):\n",
    "                colors_matches = self.COLORS.findall(line)\n",
    "                for color_match in colors_matches:\n",
    "                    if isinstance(color_match, tuple):\n",
    "                        color_parts = [p.strip() for p in color_match if p.strip()]\n",
    "                        if color_parts:\n",
    "                            colors.append(' '.join(color_parts))\n",
    "                    else:\n",
    "                        colors.append(color_match.strip())\n",
    "            \n",
    "            # Extract quantity\n",
    "            quantity = None\n",
    "            qty_match = self.QUANTITY.search(line)\n",
    "            if qty_match:\n",
    "                raw_quantity = qty_match.group(1) or qty_match.group(2)\n",
    "                quantity = raw_quantity.replace(',', '')\n",
    "            \n",
    "            references = self.REFERENCE.findall(line)\n",
    "            attributes = self.extract_attributes(line)\n",
    "            \n",
    "            # Create nodes\n",
    "            for i, entry in enumerate(entries):\n",
    "                cat_num = entry['catalog_number']\n",
    "                node_type = entry['node_type']\n",
    "                sub_type = entry['sub_type']\n",
    "                base_stamp = entry['base_stamp']\n",
    "                \n",
    "                if node_type == 'stamp' and cat_num in self.stamps_seen:\n",
    "                    continue\n",
    "                \n",
    "                node = PhilatelicNode(\n",
    "                    catalog_number=cat_num,\n",
    "                    node_type=node_type,\n",
    "                    sub_type=sub_type,\n",
    "                    denomination=denominations[0] if denominations else None,\n",
    "                    color=colors[i] if i < len(colors) else (colors[0] if colors else None),\n",
    "                    year=self.current_year,\n",
    "                    issue_name=self.current_issue,\n",
    "                    quantity=quantity if node_type == 'stamp' else None,\n",
    "                    page_number=page_number,\n",
    "                    reading_order=reading_order,\n",
    "                    raw_text=line,\n",
    "                    context_before=context_before[-3:],\n",
    "                    context_after=context_after[:1],\n",
    "                    attributes=attributes,\n",
    "                    reference_numbers=references,\n",
    "                    base_stamp=base_stamp\n",
    "                )\n",
    "                \n",
    "                if node_type == 'stamp' and self.current_section and 'regular issue' in self.current_section.lower():\n",
    "                    self.stamps_seen.add(cat_num)\n",
    "                \n",
    "                nodes.append(node)\n",
    "        \n",
    "        return nodes\n",
    "    def _parse_paragraph_entries(self, text: str, page_number: int, \n",
    "                             reading_order: int, context: List[str]) -> List[PhilatelicNode]:\n",
    "        \"\"\"Parse catalog entries from paragraph format (not tables)\"\"\"\n",
    "        nodes = []\n",
    "        \n",
    "        # Pattern: catalog number at start of line/sentence\n",
    "        # Examples: \"5    1c on ½r blue\", \"7a   surcharge on 1A\"\n",
    "        pattern = r'(?:^|\\n)\\s*([A-Z]*\\d+[a-z]*)\\s+(.*?)(?=\\n\\s*[A-Z]*\\d+[a-z]*\\s+|\\Z)'\n",
    "        \n",
    "        for match in re.finditer(pattern, text, re.MULTILINE | re.DOTALL):\n",
    "            cat_num = match.group(1).strip()\n",
    "            description = match.group(2).strip()\n",
    "            \n",
    "            # Determine node type\n",
    "            if cat_num.startswith(('DP', 'PP', 'CP')):\n",
    "                node_type = 'proof'\n",
    "                sub_type = {'DP': 'die_proof', 'PP': 'plate_proof', 'CP': 'color_proof'}[cat_num[:2]]\n",
    "                base_stamp = self.extract_base_stamp_number(cat_num)\n",
    "            elif cat_num.startswith(('S', 'M')):\n",
    "                node_type = 'specimen'\n",
    "                sub_type = 'specimen'\n",
    "                base_stamp = self.extract_base_stamp_number(cat_num)\n",
    "            elif re.match(r'^\\d+[a-z]$', cat_num):\n",
    "                node_type = 'variety'\n",
    "                sub_type = 'variety'\n",
    "                base_stamp = cat_num[:-1]\n",
    "            else:\n",
    "                node_type = 'stamp'\n",
    "                sub_type = 'regular_issue'\n",
    "                base_stamp = None\n",
    "            \n",
    "            # Extract denomination\n",
    "            denoms_raw = self.extract_denominations(description)\n",
    "            denomination = self.normalize_denomination(*denoms_raw[0]) if denoms_raw else None\n",
    "            \n",
    "            # Check for surcharge pattern: \"Xc on Yr\"\n",
    "            surcharge_match = re.search(r'(\\d+)\\s*c\\s+on\\s+([½¼\\d/]+)\\s*([rp])', description)\n",
    "            if surcharge_match:\n",
    "                new_val = surcharge_match.group(1)\n",
    "                old_val = surcharge_match.group(2)\n",
    "                old_unit = surcharge_match.group(3)\n",
    "                denomination = f\"{new_val}c on {old_val}{old_unit}\"\n",
    "            \n",
    "            # Extract color\n",
    "            color = self.extract_stamp_color(description, cat_num)\n",
    "            \n",
    "            # Extract attributes\n",
    "            attributes = self.extract_attributes(description)\n",
    "            \n",
    "            node = PhilatelicNode(\n",
    "                catalog_number=cat_num,\n",
    "                node_type=node_type,\n",
    "                sub_type=sub_type,\n",
    "                denomination=denomination,\n",
    "                color=color,\n",
    "                year=self.current_year,\n",
    "                issue_name=self.current_issue,\n",
    "                quantity=None,\n",
    "                page_number=page_number,\n",
    "                reading_order=reading_order,\n",
    "                raw_text=description,\n",
    "                context_before=context[-3:],\n",
    "                attributes=attributes,\n",
    "                base_stamp=base_stamp\n",
    "            )\n",
    "            nodes.append(node)\n",
    "        \n",
    "        return nodes\n",
    "    \n",
    "    def _parse_table(self, html_text: str, page_number: int, \n",
    "                reading_order: int, context: List[str]) -> List[PhilatelicNode]:\n",
    "        \"\"\"Parse table elements with expert denomination handling\"\"\"\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        rows = []\n",
    "        \n",
    "        for tr in soup.find_all('tr'):\n",
    "            cells = [td.get_text(strip=True) for td in tr.find_all('td')]\n",
    "            if cells and not all(c == '' for c in cells):\n",
    "                rows.append(cells)\n",
    "        \n",
    "        nodes = []\n",
    "        last_denomination = None\n",
    "        \n",
    "        for row in rows:\n",
    "            created_catalog_numbers = set(node.catalog_number for node in nodes)\n",
    "            \n",
    "            if not row or len(row) < 2:\n",
    "                continue\n",
    "            \n",
    "            print(f\"DEBUG: Processing table row with {len(row)} cells: {row}\")\n",
    "            \n",
    "            # Skip headers\n",
    "            header_keywords = ['perf', 'imperf', 'date', 'order', 'plate', 'essays']\n",
    "            if len(row) <= 5 and all(any(kw in cell.lower() for kw in header_keywords) for cell in row if cell):\n",
    "                print(f\"DEBUG: Skipping header row\")\n",
    "                continue\n",
    "            \n",
    "            first_cell = row[0].strip()\n",
    "            entries = []\n",
    "            \n",
    "            # Case 1: 2-cell variety rows\n",
    "            if len(row) == 2 and re.match(r'^\\d+[a-zA-Z]+$', first_cell):\n",
    "                base = self.extract_base_stamp_number(first_cell)\n",
    "                entries = [{\n",
    "                    'catalog_number': first_cell,\n",
    "                    'node_type': 'variety',\n",
    "                    'sub_type': 'variety',\n",
    "                    'base_stamp': base\n",
    "                }]\n",
    "                row_text = ' '.join(row)\n",
    "                print(f\"DEBUG: 2-cell variety row: {first_cell} -> base {base}\")\n",
    "                last_denomination = None\n",
    "            \n",
    "            # Case 2: 2-cell bare number rows\n",
    "            elif len(row) == 2 and re.match(r'^\\d+$', first_cell):\n",
    "                desc = row[1].lower()\n",
    "                variety_keywords = ['perf', 'diagonal', 'horizontal', 'cracked', 'impression', 'pair', 'inverted']\n",
    "                \n",
    "                # Special case: catalog #1 is likely a real stamp even with variety keywords\n",
    "                if first_cell == '1' and any(kw in desc for kw in variety_keywords):\n",
    "                    entries = [{\n",
    "                        'catalog_number': first_cell,\n",
    "                        'node_type': 'stamp',\n",
    "                        'sub_type': 'regular_issue',\n",
    "                        'base_stamp': None\n",
    "                    }]\n",
    "                    row_text = ' '.join(row)\n",
    "                    print(f\"DEBUG: Special case - stamp #1\")\n",
    "                    last_denomination = None\n",
    "                \n",
    "                elif any(kw in desc for kw in variety_keywords):\n",
    "                    print(f\"DEBUG: Row {first_cell} appears to be variety with missing letter, skipping\")\n",
    "                    continue\n",
    "                else:\n",
    "                    entries = [{\n",
    "                        'catalog_number': first_cell,\n",
    "                        'node_type': 'stamp',\n",
    "                        'sub_type': 'regular_issue',\n",
    "                        'base_stamp': None\n",
    "                    }]\n",
    "                    row_text = ' '.join(row)\n",
    "                    print(f\"DEBUG: 2-cell stamp row: {first_cell}\")\n",
    "                    last_denomination = None\n",
    "            \n",
    "            # Case 3: Multi-cell rows (3+)\n",
    "            elif len(row) >= 3 and re.match(r'^\\d+[a-zA-Z]*$', first_cell):\n",
    "                rest_of_row = ' '.join(row[1:])\n",
    "                row_text_full = ' '.join(row)\n",
    "                \n",
    "                print(f\"DEBUG: Found catalog number in first cell: {first_cell}\")\n",
    "                \n",
    "                # Subcase 3a: It's a variety\n",
    "                if re.match(r'^\\d+[a-zA-Z]+$', first_cell):\n",
    "                    base = self.extract_base_stamp_number(first_cell)\n",
    "                    entries = [{\n",
    "                        'catalog_number': first_cell,\n",
    "                        'node_type': 'variety',\n",
    "                        'sub_type': 'variety',\n",
    "                        'base_stamp': base\n",
    "                    }]\n",
    "                    print(f\"DEBUG: Identified as variety: {first_cell} -> base {base}\")\n",
    "                    \n",
    "                    # Check column 2 for additional varieties\n",
    "                    if len(row) >= 2:\n",
    "                        second_cell = row[1].strip()\n",
    "                        # Fixed version - only single letters or specific patterns:\n",
    "                        additional_varieties = []\n",
    "                        # Pattern 1: Single lowercase letter (a, b, c)\n",
    "                        for match in re.finditer(r'\\b([a-z])\\b', second_cell):\n",
    "                            additional_varieties.append(match.group(1))\n",
    "                        # Pattern 2: Uppercase + lowercase (like \"Aa\" from \"1Aa\")\n",
    "                        for match in re.finditer(r'([A-Z][a-z])', second_cell):\n",
    "                            additional_varieties.append(match.group(1))\n",
    "                        \n",
    "                        # Filter out if it's part of a color word\n",
    "                        color_words = ['dark', 'light', 'deep', 'pale', 'bright', 'red', 'green', 'blue', \n",
    "                                    'violet', 'yellow', 'orange', 'brown', 'black', 'white', 'gray', 'rose']\n",
    "                        additional_varieties = [v for v in additional_varieties \n",
    "                                            if not any(word.startswith(v.lower()) for word in color_words)]\n",
    "                        \n",
    "                        for var_code in additional_varieties:\n",
    "                            full_cat = f\"{base}{var_code}\"\n",
    "                            # Skip duplicates\n",
    "                            if full_cat not in [e['catalog_number'] for e in entries] and full_cat != first_cell and full_cat not in created_catalog_numbers:\n",
    "                                entries.append({\n",
    "                                    'catalog_number': full_cat,\n",
    "                                    'node_type': 'variety',\n",
    "                                    'sub_type': 'variety',\n",
    "                                    'base_stamp': base\n",
    "                                })\n",
    "                                print(f\"DEBUG: Additional variety: {full_cat}\")\n",
    "                \n",
    "                # Subcase 3b: It's a stamp\n",
    "                else:\n",
    "                    denoms_raw = self.extract_denominations(rest_of_row)\n",
    "                    \n",
    "                    if denoms_raw:\n",
    "                        # Found denomination in rest of row\n",
    "                        entries = [{\n",
    "                            'catalog_number': first_cell,\n",
    "                            'node_type': 'stamp',\n",
    "                            'sub_type': 'regular_issue',\n",
    "                            'base_stamp': None\n",
    "                        }]\n",
    "                        print(f\"DEBUG: Stamp with denomination in row: {first_cell}\")\n",
    "                    else:\n",
    "                        # No denomination found - use heuristics\n",
    "                        second_cell = row[1].strip() if len(row) > 1 else \"\"\n",
    "                        third_col_text = ' '.join(row[2:]) if len(row) >= 3 else \"\"\n",
    "                        \n",
    "                        # Heuristic 1: Column 2 is a variety code\n",
    "                        if re.match(r'^\\d+[a-z]$', second_cell):\n",
    "                            denoms_in_desc = self.extract_denominations(third_col_text)\n",
    "                            last_col = row[-1].strip() if row else \"\"\n",
    "                            \n",
    "                            if denoms_in_desc:\n",
    "                                entries = [{\n",
    "                                    'catalog_number': first_cell,\n",
    "                                    'node_type': 'stamp',\n",
    "                                    'sub_type': 'regular_issue',\n",
    "                                    'base_stamp': None\n",
    "                                }]\n",
    "                                rest_of_row = third_col_text\n",
    "                                print(f\"DEBUG: Stamp {first_cell} - denom in column 3\")\n",
    "                            elif re.match(r'^\\d{2,3},\\d{3}$', last_col):\n",
    "                                entries = [{\n",
    "                                    'catalog_number': first_cell,\n",
    "                                    'node_type': 'stamp',\n",
    "                                    'sub_type': 'regular_issue',\n",
    "                                    'base_stamp': None\n",
    "                                }]\n",
    "                                rest_of_row = third_col_text\n",
    "                                print(f\"DEBUG: Stamp {first_cell} inferred from quantity\")\n",
    "                                last_denomination = None\n",
    "                            else:\n",
    "                                print(f\"DEBUG: No denomination for {first_cell}, skipping\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Add variety from column 2\n",
    "                            base_of_variety = second_cell[:-1]\n",
    "                            entries.append({\n",
    "                                'catalog_number': second_cell,\n",
    "                                'node_type': 'variety',\n",
    "                                'sub_type': 'variety',\n",
    "                                'base_stamp': base_of_variety\n",
    "                            })\n",
    "                            print(f\"DEBUG: Added variety {second_cell}\")\n",
    "                        else:\n",
    "                            print(f\"DEBUG: No denomination for {first_cell}, skipping\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Check for variety in same row (after stamp created)\n",
    "                    if entries and entries[0]['node_type'] == 'stamp' and len(row) >= 2:\n",
    "                        second_cell = row[1].strip()\n",
    "                        if re.match(r'^\\d+[a-z]$', second_cell) and second_cell not in [e['catalog_number'] for e in entries]:\n",
    "                            base_of_variety = second_cell[:-1]\n",
    "                            entries.append({\n",
    "                                'catalog_number': second_cell,\n",
    "                                'node_type': 'variety',\n",
    "                                'sub_type': 'variety',\n",
    "                                'base_stamp': base_of_variety\n",
    "                            })\n",
    "                            print(f\"DEBUG: Variety {second_cell} in same row\")\n",
    "                \n",
    "                row_text = row_text_full\n",
    "            \n",
    "            # Case 4: Normal parsing\n",
    "            else:\n",
    "                row_text = ' '.join(row)\n",
    "                print(f\"DEBUG: Normal parsing\")\n",
    "                entries = self.parse_catalog_entry(row_text)\n",
    "            \n",
    "            print(f\"DEBUG: Found {len(entries)} entries: {[e['catalog_number'] for e in entries]}\")\n",
    "            \n",
    "            if entries:\n",
    "                surcharge_match = self.SURCHARGE.search(row_text)\n",
    "                surcharge_info = None\n",
    "                if surcharge_match:\n",
    "                    new_value = surcharge_match.group(1)\n",
    "                    old_value = surcharge_match.group(2)\n",
    "                    old_unit = surcharge_match.group(3)\n",
    "                    surcharge_info = f\"{new_value}c on {old_value} {old_unit}\"\n",
    "                \n",
    "                denoms_raw = self.extract_denominations(row_text)\n",
    "                denominations = [self.normalize_denomination(v, u) for v, u in denoms_raw]\n",
    "                print(f\"DEBUG: Denominations: {denominations}\")\n",
    "                \n",
    "                if not denominations and last_denomination:\n",
    "                    denominations = [last_denomination]\n",
    "                    print(f\"DEBUG: Using last denomination: {last_denomination}\")\n",
    "                elif denominations:\n",
    "                    last_denomination = denominations[0]\n",
    "                \n",
    "                if not denominations:\n",
    "                    special_match = re.search(\n",
    "                        r'([E]\\d+[a-zA-Z]*(?:\\s+[E]\\d+[a-zA-Z]*)*)\\s+(\\d+)\\s+',\n",
    "                        row_text, re.IGNORECASE\n",
    "                    )\n",
    "                    if special_match:\n",
    "                        denom_value = special_match.group(2)\n",
    "                        denominations = [self.normalize_denomination(denom_value, 'c')]\n",
    "                        last_denomination = denominations[0]\n",
    "                \n",
    "                colors = []\n",
    "                for entry in entries:\n",
    "                    color = self.extract_stamp_color(row_text, entry['catalog_number'])\n",
    "                    colors.append(color)\n",
    "                \n",
    "                attributes = self.extract_attributes(row_text)\n",
    "                \n",
    "                quantity = None\n",
    "                qty_match = self.QUANTITY.search(row_text)\n",
    "                if qty_match:\n",
    "                    raw_quantity = qty_match.group(1) or qty_match.group(2)\n",
    "                    quantity = raw_quantity.replace(',', '')\n",
    "                    print(f\"DEBUG: Quantity: {quantity}\")\n",
    "                \n",
    "                for i, entry in enumerate(entries):\n",
    "                    cat_num = entry['catalog_number']\n",
    "                    node_type = entry['node_type']\n",
    "                    sub_type = entry['sub_type']\n",
    "                    base_stamp = entry['base_stamp']\n",
    "                    \n",
    "                    final_attributes = attributes.copy()\n",
    "                    if surcharge_info:\n",
    "                        final_attributes.append(f'surcharge_{surcharge_info}')\n",
    "                    \n",
    "                    if node_type == 'stamp' and cat_num in self.stamps_seen:\n",
    "                        print(f\"DEBUG: Skipping duplicate stamp {cat_num}\")\n",
    "                        continue\n",
    "                    \n",
    "                    node = PhilatelicNode(\n",
    "                        catalog_number=cat_num,\n",
    "                        node_type=node_type,\n",
    "                        sub_type=sub_type,\n",
    "                        denomination=denominations[0] if denominations else None,\n",
    "                        color=colors[i] if i < len(colors) else (colors[0] if colors else None),\n",
    "                        year=self.current_year,\n",
    "                        issue_name=self.current_issue,\n",
    "                        quantity=quantity if node_type == 'stamp' else None,\n",
    "                        page_number=page_number,\n",
    "                        reading_order=reading_order,\n",
    "                        raw_text=row_text,\n",
    "                        context_before=context[-3:],\n",
    "                        attributes=final_attributes,\n",
    "                        base_stamp=base_stamp\n",
    "                    )\n",
    "                    nodes.append(node)\n",
    "                    print(f\"DEBUG: Created {cat_num} - {node.denomination} {node.color}\")\n",
    "                    \n",
    "                    if node_type == 'stamp' and self.current_section and 'regular' in self.current_section.lower():\n",
    "                        self.stamps_seen.add(cat_num)\n",
    "                        print(f\"DEBUG: Added {cat_num} to stamps_seen\")\n",
    "            \n",
    "        return nodes\n",
    "class PhilatelicExtractor:\n",
    "    \"\"\"Main extractor with expert Costa Rica knowledge\"\"\"\n",
    "    \n",
    "    def __init__(self, context_window: Tuple[int, int] = (-3, 1)):\n",
    "        self.context_window = context_window\n",
    "        self.parser = CostaRicaCatalogParser()\n",
    "    \n",
    "    def get_context(self, elements: List[Dict], index: int) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Get context around element\"\"\"\n",
    "        start = max(0, index + self.context_window[0])\n",
    "        end_after = min(len(elements), index + self.context_window[1] + 1)\n",
    "        \n",
    "        context_before = []\n",
    "        for i in range(start, index):\n",
    "            text = elements[i].get('text', '')\n",
    "            if text and elements[i].get('label') not in ['header', 'foot', 'fig']:\n",
    "                context_before.append(text)\n",
    "        \n",
    "        context_after = []\n",
    "        for i in range(index + 1, end_after):\n",
    "            text = elements[i].get('text', '')\n",
    "            if text and elements[i].get('label') not in ['header', 'foot', 'fig']:\n",
    "                context_after.append(text)\n",
    "        \n",
    "        return context_before, context_after\n",
    "    \n",
    "    def process_page(self, page_data: Dict) -> List[PhilatelicNode]:\n",
    "        \"\"\"Process single page\"\"\"\n",
    "        elements = page_data.get('elements', [])\n",
    "        page_number = page_data.get('page_number', 0)\n",
    "        \n",
    "        elements = sorted(elements, key=lambda x: x.get('reading_order', 0))\n",
    "        \n",
    "        # Reset state for new page\n",
    "        # BUT keep issue_name if no new <sec> found (issues can span pages)\n",
    "        self.parser.current_section = None\n",
    "        self.parser.current_sub_sec = None\n",
    "        self.parser.current_sub_sub_sec = None\n",
    "        # Don't reset current_issue - it carries over pages\n",
    "        # Don't reset current_year - it carries over\n",
    "        # Don't reset stamps_seen - we need to track across pages\n",
    "        \n",
    "        all_nodes = []\n",
    "        \n",
    "        for i, element in enumerate(elements):\n",
    "            context_before, context_after = self.get_context(elements, i)\n",
    "            nodes = self.parser.parse_element(element, context_before, context_after, page_number)\n",
    "            all_nodes.extend(nodes)\n",
    "        \n",
    "        return all_nodes\n",
    "    \n",
    "    def build_relationships(self, nodes: List[PhilatelicNode]) -> List[Dict]:\n",
    "        \"\"\"Build relationships between nodes\"\"\"\n",
    "        relationships = []\n",
    "        \n",
    "        # Index nodes by catalog number\n",
    "        by_catalog = {n.catalog_number: n for n in nodes}\n",
    "        \n",
    "        for node in nodes:\n",
    "            if node.base_stamp and node.base_stamp in by_catalog:\n",
    "                base = by_catalog[node.base_stamp]\n",
    "                relationships.append({\n",
    "                    'from': node.catalog_number,\n",
    "                    'to': base.catalog_number,\n",
    "                    'type': f'{node.node_type.upper()}_OF',\n",
    "                    'sub_type': node.sub_type,\n",
    "                    'description': f\"{node.catalog_number} ({node.sub_type}) is a {node.node_type} of stamp {base.catalog_number}\"\n",
    "                })\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def process_sample_pages(self, input_path: str, start_page: int = 30, num_pages: int = 2) -> Dict:\n",
    "        \"\"\"Process sample pages\"\"\"\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, dict) and 'page_number' in data:\n",
    "            pages = [data]\n",
    "        elif isinstance(data, list):\n",
    "            pages = data\n",
    "        else:\n",
    "            pages = []\n",
    "        \n",
    "        pages = [p for p in pages if p.get('page_number', 0) >= start_page \n",
    "                 and p.get('page_number', 0) < start_page + num_pages]\n",
    "        \n",
    "        all_nodes = []\n",
    "        for page in pages:\n",
    "            nodes = self.process_page(page)\n",
    "            all_nodes.extend(nodes)\n",
    "        \n",
    "        # Build relationships\n",
    "        relationships = self.build_relationships(all_nodes)\n",
    "        \n",
    "        result = {\n",
    "            'total_nodes': len(all_nodes),\n",
    "            'nodes_by_type': {},\n",
    "            'all_nodes': [node.to_dict() for node in all_nodes],\n",
    "            'relationships': relationships,\n",
    "            'relationship_count': len(relationships)\n",
    "        }\n",
    "        \n",
    "        for node in all_nodes:\n",
    "            node_type = node.node_type\n",
    "            if node_type not in result['nodes_by_type']:\n",
    "                result['nodes_by_type'][node_type] = []\n",
    "            result['nodes_by_type'][node_type].append(node.to_dict())\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def inspect_json_structure(self, input_path: str, max_pages: int = 3):\n",
    "        \"\"\"Inspect JSON\"\"\"\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, dict) and 'page_number' in data:\n",
    "            pages = [data]\n",
    "        elif isinstance(data, list):\n",
    "            pages = data\n",
    "        else:\n",
    "            print(\"Unexpected JSON structure\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Total pages: {len(pages)}\")\n",
    "        print(f\"\\nInspecting first {min(max_pages, len(pages))} pages:\\n\")\n",
    "        \n",
    "        for page in pages[:max_pages]:\n",
    "            print(f\"Page {page.get('page_number')}:\")\n",
    "            elements = page.get('elements', [])\n",
    "            print(f\"  Elements: {len(elements)}\")\n",
    "            \n",
    "            types = {}\n",
    "            for el in elements:\n",
    "                label = el.get('label', 'unknown')\n",
    "                types[label] = types.get(label, 0) + 1\n",
    "            \n",
    "            print(f\"  Types: {types}\")\n",
    "            \n",
    "            # Show <sec> elements (issue names)\n",
    "            sec_elements = [el for el in elements if el.get('label') == 'sec']\n",
    "            if sec_elements:\n",
    "                print(f\"  Issues found:\")\n",
    "                for sec in sec_elements:\n",
    "                    print(f\"    - {sec.get('text', 'N/A')}\")\n",
    "            print()\n",
    "    \n",
    "    def process_file(self, input_path: str, output_path: str):\n",
    "        \"\"\"Process entire file\"\"\"\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        if isinstance(data, dict) and 'page_number' in data:\n",
    "            pages = [data]\n",
    "        elif isinstance(data, list):\n",
    "            pages = data\n",
    "        else:\n",
    "            pages = []\n",
    "        \n",
    "        all_nodes = []\n",
    "        # Reset parser state at start\n",
    "        self.parser.current_issue = None\n",
    "        self.parser.current_section = None\n",
    "        self.parser.current_year = None\n",
    "        self.parser.stamps_seen = set()\n",
    "        \n",
    "        for page in pages:\n",
    "            nodes = self.process_page(page)\n",
    "            all_nodes.extend(nodes)\n",
    "        \n",
    "        # Build relationships\n",
    "        relationships = self.build_relationships(all_nodes)\n",
    "        \n",
    "        output = {\n",
    "            'total_nodes': len(all_nodes),\n",
    "            'pages_processed': len(pages),\n",
    "            'nodes': [node.to_dict() for node in all_nodes],\n",
    "            'relationships': relationships,\n",
    "            'summary': {\n",
    "                'by_type': {},\n",
    "                'by_page': {},\n",
    "                'by_issue': {},\n",
    "                'by_denomination': {},\n",
    "                'relationship_count': len(relationships)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate statistics\n",
    "        for node in all_nodes:\n",
    "            node_type = node.node_type\n",
    "            page_num = node.page_number\n",
    "            \n",
    "            # By type\n",
    "            output['summary']['by_type'][node_type] = \\\n",
    "                output['summary']['by_type'].get(node_type, 0) + 1\n",
    "            \n",
    "            # By page\n",
    "            output['summary']['by_page'][str(page_num)] = \\\n",
    "                output['summary']['by_page'].get(str(page_num), 0) + 1\n",
    "            \n",
    "            # By issue\n",
    "            if node.issue_name:\n",
    "                output['summary']['by_issue'][node.issue_name] = \\\n",
    "                    output['summary']['by_issue'].get(node.issue_name, 0) + 1\n",
    "            \n",
    "            # By denomination\n",
    "            if node.denomination:\n",
    "                output['summary']['by_denomination'][node.denomination] = \\\n",
    "                    output['summary']['by_denomination'].get(node.denomination, 0) + 1\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba516ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_denomination_extraction():\n",
    "    \"\"\"Test comprehensive denomination extraction\"\"\"\n",
    "    \n",
    "    parser = CostaRicaCatalogParser()\n",
    "    \n",
    "    test_cases = [\n",
    "        # Centavos/céntimos\n",
    "        {'text': '99 15c black', 'expected': '15c', 'note': 'Standard centavos'},\n",
    "        {'text': 'DP100 5c violet', 'expected': '5c', 'note': 'Small value'},\n",
    "        {'text': 'PP101 100c orange', 'expected': '100c', 'note': '100 centavos (not Colones)'},\n",
    "        {'text': 'S102 15centimos black', 'expected': '15c', 'note': 'Full word \"centimos\"'},\n",
    "        \n",
    "        # Colones\n",
    "        {'text': '200 1col blue', 'expected': '1col', 'note': 'One colón'},\n",
    "        {'text': '201 5C green', 'expected': '5col', 'note': 'Capital C = Colones'},\n",
    "        {'text': '202 2colones red', 'expected': '2col', 'note': 'Full word \"colones\"'},\n",
    "        \n",
    "        # Pesos\n",
    "        {'text': '50 1p black', 'expected': '1p', 'note': '1 peso'},\n",
    "        {'text': '51 10pesos violet', 'expected': '10p', 'note': 'Full word \"pesos\"'},\n",
    "        \n",
    "        # Reales with fractions\n",
    "        {'text': '10 ½r black', 'expected': '0.5r', 'note': 'Half real'},\n",
    "        {'text': '11 1/2r violet', 'expected': '0.5r', 'note': 'Fraction 1/2 real'},\n",
    "        {'text': '12 2r green', 'expected': '2r', 'note': '2 reales'},\n",
    "        {'text': '13 ¼r orange', 'expected': '0.25r', 'note': 'Quarter real'},\n",
    "        \n",
    "        # Edge cases\n",
    "        {'text': '99 15c deep violet 1,000,000', 'expected': '15c', 'note': 'Implicit \"c\", quantity present'},\n",
    "        {'text': 'DP99 15c black #34009', 'expected': '15c', 'note': 'With reference number'},\n",
    "    ]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 1: DENOMINATION EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"{i}. {test['text']}\")\n",
    "        print(f\"   Note: {test['note']}\")\n",
    "        \n",
    "        denoms_raw = parser.extract_denominations(test['text'])\n",
    "        \n",
    "        if denoms_raw:\n",
    "            denom_normalized = parser.normalize_denomination(denoms_raw[0][0], denoms_raw[0][1])\n",
    "            \n",
    "            if denom_normalized == test['expected']:\n",
    "                print(f\"   ✅ PASS - Extracted: {denom_normalized}\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"   ❌ FAIL - Expected: {test['expected']}, Got: {denom_normalized}\")\n",
    "                failed += 1\n",
    "        else:\n",
    "            print(f\"   ❌ FAIL - No denomination extracted\")\n",
    "            failed += 1\n",
    "        print()\n",
    "    \n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed\\n\")\n",
    "    return passed, failed\n",
    "\n",
    "def test_issue_name_tracking():\n",
    "    \"\"\"Test that issue names from <sec> are properly tracked\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 2: ISSUE NAME TRACKING (from <sec> elements)\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    # Simulate page with multiple issues\n",
    "    test_page = {\n",
    "        'page_number': 30,\n",
    "        'elements': [\n",
    "            {'label': 'sec', 'text': 'Simon Bolivar Birthday issue', 'reading_order': 1},\n",
    "            {'label': 'para', 'text': 'July 24, 1921. Decree #18', 'reading_order': 2},\n",
    "            {'label': 'para', 'text': 'Proof', 'reading_order': 3},\n",
    "            {'label': 'para', 'text': 'DP99 15c black', 'reading_order': 4},\n",
    "            {'label': 'para', 'text': 'Regular issue', 'reading_order': 5},\n",
    "            {'label': 'para', 'text': '99 15c deep violet', 'reading_order': 6},\n",
    "            \n",
    "            {'label': 'sec', 'text': 'Central America Independence issue', 'reading_order': 7},\n",
    "            {'label': 'para', 'text': 'September 15, 1921', 'reading_order': 8},\n",
    "            {'label': 'para', 'text': 'PP100 5c violet', 'reading_order': 9},\n",
    "            {'label': 'para', 'text': '100 5c violet', 'reading_order': 10},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    extractor = PhilatelicExtractor()\n",
    "    nodes = extractor.process_page(test_page)\n",
    "    \n",
    "    print(f\"Extracted {len(nodes)} nodes\\n\")\n",
    "    \n",
    "    expected_issues = {\n",
    "        'DP99': 'Simon Bolivar Birthday issue',\n",
    "        '99': 'Simon Bolivar Birthday issue',\n",
    "        'PP100': 'Central America Independence issue',\n",
    "        '100': 'Central America Independence issue'\n",
    "    }\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for cat_num, expected_issue in expected_issues.items():\n",
    "        matching = [n for n in nodes if n.catalog_number == cat_num]\n",
    "        \n",
    "        if matching:\n",
    "            node = matching[0]\n",
    "            if node.issue_name == expected_issue:\n",
    "                print(f\"✅ {cat_num}: '{node.issue_name}'\")\n",
    "                passed += 1\n",
    "            else:\n",
    "                print(f\"❌ {cat_num}: Expected '{expected_issue}', got '{node.issue_name}'\")\n",
    "                failed += 1\n",
    "        else:\n",
    "            print(f\"❌ {cat_num}: Node not found\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(f\"\\nResults: {passed}/{len(expected_issues)} passed\\n\")\n",
    "    return passed, failed\n",
    "\n",
    "def test_quantity_vs_denomination():\n",
    "    \"\"\"Test that quantities are not confused with denominations\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 3: QUANTITY vs DENOMINATION DISAMBIGUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            'text': '99 15c deep violet 1,000,000',\n",
    "            'catalog': '99',\n",
    "            'expected_denom': '15c',\n",
    "            'expected_qty': '1,000,000',\n",
    "            'note': 'Quantity has comma'\n",
    "        },\n",
    "        {\n",
    "            'text': 'DP99 15c black #34009',\n",
    "            'catalog': 'DP99',\n",
    "            'expected_denom': '15c',\n",
    "            'expected_qty': None,\n",
    "            'note': 'No quantity'\n",
    "        },\n",
    "        {\n",
    "            'text': '100 5c violet printed 500,000',\n",
    "            'catalog': '100',\n",
    "            'expected_denom': '5c',\n",
    "            'expected_qty': '500,000',\n",
    "            'note': 'Quantity with \"printed\" keyword'\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    parser = CostaRicaCatalogParser()\n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"{i}. {test['text']}\")\n",
    "        print(f\"   Note: {test['note']}\")\n",
    "        \n",
    "        # Extract denomination\n",
    "        denoms_raw = parser.extract_denominations(test['text'])\n",
    "        denom = parser.normalize_denomination(denoms_raw[0][0], denoms_raw[0][1]) if denoms_raw else None\n",
    "        \n",
    "        # Extract quantity\n",
    "        qty_match = parser.QUANTITY.search(test['text'])\n",
    "        qty = qty_match.group(1) or qty_match.group(2) if qty_match else None\n",
    "        \n",
    "        denom_ok = (denom == test['expected_denom'])\n",
    "        qty_ok = (qty == test['expected_qty'])\n",
    "        \n",
    "        if denom_ok and qty_ok:\n",
    "            print(f\"   ✅ PASS - Denom: {denom}, Qty: {qty}\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"   ❌ FAIL\")\n",
    "            if not denom_ok:\n",
    "                print(f\"      Denom: Expected {test['expected_denom']}, Got {denom}\")\n",
    "            if not qty_ok:\n",
    "                print(f\"      Qty: Expected {test['expected_qty']}, Got {qty}\")\n",
    "            failed += 1\n",
    "        print()\n",
    "    \n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed\\n\")\n",
    "    return passed, failed\n",
    "\n",
    "def test_color_extraction_expert():\n",
    "    \"\"\"Expert-level color extraction test\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 4: EXPERT COLOR EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    parser = CostaRicaCatalogParser()\n",
    "    \n",
    "    test_cases = [\n",
    "        {\n",
    "            'text': 'S99 15c violet, op \"specimen\" in red with hole',\n",
    "            'catalog': 'S99',\n",
    "            'expected_color': 'violet',\n",
    "            'should_have_attr': 'overprint_red',\n",
    "            'note': 'Red is overprint color, not stamp color'\n",
    "        },\n",
    "        {\n",
    "            'text': '106 1c brown, blue op inverted op',\n",
    "            'catalog': '106',\n",
    "            'expected_color': 'brown',\n",
    "            'should_have_attr': 'overprint_blue',\n",
    "            'note': 'Blue is overprint, brown is stamp'\n",
    "        },\n",
    "        {\n",
    "            'text': 'PP99 15centimos light gray violet, imperf',\n",
    "            'catalog': 'PP99',\n",
    "            'expected_color': 'light gray violet',\n",
    "            'note': 'Compound color with full word denomination'\n",
    "        },\n",
    "        {\n",
    "            'text': '50 1p black on green',\n",
    "            'catalog': '50',\n",
    "            'expected_color': 'black',\n",
    "            'note': 'Bicolor: black on green paper'\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    passed = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, test in enumerate(test_cases, 1):\n",
    "        print(f\"{i}. {test['text']}\")\n",
    "        print(f\"   Note: {test['note']}\")\n",
    "        \n",
    "        color = parser.extract_stamp_color(test['text'], test['catalog'])\n",
    "        attrs = parser.extract_attributes(test['text'])\n",
    "        \n",
    "        color_ok = (color == test['expected_color'])\n",
    "        attr_ok = True\n",
    "        \n",
    "        if 'should_have_attr' in test:\n",
    "            attr_ok = test['should_have_attr'] in attrs\n",
    "        \n",
    "        if color_ok and attr_ok:\n",
    "            print(f\"   ✅ PASS - Color: {color}\")\n",
    "            if 'should_have_attr' in test:\n",
    "                print(f\"      Attribute found: {test['should_have_attr']}\")\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f\"   ❌ FAIL\")\n",
    "            if not color_ok:\n",
    "                print(f\"      Expected color: {test['expected_color']}, Got: {color}\")\n",
    "            if not attr_ok:\n",
    "                print(f\"      Missing attribute: {test['should_have_attr']}\")\n",
    "            failed += 1\n",
    "        print()\n",
    "    \n",
    "    print(f\"Results: {passed}/{len(test_cases)} passed\\n\")\n",
    "    return passed, failed\n",
    "\n",
    "def test_full_integration():\n",
    "    \"\"\"Full integration test with real pages 30-31 from file\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TEST 5: FULL INTEGRATION (Real Pages 30-31 from file)\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    input_path = \"./results/recognition_json/Mena 2018 CRPC .json\"\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"⚠️  File not found: {input_path}\")\n",
    "        print(\"   Using minimal sample data instead\\n\")\n",
    "        \n",
    "        # Fallback to minimal sample\n",
    "        sample_data = [{\n",
    "            'page_number': 30,\n",
    "            'elements': [\n",
    "                {\"label\": \"sec\", \"text\": \"Simon Bolivar Birthday issue\", \"reading_order\": 8},\n",
    "                {\"label\": \"para\", \"text\": \"DP99 15c black\", \"reading_order\": 11},\n",
    "                {\"label\": \"para\", \"text\": \"99 15c deep violet\", \"reading_order\": 17}\n",
    "            ]\n",
    "        }]\n",
    "    else:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # CORRECCIÓN: Extraer páginas del diccionario\n",
    "        if isinstance(data, dict) and 'pages' in data:\n",
    "            all_pages = data['pages']\n",
    "        elif isinstance(data, list):\n",
    "            all_pages = data\n",
    "        else:\n",
    "            print(\"⚠️  Unexpected JSON structure\")\n",
    "            all_pages = []\n",
    "        \n",
    "        # Filter pages 30-31\n",
    "        sample_data = [p for p in all_pages if p.get('page_number') in [10, 11]]\n",
    "        print(f\"Loaded {len(sample_data)} pages from file\\n\")\n",
    "        print(f\"Total pages in catalog: {len(all_pages)}\\n\")\n",
    "    \n",
    "    extractor = PhilatelicExtractor()\n",
    "    \n",
    "    all_nodes = []\n",
    "    for page in sample_data:\n",
    "        nodes = extractor.process_page(page)\n",
    "        all_nodes.extend(nodes)\n",
    "    \n",
    "    print(f\"Total nodes extracted: {len(all_nodes)}\")\n",
    "    print(f\"Expected for pages 30-31: 58-70 nodes\\n\")\n",
    "    \n",
    "    # Group by issue\n",
    "    by_issue = {}\n",
    "    for node in all_nodes:\n",
    "        issue = node.issue_name or 'No issue'\n",
    "        if issue not in by_issue:\n",
    "            by_issue[issue] = []\n",
    "        by_issue[issue].append(node)\n",
    "    \n",
    "    print(\"Nodes by Issue:\")\n",
    "    for issue, nodes in by_issue.items():\n",
    "        print(f\"  {issue}: {len(nodes)} nodes\")\n",
    "    print()\n",
    "    \n",
    "    # Group by type\n",
    "    by_type = {}\n",
    "    for node in all_nodes:\n",
    "        if node.node_type not in by_type:\n",
    "            by_type[node.node_type] = []\n",
    "        by_type[node.node_type].append(node)\n",
    "    \n",
    "    print(\"Nodes by Type:\")\n",
    "    for node_type, nodes in sorted(by_type.items()):\n",
    "        print(f\"  {node_type:15s}: {len(nodes)} nodes\")\n",
    "    print()\n",
    "    \n",
    "    # Validation checks - GENERALIZED\n",
    "    checks = []\n",
    "    \n",
    "    # Check 1: All nodes have issue_name\n",
    "    nodes_with_issue = [n for n in all_nodes if n.issue_name]\n",
    "    checks.append({\n",
    "        'name': 'All nodes have issue_name',\n",
    "        'passed': len(nodes_with_issue) == len(all_nodes),\n",
    "        'detail': f\"{len(nodes_with_issue)}/{len(all_nodes)}\"\n",
    "    })\n",
    "    \n",
    "    # Check 2: All nodes have denomination\n",
    "    nodes_with_denom = [n for n in all_nodes if n.denomination]\n",
    "    checks.append({\n",
    "        'name': 'All nodes have denomination',\n",
    "        'passed': len(nodes_with_denom) == len(all_nodes),\n",
    "        'detail': f\"{len(nodes_with_denom)}/{len(all_nodes)}\"\n",
    "    })\n",
    "    \n",
    "    # Check 3: Multiple issues detected (GENERALIZED)\n",
    "    unique_issues = set(n.issue_name for n in all_nodes if n.issue_name)\n",
    "    checks.append({\n",
    "        'name': 'Multiple issues detected',\n",
    "        'passed': len(unique_issues) >= 1,\n",
    "        'detail': f\"{len(unique_issues)} unique issues found\"\n",
    "    })\n",
    "    \n",
    "    # Check 4: Overprint colors captured as attributes (GENERALIZED)\n",
    "    nodes_with_op_attrs = [n for n in all_nodes \n",
    "                          if any('overprint' in attr.lower() for attr in n.attributes)]\n",
    "    checks.append({\n",
    "        'name': 'Overprint colors in attributes',\n",
    "        'passed': True,  # Non-critical, just informational\n",
    "        'detail': f\"{len(nodes_with_op_attrs)} nodes with overprint attributes\"\n",
    "    })\n",
    "    \n",
    "    # Check 5: Base stamps exist\n",
    "    base_stamps = [n for n in all_nodes if n.node_type == 'stamp' and not n.base_stamp]\n",
    "    checks.append({\n",
    "        'name': 'Base stamps detected',\n",
    "        'passed': len(base_stamps) > 0,\n",
    "        'detail': f\"{len(base_stamps)} base stamps\"\n",
    "    })\n",
    "    \n",
    "    # Check 6: Varieties/Proofs have base_stamp reference\n",
    "    derived_nodes = [n for n in all_nodes if n.node_type in ['proof', 'variety', 'specimen']]\n",
    "    with_base = [n for n in derived_nodes if n.base_stamp]\n",
    "    checks.append({\n",
    "        'name': 'Derived nodes have base_stamp',\n",
    "        'passed': len(with_base) > 0 if derived_nodes else True,\n",
    "        'detail': f\"{len(with_base)}/{len(derived_nodes)} have base reference\" if derived_nodes else \"N/A\"\n",
    "    })\n",
    "    \n",
    "    # Check 7: Relationships exist\n",
    "    relationships = extractor.build_relationships(all_nodes)\n",
    "    checks.append({\n",
    "        'name': 'Relationships built',\n",
    "        'passed': len(relationships) > 0,\n",
    "        'detail': f\"{len(relationships)} relationships\"\n",
    "    })\n",
    "    \n",
    "    # Check 8: Year captured\n",
    "    nodes_with_year = [n for n in all_nodes if n.year]\n",
    "    checks.append({\n",
    "        'name': 'Year information captured',\n",
    "        'passed': len(nodes_with_year) > 0,\n",
    "        'detail': f\"{len(nodes_with_year)} nodes with year info\"\n",
    "    })\n",
    "    \n",
    "    # Check 9: Node count reasonable\n",
    "    reasonable_count = 50 <= len(all_nodes) <= 80\n",
    "    checks.append({\n",
    "        'name': 'Node count within expected range',\n",
    "        'passed': reasonable_count,\n",
    "        'detail': f\"{len(all_nodes)} nodes (expected 58-70)\"\n",
    "    })\n",
    "    \n",
    "    # Print validation results\n",
    "    print(\"=\"*80)\n",
    "    print(\"VALIDATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    passed = sum(1 for c in checks if c['passed'])\n",
    "    \n",
    "    for check in checks:\n",
    "        status = '✅' if check['passed'] else '❌'\n",
    "        print(f\"{status} {check['name']}: {check['detail']}\")\n",
    "    \n",
    "    print(f\"\\nValidation: {passed}/{len(checks)} checks passed\\n\")\n",
    "    \n",
    "    # Show detailed breakdown\n",
    "    print(\"=\"*80)\n",
    "    print(\"DETAILED BREAKDOWN BY ISSUE\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    for issue_name, nodes in sorted(by_issue.items()):\n",
    "        print(f\"\\n{issue_name}:\")\n",
    "        print(f\"  Total: {len(nodes)} nodes\")\n",
    "        \n",
    "        # Count by type within this issue\n",
    "        types_in_issue = {}\n",
    "        for n in nodes:\n",
    "            types_in_issue[n.node_type] = types_in_issue.get(n.node_type, 0) + 1\n",
    "        \n",
    "        print(f\"  Types: {dict(types_in_issue)}\\n\")\n",
    "        \n",
    "        # Show first 5 nodes\n",
    "        for i, node in enumerate(nodes, 1): #enumerate(nodes[:5], 1)\n",
    "            attrs_str = f\", attrs: {node.attributes if node.attributes else '[]'}\"\n",
    "            refs_str = f\", refs: {node.reference_numbers[:2]}\" if node.reference_numbers else \"\"\n",
    "            base_str = f\" → {node.base_stamp}\" if node.base_stamp else \"\"\n",
    "            \n",
    "            print(f\"  {i}. {node.catalog_number} ({node.node_type}/{node.sub_type}){base_str}\")\n",
    "            qty_str = f\", qty: {node.quantity}\" if node.quantity else \"\"\n",
    "            print(f\"     {node.denomination} {node.color or 'N/A'}{qty_str}{attrs_str}{refs_str}\")\n",
    "        \n",
    "        # if len(nodes) > 5:\n",
    "        #     print(f\"  ... and {len(nodes) - 5} more\")\n",
    "    \n",
    "    # Show relationships\n",
    "    if relationships:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"RELATIONSHIPS\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "        \n",
    "        # Group relationships by type\n",
    "        by_rel_type = {}\n",
    "        for rel in relationships:\n",
    "            rel_type = rel['type']\n",
    "            if rel_type not in by_rel_type:\n",
    "                by_rel_type[rel_type] = []\n",
    "            by_rel_type[rel_type].append(rel)\n",
    "        \n",
    "        for rel_type, rels in sorted(by_rel_type.items()):\n",
    "            print(f\"\\n{rel_type} ({len(rels)} relationships):\")\n",
    "            for rel in rels[:5]:\n",
    "                print(f\"  {rel['from']:10s} --> {rel['to']}\")\n",
    "            if len(rels) > 5:\n",
    "                print(f\"  ... and {len(rels) - 5} more\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Results: {passed}/{len(checks)} checks passed\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # DEBUG: Mostrar nodos sin denominación\n",
    "    nodes_without_denom = [n for n in all_nodes if not n.denomination]\n",
    "    if nodes_without_denom:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DEBUG: NODES WITHOUT DENOMINATION\")\n",
    "        print(\"=\"*80)\n",
    "        print()\n",
    "        print(f\"Found {len(nodes_without_denom)} nodes without denomination:\\n\")\n",
    "        \n",
    "        for i, node in enumerate(nodes_without_denom, 1):\n",
    "            print(f\"{i}. Catalog: {node.catalog_number}\")\n",
    "            print(f\"   Type: {node.node_type}/{node.sub_type}\")\n",
    "            print(f\"   Issue: {node.issue_name}\")\n",
    "            print(f\"   Page: {node.page_number}, Order: {node.reading_order}\")\n",
    "            print(f\"   Raw text: {node.raw_text[:100]}...\")\n",
    "            print(f\"   Color: {node.color}\")\n",
    "            print()\n",
    "        \n",
    "    return passed, len(checks) - passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def create_stamp_graph(base_stamp_node, all_nodes, relationships, output_dir=\"./graph_outputs\"):\n",
    "    \"\"\"\n",
    "    Create a detailed graph for a single stamp and all its relationships.\n",
    "    \n",
    "    Args:\n",
    "        base_stamp_node: The main stamp node\n",
    "        all_nodes: List of all nodes\n",
    "        relationships: List of all relationships\n",
    "        output_dir: Directory to save graph images\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Find all related nodes\n",
    "    related_nodes = set()\n",
    "    related_relationships = []\n",
    "    \n",
    "    # Get the catalog number of the base stamp\n",
    "    base_id = base_stamp_node.catalog_number\n",
    "    \n",
    "    # Add base stamp to graph\n",
    "    qty_display = f\"\\nQty: {base_stamp_node.quantity}\" if base_stamp_node.quantity else \"\"\n",
    "    G.add_node(base_id, \n",
    "            node_obj=base_stamp_node,\n",
    "            node_type=base_stamp_node.node_type,\n",
    "            label=f\"{base_id}\\n{base_stamp_node.denomination}\\n{base_stamp_node.color or ''}{qty_display}\".strip())\n",
    "    \n",
    "    # Find all relationships involving this stamp\n",
    "    for rel in relationships:\n",
    "        if rel['from'] == base_id or rel['to'] == base_id:\n",
    "            related_relationships.append(rel)\n",
    "            related_nodes.add(rel['from'])\n",
    "            related_nodes.add(rel['to'])\n",
    "    \n",
    "    # Add related nodes to graph\n",
    "    node_lookup = {n.catalog_number: n for n in all_nodes}\n",
    "    \n",
    "    for node_id in related_nodes:\n",
    "        if node_id != base_id and node_id in node_lookup:\n",
    "            node = node_lookup[node_id]\n",
    "            label_parts = [node_id]\n",
    "            if node.denomination:\n",
    "                label_parts.append(node.denomination)\n",
    "            if node.color:\n",
    "                label_parts.append(node.color)\n",
    "            if node.node_type != 'stamp':\n",
    "                label_parts.append(f\"({node.node_type})\")\n",
    "            \n",
    "            G.add_node(node_id, \n",
    "                      node_obj=node,\n",
    "                      node_type=node.node_type,\n",
    "                      label='\\n'.join(label_parts))\n",
    "    \n",
    "    # Add edges with relationship types\n",
    "    for rel in related_relationships:\n",
    "        G.add_edge(rel['from'], rel['to'], \n",
    "                  rel_type=rel['type'],\n",
    "                  label=rel['type'].replace('_', ' ').title())\n",
    "    \n",
    "    # Create figure with better size\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Define node colors by type\n",
    "    node_colors = {\n",
    "        'stamp': '#4CAF50',        # Green for base stamps\n",
    "        'variety': '#2196F3',       # Blue for varieties\n",
    "        'proof': '#FF9800',         # Orange for proofs\n",
    "        'specimen': '#9C27B0',      # Purple for specimens\n",
    "        'error': '#F44336',         # Red for errors\n",
    "        'overprint': '#00BCD4',     # Cyan for overprints\n",
    "        'surcharge': '#FFEB3B',     # Yellow for surcharges\n",
    "    }\n",
    "    \n",
    "    # Get colors for nodes\n",
    "    colors = []\n",
    "    for node_id in G.nodes():\n",
    "        node_type = G.nodes[node_id].get('node_type', 'stamp')\n",
    "        colors.append(node_colors.get(node_type, '#9E9E9E'))\n",
    "    \n",
    "    # Calculate layout - hierarchical for better visualization\n",
    "    if len(G.nodes()) > 1:\n",
    "        # Try hierarchical layout with base stamp at center\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "        \n",
    "        # Adjust position to put base stamp at center-top\n",
    "        if base_id in pos:\n",
    "            # Center the base stamp\n",
    "            base_pos = pos[base_id]\n",
    "            pos[base_id] = (0.5, 0.9)\n",
    "            \n",
    "            # Arrange related nodes in a semi-circle below\n",
    "            related = [n for n in G.nodes() if n != base_id]\n",
    "            if related:\n",
    "                angle_step = 3.14 / (len(related) + 1)\n",
    "                for i, node_id in enumerate(related):\n",
    "                    angle = angle_step * (i + 1)\n",
    "                    radius = 0.4\n",
    "                    x = 0.5 + radius * np.cos(angle + 3.14)\n",
    "                    y = 0.4 + radius * np.sin(angle + 3.14) * 0.6\n",
    "                    pos[node_id] = (x, y)\n",
    "    else:\n",
    "        pos = {base_id: (0.5, 0.5)}\n",
    "    \n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_color=colors, \n",
    "                          node_size=3000,\n",
    "                          alpha=0.9,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Draw labels with better formatting\n",
    "    labels = nx.get_node_attributes(G, 'label')\n",
    "    nx.draw_networkx_labels(G, pos, labels, \n",
    "                           font_size=8, \n",
    "                           font_weight='bold',\n",
    "                           ax=ax)\n",
    "    \n",
    "    # Draw edges with labels\n",
    "    nx.draw_networkx_edges(G, pos, \n",
    "                          edge_color='gray',\n",
    "                          arrows=True,\n",
    "                          arrowsize=20,\n",
    "                          arrowstyle='-|>',\n",
    "                          width=2,\n",
    "                          alpha=0.6,\n",
    "                          ax=ax)\n",
    "    \n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, \n",
    "                                font_size=7,\n",
    "                                font_color='red',\n",
    "                                ax=ax)\n",
    "    \n",
    "    # Add title with stamp details\n",
    "    title_parts = [f\"Stamp Network: {base_id}\"]\n",
    "    if base_stamp_node.issue_name:\n",
    "        title_parts.append(f\"Issue: {base_stamp_node.issue_name}\")\n",
    "    if base_stamp_node.year:\n",
    "        title_parts.append(f\"Year: {base_stamp_node.year}\")\n",
    "    \n",
    "    plt.title('\\n'.join(title_parts), fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = []\n",
    "    for node_type, color in node_colors.items():\n",
    "        if any(G.nodes[n].get('node_type') == node_type for n in G.nodes()):\n",
    "            legend_elements.append(plt.scatter([], [], c=color, s=100, \n",
    "                                              label=node_type.title()))\n",
    "    \n",
    "    if legend_elements:\n",
    "        plt.legend(handles=legend_elements, loc='upper left', frameon=True)\n",
    "    \n",
    "    # Add statistics box\n",
    "    stats_text = f\"Nodes: {len(G.nodes())}\\nRelationships: {len(G.edges())}\"\n",
    "    plt.text(0.02, 0.02, stats_text, transform=ax.transAxes,\n",
    "            fontsize=10, verticalalignment='bottom',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    safe_filename = base_id.replace('/', '_').replace(' ', '_')\n",
    "    output_path = os.path.join(output_dir, f\"stamp_{safe_filename}.png\")\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return output_path, G\n",
    "\n",
    "\n",
    "def create_comparative_table(base_stamp_node, all_nodes, relationships, output_dir=\"./graph_outputs\"):\n",
    "    \"\"\"\n",
    "    Create a comparative table showing the stamp and all its variants/related items.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    base_id = base_stamp_node.catalog_number\n",
    "    related_data = []\n",
    "    \n",
    "    # Add base stamp\n",
    "    related_data.append({\n",
    "        'Catalog #': base_id,\n",
    "        'Type': base_stamp_node.node_type,\n",
    "        'Denomination': base_stamp_node.denomination,\n",
    "        'Color': base_stamp_node.color or 'N/A',\n",
    "        'Quantity': base_stamp_node.quantity or 'N/A',\n",
    "        'Year': base_stamp_node.year or 'N/A',\n",
    "        'Attributes': ', '.join(base_stamp_node.attributes) if base_stamp_node.attributes else 'N/A',\n",
    "        'References': ', '.join(base_stamp_node.reference_numbers) if base_stamp_node.reference_numbers else 'N/A',\n",
    "        'Relationship': 'BASE STAMP'\n",
    "    })\n",
    "    \n",
    "    # Find all related nodes\n",
    "    node_lookup = {n.catalog_number: n for n in all_nodes}\n",
    "    \n",
    "    for rel in relationships:\n",
    "        if rel['from'] == base_id:\n",
    "            if rel['to'] in node_lookup:\n",
    "                node = node_lookup[rel['to']]\n",
    "                related_data.append({\n",
    "                    'Catalog #': node.catalog_number,\n",
    "                    'Type': node.node_type,\n",
    "                    'Denomination': node.denomination or 'N/A',\n",
    "                    'Color': node.color or 'N/A',\n",
    "                    'Year': node.year or 'N/A',\n",
    "                    'Attributes': ', '.join(node.attributes) if node.attributes else 'N/A',\n",
    "                    'References': ', '.join(node.reference_numbers) if node.reference_numbers else 'N/A',\n",
    "                    'Relationship': rel['type'].replace('_', ' ').upper()\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(related_data)\n",
    "    \n",
    "    # Create figure with table\n",
    "    fig, ax = plt.subplots(figsize=(14, max(4, len(related_data) * 0.5)))\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create table\n",
    "    table = ax.table(cellText=df.values, \n",
    "                    colLabels=df.columns,\n",
    "                    cellLoc='left',\n",
    "                    loc='center',\n",
    "                    colWidths=[0.10, 0.09, 0.11, 0.10, 0.09, 0.08, 0.17, 0.13, 0.10])\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1, 1.5)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(df.columns)):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Highlight base stamp row\n",
    "    table[(1, 0)].set_facecolor('#E8F5E9')\n",
    "    for i in range(len(df.columns)):\n",
    "        table[(1, i)].set_facecolor('#E8F5E9')\n",
    "    \n",
    "    # Add title\n",
    "    title = f\"Stamp Catalog Comparison: {base_id}\"\n",
    "    if base_stamp_node.issue_name:\n",
    "        title += f\"\\nIssue: {base_stamp_node.issue_name}\"\n",
    "    plt.title(title, fontsize=12, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Save\n",
    "    safe_filename = base_id.replace('/', '_').replace(' ', '_')\n",
    "    output_path = os.path.join(output_dir, f\"table_{safe_filename}.png\")\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n",
    "def enhanced_test_full_integration():\n",
    "    \"\"\"Enhanced version with graph visualization per stamp\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED TEST WITH GRAPH VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    input_path = \"./results/recognition_json/Mena 2018 CRPC .json\"\n",
    "    \n",
    "    # [Previous loading code remains the same...]\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"⚠️  File not found: {input_path}\")\n",
    "        return\n",
    "    \n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, dict) and 'pages' in data:\n",
    "        all_pages = data['pages']\n",
    "    elif isinstance(data, list):\n",
    "        all_pages = data\n",
    "    else:\n",
    "        print(\"⚠️  Unexpected JSON structure\")\n",
    "        return\n",
    "    \n",
    "    # Filter pages as before\n",
    "    sample_data = [p for p in all_pages if p.get('page_number') in [8, 9]]\n",
    "    \n",
    "    # Process with your extractor\n",
    "    extractor = PhilatelicExtractor()\n",
    "    \n",
    "    all_nodes = []\n",
    "    for page in sample_data:\n",
    "        nodes = extractor.process_page(page)\n",
    "        all_nodes.extend(nodes)\n",
    "    \n",
    "    # Build relationships\n",
    "    relationships = extractor.build_relationships(all_nodes)\n",
    "    \n",
    "    # Find all base stamps (stamps without base_stamp reference)\n",
    "    base_stamps = [n for n in all_nodes if n.node_type == 'stamp' and not n.base_stamp]\n",
    "    \n",
    "    print(f\"Found {len(base_stamps)} base stamps to visualize\")\n",
    "    print(\"Creating individual graphs for each stamp...\\n\")\n",
    "    \n",
    "    # Create graphs for each base stamp\n",
    "    graph_paths = []\n",
    "    table_paths = []\n",
    "    \n",
    "    for i, stamp in enumerate(base_stamps, 1):\n",
    "        print(f\"Processing {i}/{len(base_stamps)}: {stamp.catalog_number} - {stamp.issue_name}\")\n",
    "        \n",
    "        # Create network graph\n",
    "        graph_path, G = create_stamp_graph(stamp, all_nodes, relationships)\n",
    "        graph_paths.append(graph_path)\n",
    "        print(f\"  ✅ Graph saved: {graph_path}\")\n",
    "        \n",
    "        # Create comparative table\n",
    "        table_path = create_comparative_table(stamp, all_nodes, relationships)\n",
    "        table_paths.append(table_path)\n",
    "        print(f\"  ✅ Table saved: {table_path}\")\n",
    "        \n",
    "        # Print graph statistics\n",
    "        print(f\"  📊 Graph stats: {len(G.nodes())} nodes, {len(G.edges())} relationships\")\n",
    "        print()\n",
    "    \n",
    "    # Create summary HTML file for easy viewing\n",
    "    create_html_summary(base_stamps, graph_paths, table_paths)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"VISUALIZATION COMPLETE\")\n",
    "    print(f\"Generated {len(graph_paths)} stamp graphs\")\n",
    "    print(f\"Generated {len(table_paths)} comparison tables\")\n",
    "    print(\"Check ./graph_outputs/ directory for results\")\n",
    "    print(\"Open summary.html for easy navigation\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "def create_html_summary(base_stamps, graph_paths, table_paths, output_dir=\"./graph_outputs\"):\n",
    "    \"\"\"Create an HTML file to easily view all generated graphs\"\"\"\n",
    "    \n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Philatelic Catalog Verification</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            h1 { color: #333; }\n",
    "            .stamp-section { \n",
    "                border: 2px solid #ddd; \n",
    "                margin: 20px 0; \n",
    "                padding: 15px; \n",
    "                border-radius: 5px;\n",
    "            }\n",
    "            .stamp-header { \n",
    "                background: #f5f5f5; \n",
    "                padding: 10px; \n",
    "                margin: -15px -15px 15px -15px;\n",
    "                border-radius: 3px 3px 0 0;\n",
    "            }\n",
    "            .images { display: flex; gap: 20px; }\n",
    "            .image-container { flex: 1; text-align: center; }\n",
    "            img { max-width: 100%; border: 1px solid #ccc; }\n",
    "            .navigation { \n",
    "                position: fixed; \n",
    "                right: 20px; \n",
    "                top: 20px; \n",
    "                background: white; \n",
    "                border: 1px solid #ddd;\n",
    "                padding: 10px;\n",
    "                max-height: 80vh;\n",
    "                overflow-y: auto;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Philatelic Catalog Verification Graphs</h1>\n",
    "        <div class=\"navigation\">\n",
    "            <h3>Quick Navigation</h3>\n",
    "            <ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add navigation links\n",
    "    for i, stamp in enumerate(base_stamps):\n",
    "        safe_id = stamp.catalog_number.replace('/', '_').replace(' ', '_')\n",
    "        html_content += f'<li><a href=\"#{safe_id}\">{stamp.catalog_number}</a></li>\\n'\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "            </ul>\n",
    "        </div>\n",
    "        <div style=\"margin-right: 200px;\">\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add stamp sections\n",
    "    for i, stamp in enumerate(base_stamps):\n",
    "        safe_id = stamp.catalog_number.replace('/', '_').replace(' ', '_')\n",
    "        graph_filename = os.path.basename(graph_paths[i])\n",
    "        table_filename = os.path.basename(table_paths[i])\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"stamp-section\" id=\"{safe_id}\">\n",
    "            <div class=\"stamp-header\">\n",
    "                <h2>{stamp.catalog_number}</h2>\n",
    "                <p><strong>Issue:</strong> {stamp.issue_name or 'N/A'}</p>\n",
    "                <p><strong>Denomination:</strong> {stamp.denomination or 'N/A'}</p>\n",
    "                <p><strong>Color:</strong> {stamp.color or 'N/A'}</p>\n",
    "                <p><strong>Year:</strong> {stamp.year or 'N/A'}</p>\n",
    "            </div>\n",
    "            <div class=\"images\">\n",
    "                <div class=\"image-container\">\n",
    "                    <h3>Relationship Graph</h3>\n",
    "                    <img src=\"{graph_filename}\" alt=\"Graph for {stamp.catalog_number}\">\n",
    "                </div>\n",
    "                <div class=\"image-container\">\n",
    "                    <h3>Comparison Table</h3>\n",
    "                    <img src=\"{table_filename}\" alt=\"Table for {stamp.catalog_number}\">\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save HTML file\n",
    "    html_path = os.path.join(output_dir, \"summary.html\")\n",
    "    with open(html_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"✅ HTML summary created: {html_path}\")\n",
    "\n",
    "\n",
    "# Additional utility function for verification\n",
    "def export_verification_csv(all_nodes, relationships, output_path=\"./graph_outputs/verification.csv\"):\n",
    "    \"\"\"Export all data to CSV for manual verification against catalog\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for node in all_nodes:\n",
    "        # Find relationships for this node\n",
    "        related_to = []\n",
    "        for rel in relationships:\n",
    "            if rel['from'] == node.catalog_number:\n",
    "                related_to.append(f\"{rel['to']} ({rel['type']})\")\n",
    "        \n",
    "        data.append({\n",
    "            'Catalog_Number': node.catalog_number,\n",
    "            'Type': node.node_type,\n",
    "            'SubType': node.sub_type,\n",
    "            'Issue': node.issue_name,\n",
    "            'Year': node.year,\n",
    "            'Denomination': node.denomination,\n",
    "            'Color': node.color,\n",
    "            'Attributes': ', '.join(node.attributes) if node.attributes else '',\n",
    "            'References': ', '.join(node.reference_numbers) if node.reference_numbers else '',\n",
    "            'Base_Stamp': node.base_stamp or '',\n",
    "            'Related_To': '; '.join(related_to),\n",
    "            'Page': node.page_number,\n",
    "            'Reading_Order': node.reading_order\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Verification CSV exported: {output_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3728ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run all expert validation tests\"\"\"\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(\"╔\" + \"=\"*78 + \"╗\")\n",
    "print(\"║\" + \" \"*15 + \"EXPERT VALIDATION TEST SUITE - COSTA RICA\" + \" \"*22 + \"║\")\n",
    "print(\"║\" + \" \"*22 + \"Philately & Regex Expert Edition\" + \" \"*24 + \"║\")\n",
    "print(\"╚\" + \"=\"*78 + \"╝\")\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Test 1: Denominations\n",
    "p1, f1 = test_denomination_extraction()\n",
    "results.append(('Denomination Extraction', p1, f1))\n",
    "\n",
    "# Test 2: Issue names\n",
    "p2, f2 = test_issue_name_tracking()\n",
    "results.append(('Issue Name Tracking', p2, f2))\n",
    "\n",
    "# Test 3: Quantity vs Denomination\n",
    "p3, f3 = test_quantity_vs_denomination()\n",
    "results.append(('Quantity vs Denomination', p3, f3))\n",
    "\n",
    "# Test 4: Color extraction\n",
    "p4, f4 = test_color_extraction_expert()\n",
    "results.append(('Expert Color Extraction', p4, f4))\n",
    "\n",
    "# Test 5: Full integration\n",
    "p5, f5 = test_full_integration()\n",
    "results.append(('Full Integration', p5, f5))\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "total_passed = sum(r[1] for r in results)\n",
    "total_tests = sum(r[1] + r[2] for r in results)\n",
    "\n",
    "for name, passed, failed in results:\n",
    "    total = passed + failed\n",
    "    pct = (passed / total * 100) if total > 0 else 0\n",
    "    status = '✅' if failed == 0 else '⚠️ '\n",
    "    print(f\"{status} {name:30s}: {passed:2d}/{total:2d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL: {total_passed}/{total_tests} tests passed ({total_passed/total_tests*100:.1f}%)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "if total_passed == total_tests:\n",
    "    print(\"🎉 ALL TESTS PASSED!\")\n",
    "    print(\"\\n✅ El extractor está listo para procesar el catálogo completo de Costa Rica\")\n",
    "    print(\"✅ Maneja todas las denominaciones: centavos, colones, pesos, reales\")\n",
    "    print(\"✅ Extrae issue names correctamente de elementos <sec>\")\n",
    "    print(\"✅ Distingue cantidades de denominaciones\")\n",
    "    print(\"✅ Extrae colores sin confundirlos con overprints\")\n",
    "    print(\"\\n🚀 Próximo paso: Ejecutar en páginas 30-31 completas\")\n",
    "else:\n",
    "    print(f\"⚠️  {total_tests - total_passed} tests fallaron\")\n",
    "    print(\"Revisa los detalles arriba para corregir los problemas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_test_full_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef0808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4c86e5",
   "metadata": {},
   "source": [
    "## Get the Catalogues with Landing AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f51e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from landingai_ade import LandingAIADE\n",
    "# Load environment variables \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF PATH\n",
    "pdf_path = \"./pdfs/Catalogues/\"\n",
    "pdf_file_name = \"Scott CR 2024 18-34\"\n",
    "\n",
    "# Parse the document\n",
    "response = LandingAIADE().parse(document_url=pdf_path+pdf_file_name+\".pdf\",model=\"dpt-2-latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6588154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(\"Extracted Markdown:\")\n",
    "print(response.markdown)\n",
    "print(\"Extracted Chunks:\")\n",
    "print(response.chunks)\n",
    "\n",
    "# Save Markdown to a file\n",
    "if response.markdown:\n",
    "    with open(f'results/parsed_catalogues/{pdf_file_name}.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(response.markdown)\n",
    "    print(\"\\nMarkdown content saved to a Markdown file.\")\n",
    "else:\n",
    "    print(\"No 'markdown' field found in the response\")\n",
    "    \n",
    "# Save Chunks to a JSON file\n",
    "if response.chunks:\n",
    "    # Convertir chunks a diccionarios para serialización JSON\n",
    "    chunks_data = [chunk.model_dump() for chunk in response.chunks]\n",
    "    \n",
    "    with open(f'results/parsed_catalogues/{pdf_file_name}_chunks.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\n{len(chunks_data)} chunks saved to JSON file.\")\n",
    "else:\n",
    "    print(\"No 'chunks' field found in the response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbf6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_chunk_text(text: str) -> str:\n",
    "    \"\"\"Clean markdown and formatting artifacts\"\"\"\n",
    "    # Remove anchor tags\n",
    "    text = re.sub(r'<a id=[\\'\"][^\\'\"]+[\\'\"]></a>\\n*', '', text)\n",
    "    # Remove figure markup but keep content\n",
    "    #text = re.sub(r'<::(.*?)::>', r'\\1', text, flags=re.DOTALL)\n",
    "    # Clean excessive whitespace\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chunks = []\n",
    "with open(f'results/parsed_catalogues/{pdf_file_name}_chunks.json', 'r', encoding='utf-8') as f:\n",
    "    chunks_data = json.load(f)\n",
    "    print(f\"Se cargaron {len(chunks_data)} chunks desde el archivo JSON.\\n\")\n",
    "    \n",
    "    # Iterar sobre cada chunk\n",
    "    temp_merge = []\n",
    "    for i, chunk in enumerate(chunks_data):\n",
    "        print(f\"--- Chunk {i + 1} ---\")\n",
    "        print(f\"Tipo: {chunk.get('type', 'N/A')}\")\n",
    "        print(f\"Texto: {chunk.get('markdown', 'N/A')[:100]}...\")  # Primeros 100 caracteres\n",
    "        print(f\"Página: {chunk.get('grounding', 'N/A')['page']}\")\n",
    "        \n",
    "        print()  # Línea en blanco entre chunks\n",
    "        temp_merge.append(chunk)\n",
    "        # Ejemplo: procesar solo chunks de texto\n",
    "        if chunk.get('type') == 'text':\n",
    "            # Tu lógica aquí            \n",
    "            group_chunks.append(temp_merge)\n",
    "            temp_merge = []\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d16d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from icecream import ic\n",
    "# for i,group_chunk in enumerate(group_chunks):\n",
    "#     if len(group_chunk) == 1:\n",
    "#         print(\"Group Number \",i)\n",
    "#         for chunk in group_chunk:\n",
    "#             print(\"SOLUTION\")\n",
    "#             chunk_solution = group_chunks[i-1][-1]\n",
    "#             print(f\"Tipo: {chunk_solution.get('type', 'N/A')}\")\n",
    "#             print(f\"Texto: {_clean_chunk_text(chunk_solution.get('markdown', 'N/A'))}\")\n",
    "#             print(f\"Página: {chunk_solution.get('grounding', 'N/A')['page']}\")\n",
    "#             print(\"-------END SOLUTION----------\")\n",
    "#             print()                \n",
    "#             print(f\"Tipo: {chunk.get('type', 'N/A')}\")\n",
    "#             print(f\"Texto: {_clean_chunk_text(chunk.get('markdown', 'N/A'))}\")\n",
    "#             print(f\"Página: {chunk.get('grounding', 'N/A')['page']}\")\n",
    "#             print(\"---------------\")\n",
    "#         print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62298fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "\n",
    "# Crear el nuevo arreglo fusionado\n",
    "group_chunks_merged = []\n",
    "\n",
    "for i, group_chunk in enumerate(group_chunks):\n",
    "    # Si es el primer grupo, simplemente lo agregamos\n",
    "    if i == 0:\n",
    "        group_chunks_merged.append(group_chunk[:])  # Copia del grupo\n",
    "    # Si el grupo actual tiene solo 1 elemento\n",
    "    elif len(group_chunk) == 1:\n",
    "        # Obtenemos el grupo anterior del nuevo arreglo (ya procesado)\n",
    "        previous_group = group_chunks_merged[-1]\n",
    "        \n",
    "        # Si el grupo anterior tiene elementos\n",
    "        if len(previous_group) > 0:\n",
    "            # Extraemos el último elemento del grupo anterior\n",
    "            chunk_solution = previous_group[-1]\n",
    "            \n",
    "            # Removemos ese elemento del grupo anterior en el nuevo arreglo\n",
    "            group_chunks_merged[-1] = previous_group[:-1]\n",
    "            \n",
    "            # Creamos el nuevo grupo fusionado: [chunk_solution, chunk_actual]\n",
    "            merged_group = [chunk_solution, group_chunk[0]]\n",
    "            group_chunks_merged.append(merged_group)\n",
    "            \n",
    "            print(f\"✓ Grupo {i} fusionado con último elemento del grupo {i-1}\")\n",
    "        else:\n",
    "            # Si el grupo anterior ya está vacío, solo agregamos el actual\n",
    "            group_chunks_merged.append(group_chunk[:])\n",
    "    else:\n",
    "        # Si tiene más de 1 elemento, lo agregamos tal cual\n",
    "        group_chunks_merged.append(group_chunk[:])\n",
    "\n",
    "# Limpiamos grupos vacíos si existen\n",
    "group_chunks_merged = [group for group in group_chunks_merged if len(group) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0af7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "for i,group_chunk in enumerate(group_chunks_merged[0:30]):    \n",
    "    print(\"Group Number \",i)\n",
    "    for chunk in group_chunk:\n",
    "        print(f\"Tipo: {chunk.get('type', 'N/A')}\")\n",
    "        print(f\"Texto: {_clean_chunk_text(chunk.get('markdown', 'N/A'))}\")\n",
    "        print(f\"Página: {chunk.get('grounding', 'N/A')['page']}\")\n",
    "        print(\"---------------\")\n",
    "    print(\"****************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9dce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(group_chunks_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b9be8",
   "metadata": {},
   "source": [
    "## Parseando el Catalogo Scott con LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86714d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import your stamp models (assuming they're in a file called 'kg_pydantic.py')\n",
    "# from stamp_models import ScottEntry, ScottNumber, Denomination, ColorDescription, Perforation, MonetaryValue, PrintingMethod, StampType, PaperType\n",
    "from kg_pydantic import *\n",
    "# Load environment variables \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified Scott Catalog Parser with Direct Examples\n",
    "This version uses a more straightforward approach with explicit examples\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "from decimal import Decimal\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "\n",
    "class SimpleScottParser:\n",
    "    \"\"\"Simplified parser that works with direct examples\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_api_key: str, model_name: str = \"gpt-4o-mini\"):\n",
    "        self.llm = ChatOpenAI(\n",
    "            temperature=0.0,\n",
    "            model_name=model_name,\n",
    "            openai_api_key=openai_api_key,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        \n",
    "        # Use JSON output parser instead of Pydantic\n",
    "        self.output_parser = JsonOutputParser()\n",
    "        \n",
    "        # Create the chain\n",
    "        self.chain = self._create_chain()\n",
    "    \n",
    "    def parse_chunk(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse a chunk of catalog text\"\"\"\n",
    "        \n",
    "        # Clean the text\n",
    "        # text = re.sub(r'<a id=[\\'\"][^\\'\"]+[\\'\"]></a>\\n*', '', text)\n",
    "        # text = re.sub(r'<::(.*?)::>', r'\\1', text, flags=re.DOTALL)\n",
    "        # text = text.strip()\n",
    "        \n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                result = self.chain.invoke({\"input\": text})\n",
    "                print(\"Prompt tokens:\", cb.prompt_tokens)\n",
    "                print(\"Completion tokens:\", cb.completion_tokens)\n",
    "                print(\"Total tokens:\", cb.total_tokens)\n",
    "                print(\"Costo (USD):\", cb.total_cost)\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"Parsing error: {e}\")\n",
    "            return {\"stamps\": [], \"error\": str(e)}\n",
    "    \n",
    "    def _create_chain(self):\n",
    "        \"\"\"Create the parsing chain with explicit examples\"\"\"\n",
    "        \n",
    "        # Define examples with exact input/output\n",
    "        examples = [\n",
    "            {\n",
    "                \"input\": \"\"\"1889 Black Overprint\n",
    "23 A8 1c rose     5.00 3.00\n",
    "24 A9 5c brown    7.00 3.00\n",
    "Vertical and inverted overprints are fakes.\"\"\",\n",
    "                \"output\": json.dumps({\n",
    "                    \"stamps\": [\n",
    "                        {\n",
    "                            \"scott_number\": \"23\",\n",
    "                            \"illustration\": \"A8\",\n",
    "                            \"denomination\": \"1c\",\n",
    "                            \"color\": \"rose\",\n",
    "                            \"mint_value\": 5.00,\n",
    "                            \"used_value\": 3.00\n",
    "                        },\n",
    "                        {\n",
    "                            \"scott_number\": \"24\",\n",
    "                            \"illustration\": \"A9\",\n",
    "                            \"denomination\": \"5c\",\n",
    "                            \"color\": \"brown\",\n",
    "                            \"mint_value\": 7.00,\n",
    "                            \"used_value\": 3.00\n",
    "                        }\n",
    "                    ],\n",
    "                    \"header\": \"1889 Black Overprint\",\n",
    "                    \"notes\": [\"Vertical and inverted overprints are fakes.\"]\n",
    "                }, indent=2)\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"\"\"1901, Jan.                                Perf. 12-15½\n",
    "45 A30    1c green & blk                  3.25    .30\n",
    "a.        Horiz. pair, imperf. btwn.      150.00\n",
    "46 A31    2c ver & blk                    1.25    .30\"\"\",\n",
    "                \"output\": json.dumps({\n",
    "                    \"stamps\": [\n",
    "                        {\n",
    "                            \"scott_number\": \"45\",\n",
    "                            \"illustration\": \"A30\",\n",
    "                            \"denomination\": \"1c\",\n",
    "                            \"color\": \"green & blk\",\n",
    "                            \"mint_value\": 3.25,\n",
    "                            \"used_value\": 0.30,\n",
    "                            \"perforation\": \"12-15½\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"scott_number\": \"45a\",\n",
    "                            \"variety_of\": \"45\",\n",
    "                            \"description\": \"Horiz. pair, imperf. btwn.\",\n",
    "                            \"mint_value\": 150.00\n",
    "                        },\n",
    "                        {\n",
    "                            \"scott_number\": \"46\",\n",
    "                            \"illustration\": \"A31\",\n",
    "                            \"denomination\": \"2c\",\n",
    "                            \"color\": \"ver & blk\",\n",
    "                            \"mint_value\": 1.25,\n",
    "                            \"used_value\": 0.30,\n",
    "                            \"perforation\": \"12-15½\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"header\": \"1901, Jan.\",\n",
    "                    \"perforation\": \"Perf. 12-15½\"\n",
    "                }, indent=2)\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"\"\"<::A dark-colored postage stamp with a portrait of a man with a mustache in the center. The stamp has \"CORREOS\" and \"COSTA RICA\" written in a circular pattern around the portrait. The number \"5\" is visible in the top left and bottom right corners, and \"CENTAVOS\" is at the bottom. The stamp has perforated edges. : postage stamp::>\n",
    "President Bernardo Soto Alfaro — A7\"\"\",\n",
    "                \"output\": json.dumps({\n",
    "                    \"stamps\": [],\n",
    "                    \"illustrations\": [\n",
    "                        {\n",
    "                            \"illustration_number\": \"A7\",\n",
    "                            \"design_name\": \"President Bernardo Soto Alfaro\",\n",
    "                            \"design_description\": \"A dark-colored postage stamp with a portrait of a man with a mustache in the center. The stamp has \\\"CORREOS\\\" and \\\"COSTA RICA\\\" written in a circular pattern around the portrait. The number \\\"5\\\" is visible in the top left and bottom right corners, and \\\"CENTAVOS\\\" is at the bottom. The stamp has perforated edges.\",\n",
    "                            \"denomination\": \"5 CENTAVOS\"\n",
    "                        }\n",
    "                    ]\n",
    "                }, indent=2)\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"\"\"<::A collection of six postage stamps, each featuring a portrait of President Soto Alfaro. The stamps are arranged in two columns. Top row: - Left stamp (A10): A brown stamp with a portrait of a man, labeled \"COSTA RICA\" at the top and \"1 CENTAVO\" at the bottom, with the number \"1\" in the upper corners. - Right stamp (A11): A greenish-blue stamp with a portrait of a man, labeled \"COSTA RICA\" at the top and \"2 CENTAVOS\" at the bottom, with the number \"2\" in the upper corners. Middle row: - Left stamp (A12): A reddish-orange stamp with a portrait of a man, labeled \"COSTA RICA\" at the top and \"5 CENTAVOS\" at the bottom, with the number \"5\" in the upper corners. - Right stamp (A13): A reddish-orange stamp with a portrait of a man, labeled \"COSTA RICA\" at the top and \"10 CENTAVOS\" at the bottom, with the number \"10\" in the upper corners. Bottom row: - Left stamp (A14): A green stamp with a portrait of a man, labeled \"COSTA RICA\" at the top and \"20 CENTAVOS\" at the bottom, with the number \"20\" in the upper corners. - Right stamp (A15): A reddish-orange stamp with a portrait of a man, labeled \"COSTA RICA\" at the top and \"50 CENTAVOS\" at the bottom, with the number \"50\" in the upper corners. : figure::>\"\"\",\n",
    "                \"output\": json.dumps({\n",
    "                    \"stamps\": [],\n",
    "                    \"illustrations\": [\n",
    "                        {\n",
    "                            \"illustration_number\": \"A10\",\n",
    "                            \"design_description\": \"A brown stamp with a portrait of President Soto Alfaro\",\n",
    "                            \"denomination\": \"1 CENTAVO\",\n",
    "                            \"color\": \"brown\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"illustration_number\": \"A11\",\n",
    "                            \"design_description\": \"A greenish-blue stamp with a portrait of President Soto Alfaro\",\n",
    "                            \"denomination\": \"2 CENTAVOS\",\n",
    "                            \"color\": \"greenish-blue\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"illustration_number\": \"A12\",\n",
    "                            \"design_description\": \"A reddish-orange stamp with a portrait of President Soto Alfaro\",\n",
    "                            \"denomination\": \"5 CENTAVOS\",\n",
    "                            \"color\": \"reddish-orange\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"illustration_number\": \"A13\",\n",
    "                            \"design_description\": \"A reddish-orange stamp with a portrait of President Soto Alfaro\",\n",
    "                            \"denomination\": \"10 CENTAVOS\",\n",
    "                            \"color\": \"reddish-orange\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"illustration_number\": \"A14\",\n",
    "                            \"design_description\": \"A green stamp with a portrait of President Soto Alfaro\",\n",
    "                            \"denomination\": \"20 CENTAVOS\",\n",
    "                            \"color\": \"green\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"illustration_number\": \"A15\",\n",
    "                            \"design_description\": \"A reddish-orange stamp with a portrait of President Soto Alfaro\",\n",
    "                            \"denomination\": \"50 CENTAVOS\",\n",
    "                            \"color\": \"reddish-orange\"\n",
    "                        }\n",
    "                    ]\n",
    "                }, indent=2)\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Create the few-shot prompt\n",
    "        example_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\"ai\", \"{output}\")\n",
    "        ])\n",
    "        \n",
    "        few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "            example_prompt=example_prompt,\n",
    "            examples=examples,\n",
    "        )\n",
    "        \n",
    "        # Final prompt\n",
    "        final_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "    You are a Scott catalog parser. Extract stamp information and illustration descriptions from catalog text.\n",
    "\n",
    "    CONTENT TYPES:\n",
    "    1. STAMP ENTRIES: Lines starting with numbers (45, 46, 47) containing catalog data\n",
    "    2. VARIETIES: Lines starting with letters (a., b., c.) that modify the stamp above\n",
    "    3. ILLUSTRATIONS: Text within <::...::> describes stamp images, often followed by illustration numbers (A7, A10, etc.)\n",
    "\n",
    "    RULES FOR STAMPS:\n",
    "    - Main stamps start with numbers\n",
    "    - Format: [number] [illustration] [denomination] [color] [mint_price] [used_price]\n",
    "    - Multi-line entries continue from above\n",
    "    - Prices: \"3.25 .30\" = mint $3.25, used $0.30\n",
    "    - Dash (—) = no price\n",
    "\n",
    "    RULES FOR ILLUSTRATIONS:\n",
    "    - Text between <:: and ::> is a design description\n",
    "    - Look for illustration numbers (A10, A11) within or after the description\n",
    "    - Extract denomination and color when mentioned\n",
    "    - May describe single or multiple stamps\n",
    "\n",
    "    EXAMPLE COLOR ABBREVIATIONS:\n",
    "    blk=black, grn=green, ver=vermillion, lil=lilac, ol=olive, bis=bistre, car=carmine, yel=yellow, brn=brown, dk=dark\n",
    "\n",
    "    ALWAYS return valid JSON with the keys: \"stamps\", \"illustrations\", \"header\" and \"notes\" .\n",
    "    Example of JSON for return:\n",
    "\n",
    "    \"stamps\": [],        // If there are stamps if not empty\n",
    "    \"illustrations\": [], // If there are illustrations if not empty\n",
    "    \"header\": \"1901, Jan.\",  //Always if is possible \n",
    "    \"notes\": [\"....\"] //Always if there are notes if not empty\n",
    "\n",
    "\n",
    "\"\"\"),\n",
    "            few_shot_prompt,\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        return final_prompt | self.llm | self.output_parser\n",
    "    \n",
    "    def parse_and_display(self, text: str):\n",
    "        \"\"\"Parse and display results in a readable format\"\"\"\n",
    "        \n",
    "        # print(\"INPUT TEXT:\")\n",
    "        # print(\"-\" * 60)\n",
    "        # print(text[:500] + \"...\" if len(text) > 500 else text)\n",
    "        # print(\"-\" * 60)\n",
    "        \n",
    "        result = self.parse_chunk(text)\n",
    "        print(result)\n",
    "                \n",
    "        print(\"\\nPARSING RESULTS:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"ERROR: {result['error']}\")\n",
    "            return result\n",
    "        \n",
    "        # Display stamps\n",
    "        stamps = result.get(\"stamps\", [])\n",
    "        if stamps:\n",
    "            print(f\"Found {len(stamps)} stamps:\\n\")\n",
    "            \n",
    "            for stamp in stamps:\n",
    "                if stamp.get(\"variety_of\"):\n",
    "                    print(f\"  └─ #{stamp['scott_number']}: {stamp.get('description', 'Variety')}\")\n",
    "                    if stamp.get('mint_value'):\n",
    "                        print(f\"      Value: ${stamp['mint_value']}\")\n",
    "                else:\n",
    "                    print(f\"#{stamp['scott_number']} ({stamp.get('illustration', 'N/A')}): \"\n",
    "                         f\"{stamp.get('denomination', '')} {stamp.get('color', '')}\")\n",
    "                    if stamp.get('mint_value'):\n",
    "                        print(f\"  Values: ${stamp['mint_value']} mint / \"\n",
    "                             f\"${stamp.get('used_value', 'N/A')} used\")\n",
    "        \n",
    "        # Display illustrations\n",
    "        illustrations = result.get(\"illustrations\", [])\n",
    "        if illustrations:\n",
    "            print(f\"\\nFound {len(illustrations)} illustration descriptions:\\n\")\n",
    "            \n",
    "            for illus in illustrations:\n",
    "                print(f\"Illustration {illus['illustration_number']}:\")\n",
    "                if illus.get('design_name'):\n",
    "                    print(f\"  Name: {illus['design_name']}\")\n",
    "                if illus.get('denomination'):\n",
    "                    print(f\"  Denomination: {illus['denomination']}\")\n",
    "                if illus.get('color'):\n",
    "                    print(f\"  Color: {illus['color']}\")\n",
    "                desc = illus.get('design_description', '')\n",
    "                if desc:\n",
    "                    # Truncate long descriptions for display\n",
    "                    if len(desc) > 100:\n",
    "                        print(f\"  Description: {desc[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"  Description: {desc}\")\n",
    "        \n",
    "        # Display notes\n",
    "        if result.get(\"notes\"):\n",
    "            print(\"\\nNOTES:\")\n",
    "            for note in result[\"notes\"]:\n",
    "                print(f\"  • {note}\")\n",
    "        \n",
    "        # Display header info\n",
    "        if result.get(\"header\"):\n",
    "            print(f\"\\nHEADER: {result['header']}\")\n",
    "        if result.get(\"perforation\"):\n",
    "            print(f\"PERFORATION: {result['perforation']}\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        return result\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test_parser():\n",
    "    \"\"\"Test the parser with your actual chunk\"\"\"\n",
    "    \n",
    "    # Your actual chunk text\n",
    "    chunk_text = \"\"\"1881-82\n",
    "Red or Black Surcharge\n",
    "7 A1(a) 1c on ½r ('82) 3.00 6.00\n",
    "a. On No. 1a 15.00 -\n",
    "8 A1(b) 1c on ½r ('82) 18.00 30.00\n",
    "9 A1(c) 2c on ½r, #1a 3.00 2.75\n",
    "a. On No. 1 8.00\n",
    "12 A1(c) 5c on ½r 15.00\n",
    "13 A1(d) 5c on ½r ('82) 35.00\n",
    "14 A1(d) 10c on 2r (Bk)\n",
    "('82) 72.50 -\n",
    "15 A1(e) 20c on 4r ('82) 300.00 -\n",
    "\n",
    "Overprints with different fonts and \"OFICIAL\" were never placed in use, and are said to have been surcharged to a dealer's order. The ½r surcharged \"DOS CTS\" is not a postage stamp. It probably is an essay.\n",
    "Postally used examples of Nos. 7-15 are rare. Nos. 13-15 exist with a favor cancel having a hyphen between \"San\" and \"Jose.\" Values same as unused. Fake cancellations exist.\n",
    "Counterfeits exist of surcharges on Nos. 7-15.\n",
    "---------------\n",
    " stamp of Gen. Prospero Fernández : figure\n",
    "\n",
    "Gen. Prospero\n",
    "Fernández - A6\n",
    "\n",
    "1883, Jan. 1\n",
    "\n",
    "| | | | | |\n",
    "|---|---|---|---|---|\n",
    "| 16 | A6 | 1c green | 3.00 | 1.50 |\n",
    "| 17 | A6 | 2c carmine | 3.25 | 1.50 |\n",
    "| 18 | A6 | 5c blue violet | 32.50 | 2.00 |\n",
    "| 19 | A6 | 10c orange | 150.00 | 12.00 |\n",
    "| 20 | A6 | 40c blue | 3.00 | 3.00 |\n",
    "| Nos. 16-20 (5) | | | 191.75 | 20.00 |\n",
    "\n",
    "Unused examples of 40c usually lack gum.\n",
    "For overprints see Nos. O1-O20, O24,\n",
    "Guanacaste 1-38, 44.\n",
    "    \n",
    "\"\"\"\n",
    "    \n",
    "    # Initialize parser\n",
    "    parser = SimpleScottParser(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"your-api-key\"),\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    # Parse and display\n",
    "    result = parser.parse_and_display(chunk_text)\n",
    "    \n",
    "    # Save results\n",
    "    with open(\"scott_parse_results.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to scott_parse_results.json\")\n",
    "    \n",
    "    # Summary\n",
    "    stamps = result.get(\"stamps\", [])\n",
    "    main_stamps = [s for s in stamps if not s.get(\"variety_of\")]\n",
    "    varieties = [s for s in stamps if s.get(\"variety_of\")]\n",
    "    \n",
    "    \n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"  Total entries: {len(stamps)}\")\n",
    "    print(f\"  Main stamps: {len(main_stamps)}\")\n",
    "    print(f\"  Varieties: {len(varieties)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Run the test\n",
    "#     test_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9df3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(group_chunks_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72507695",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chunk = group_chunks_merged[592]\n",
    "group_text = \"\"\n",
    "for chunk in group_chunk:\n",
    "        # print(f\"Tipo: {chunk.get('type', 'N/A')}\")\n",
    "        # print(f\"Texto: {_clean_chunk_text(chunk.get('markdown', 'N/A'))}\")\n",
    "        # print(f\"Página: {chunk.get('grounding', 'N/A')['page']}\")\n",
    "        # print(\"---------------\")\n",
    "        group_text += _clean_chunk_text(chunk.get('markdown', 'N/A'))\n",
    "print(group_text)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642347c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = SimpleScottParser(\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"your-api-key\"),\n",
    "model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "# Parse and display\n",
    "result = parser.parse_and_display(group_text)\n",
    "\n",
    "# Save results\n",
    "with open(\"scott_parse_results_1-17.json\", \"w\") as f:\n",
    "    json.dump(result, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to scott_parse_results.json\")\n",
    "\n",
    "# Summary\n",
    "stamps = result.get(\"stamps\", [])\n",
    "main_stamps = [s for s in stamps if not s.get(\"variety_of\")]\n",
    "varieties = [s for s in stamps if s.get(\"variety_of\")]\n",
    "illustrations = result.get(\"illustrations\", [])\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"  Total entries: {len(stamps)}\")\n",
    "print(f\"  Main stamps: {len(main_stamps)}\")\n",
    "print(f\"  Varieties: {len(varieties)}\")\n",
    "print(f\"  Illustrations: {len(illustrations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b500dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "error_groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, datetime, traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "parser = SimpleScottParser(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"your-api-key\"),\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "total = len(group_chunks_merged)\n",
    "start = time.perf_counter()\n",
    "\n",
    "start_num = 1\n",
    "start_idx = start_num - 1  # = 13\n",
    "\n",
    "remaining = len(group_chunks_merged[start_idx:])\n",
    "\n",
    "\n",
    "with tqdm(total=total, desc=\"Parseando grupos\", unit=\"grp\") as pbar:\n",
    "    for i, group_chunk in enumerate(group_chunks_merged[start_idx:], start_num):\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            group_text = \"\".join(_clean_chunk_text(ch.get('markdown', 'N/A')) for ch in group_chunk)\n",
    "            result = parser.parse_and_display(group_text)\n",
    "            results.append(result)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Guarda índice, error y (opcional) un recorte del texto para depurar\n",
    "            error_groups.append({\n",
    "                \"group_number\": i,\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "                # Si quieres: \"sample\": group_text[:500] if 'group_text' in locals() else \"\"\n",
    "            })\n",
    "        finally:\n",
    "            # Actualiza métricas/ETA y la barra aunque haya fallo\n",
    "            iter_sec = time.perf_counter() - t0\n",
    "            elapsed = time.perf_counter() - start\n",
    "            done = (i - start_num + 1)  # iteraciones totales (éxito+fallo) desde que empezaste\n",
    "            avg = elapsed / done\n",
    "            remaining_sec = avg * (remaining - done)\n",
    "            eta = datetime.timedelta(seconds=max(0, int(remaining_sec)))\n",
    "\n",
    "            pbar.set_postfix(iter_s=f\"{iter_sec:.2f}\", avg_s=f\"{avg:.2f}\", eta=str(eta))\n",
    "            pbar.update(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd26972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar\n",
    "with open(\"results/parsed_catalogues/scott_parse_results_1-17.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nResults saved to scott_parse_results_1-17.json\")\n",
    "print(f\"Tiempo total: {datetime.timedelta(seconds=int(time.perf_counter()-start))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e19506",
   "metadata": {},
   "source": [
    "### Codigo en Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c118681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, datetime\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "def chunked(it, size):\n",
    "    it = iter(it)\n",
    "    while True:\n",
    "        batch = list(islice(it, size))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "\n",
    "# --- Preparación de entradas ---\n",
    "inputs = []\n",
    "start_num = 1\n",
    "start_idx = start_num - 1\n",
    "for i, group_chunk in enumerate(group_chunks_merged[start_idx:], start_num):\n",
    "    try:\n",
    "        group_text = \"\".join(_clean_chunk_text(ch.get('markdown', 'N/A')) for ch in group_chunk)\n",
    "        inputs.append({\"i\": i, \"input\": group_text})\n",
    "    except Exception as e:\n",
    "        # Si incluso preparar el texto falla, lo registramos y NO lo mandamos al LLM\n",
    "        # (opcional: podrías agregarlo igual y que falle abajo)\n",
    "        pass\n",
    "\n",
    "max_concurrency = 3      # ajusta según límites\n",
    "subbatch_size   = 5     # tamaño de oleadas\n",
    "max_retries     = 2      # reintentos por oleada\n",
    "\n",
    "results = [None] * len(inputs)\n",
    "error_groups = []\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "with tqdm(total=len(inputs), desc=\"Parseando (batch)\", unit=\"grp\") as pbar:\n",
    "    with get_openai_callback() as cb:\n",
    "        base = 0\n",
    "        for sub in chunked(inputs, subbatch_size):\n",
    "            sub_payload = [{\"input\": s[\"input\"]} for s in sub]\n",
    "\n",
    "            # --- Llamada batch con reintentos (try/except) ---\n",
    "            outs = None\n",
    "            last_err = None\n",
    "            for attempt in range(1, max_retries + 1):\n",
    "                try:\n",
    "                    outs = parser.chain.batch(\n",
    "                        sub_payload,\n",
    "                        config={\"max_concurrency\": max_concurrency},\n",
    "                        return_exceptions=True  # <- errores por ítem como objetos Exception\n",
    "                    )\n",
    "                    break  # éxito: salimos del bucle de reintentos\n",
    "                except Exception as e:\n",
    "                    last_err = e\n",
    "                    # Backoff exponencial simple\n",
    "                    if attempt < max_retries:\n",
    "                        sleep(2 ** (attempt - 1))\n",
    "                    else:\n",
    "                        # Si falló toda la oleada tras reintentos, marcamos todos los ítems de esta oleada como error\n",
    "                        for j in range(len(sub)):\n",
    "                            idx_global = base + j\n",
    "                            item = inputs[idx_global]\n",
    "                            error_groups.append({\n",
    "                                \"group_number\": item[\"i\"],\n",
    "                                \"error\": f\"BATCH_FAILURE: {type(e).__name__}: {str(e)}\"\n",
    "                            })\n",
    "                        # avanzamos la barra igualmente\n",
    "                        pbar.update(len(sub))\n",
    "\n",
    "            if outs is None:\n",
    "                # Ya registramos los errores y actualizamos pbar arriba\n",
    "                base += len(sub)\n",
    "                continue\n",
    "\n",
    "            # --- Procesar salidas por ítem (try implícito con return_exceptions=True) ---\n",
    "            for j, out in enumerate(outs):\n",
    "                idx_global = base + j\n",
    "                item = inputs[idx_global]\n",
    "                if isinstance(out, Exception):\n",
    "                    error_groups.append({\n",
    "                        \"group_number\": item[\"i\"],\n",
    "                        \"error\": f\"ITEM_FAILURE: {type(out).__name__}: {str(out)}\"\n",
    "                    })\n",
    "                else:\n",
    "                    results[idx_global] = out\n",
    "                pbar.update(1)\n",
    "\n",
    "            base += len(sub)\n",
    "\n",
    "        print(f\"\\nTokens prompt: {cb.prompt_tokens} | completion: {cb.completion_tokens} | total: {cb.total_tokens}\")\n",
    "        print(f\"Costo total (USD): {cb.total_cost:.6f}\")\n",
    "\n",
    "elapsed = time.perf_counter() - t0\n",
    "print(f\"Tiempo total: {datetime.timedelta(seconds=int(elapsed))}\")\n",
    "\n",
    "# --- Guardar ---\n",
    "os.makedirs(\"results/parsed_catalogues\", exist_ok=True)\n",
    "ok = [r for r in results if r is not None]\n",
    "with open(\"results/parsed_catalogues/scott_parse_results_18-34.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ok, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with open(\"results/parsed_catalogues/scott_parse_errors_18-34.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(error_groups, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"OK: {len(ok)} | Errores: {len(error_groups)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be511cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Scott Catalog Parser using LangChain + OpenAI + Pydantic\n",
    "# Enhanced version with expert philatelic knowledge for Costa Rica stamps\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import re\n",
    "# from typing import List, Optional, Dict, Any, Union\n",
    "# from decimal import Decimal\n",
    "# from datetime import datetime\n",
    "# from enum import Enum\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import PydanticOutputParser\n",
    "# from langchain.output_parsers import OutputFixingParser\n",
    "# from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "\n",
    "# # Import your stamp models (assuming they're in a file called 'stamp_models.py')\n",
    "# # from stamp_models import ScottEntry, ScottNumber, Denomination, ColorDescription, Perforation, MonetaryValue, PrintingMethod, StampType, PaperType\n",
    "\n",
    "# # ============================================================================\n",
    "# # ENHANCED PARSING MODELS\n",
    "# # ============================================================================\n",
    "\n",
    "# class ChunkType(str, Enum):\n",
    "#     \"\"\"Types of content chunks from ADE Parse\"\"\"\n",
    "#     TEXT = \"text\"           # Regular text content\n",
    "#     TABLE = \"table\"         # Tabular data\n",
    "#     MARGINALIA = \"marginalia\"  # Side notes\n",
    "#     FIGURE = \"figure\"       # Image descriptions\n",
    "#     LOGO = \"logo\"          # Logo images\n",
    "#     CARD = \"card\"          # Card elements (DPT-2)\n",
    "#     ATTESTATION = \"attestation\"  # Attestations (DPT-2)\n",
    "#     SCAN_CODE = \"scan_code\"  # QR/barcodes (DPT-2)\n",
    "\n",
    "\n",
    "# class StampEntryRaw(BaseModel):\n",
    "#     \"\"\"Raw stamp entry as parsed from catalog\"\"\"\n",
    "#     scott_number: str = Field(..., description=\"Full Scott number like '23', 'C146a', '45'\")\n",
    "#     illustration_number: Optional[str] = Field(None, description=\"Illustration like 'A8', 'A30'\")\n",
    "#     denomination: str = Field(..., description=\"Raw denomination like '1c', '5c', '1col'\")\n",
    "#     color: str = Field(..., description=\"Color description like 'green & blk'\")\n",
    "#     mint_value: Optional[float] = Field(None, description=\"Mint condition value in USD\")\n",
    "#     used_value: Optional[float] = Field(None, description=\"Used condition value in USD\")\n",
    "#     perforation: Optional[str] = Field(None, description=\"Perforation measurement\")\n",
    "#     year: Optional[int] = Field(None, description=\"Year of issue\")\n",
    "#     notes: Optional[str] = Field(None, description=\"Special notes about this stamp\")\n",
    "#     is_variety: bool = Field(default=False, description=\"Is this a variety (a, b, c suffix)\")\n",
    "#     variety_description: Optional[str] = Field(None, description=\"Description of the variety\")\n",
    "#     parent_scott_number: Optional[str] = Field(None, description=\"Main stamp number if this is a variety\")\n",
    "\n",
    "\n",
    "# class ScottEntryResponse(BaseModel):\n",
    "#     \"\"\"Response model for parsing Scott catalog entries\"\"\"\n",
    "#     entries: List[StampEntryRaw] = Field(\n",
    "#         default_factory=list,\n",
    "#         description=\"List of Scott stamp entries found in the chunk\"\n",
    "#     )\n",
    "#     has_stamp_info: bool = Field(\n",
    "#         ...,\n",
    "#         description=\"Whether this chunk contains actual stamp catalog entries\"\n",
    "#     )\n",
    "#     chunk_type: ChunkType = Field(\n",
    "#         ...,\n",
    "#         description=\"Original chunk type from ADE Parse\"\n",
    "#     )\n",
    "#     section_header: Optional[str] = Field(\n",
    "#         None,\n",
    "#         description=\"Section header if present (e.g., '1901, Jan.', 'Perf. 12-15½')\"\n",
    "#     )\n",
    "#     perforation_info: Optional[str] = Field(\n",
    "#         None,\n",
    "#         description=\"Perforation specification for the section\"\n",
    "#     )\n",
    "#     cross_references: List[str] = Field(\n",
    "#         default_factory=list,\n",
    "#         description=\"Cross-references to other catalog sections\"\n",
    "#     )\n",
    "#     warnings: List[str] = Field(\n",
    "#         default_factory=list,\n",
    "#         description=\"Warnings or notes about forgeries, reprints, etc.\"\n",
    "#     )\n",
    "#     summary_line: Optional[str] = Field(\n",
    "#         None,\n",
    "#         description=\"Summary line like 'Nos. 45-54 (10)'\"\n",
    "#     )\n",
    "#     parsing_notes: Optional[str] = Field(\n",
    "#         None,\n",
    "#         description=\"Notes about the parsing process\"\n",
    "#     )\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # ENHANCED SCOTT CATALOG PARSER\n",
    "# # ============================================================================\n",
    "\n",
    "# class ScottCatalogExpertParser:\n",
    "#     \"\"\"Expert parser for Scott catalog with deep philatelic knowledge\"\"\"\n",
    "    \n",
    "#     def __init__(self, openai_api_key: str, model_name: str = \"gpt-4o-mini\"):\n",
    "#         \"\"\"\n",
    "#         Initialize the expert parser\n",
    "        \n",
    "#         Args:\n",
    "#             openai_api_key: OpenAI API key\n",
    "#             model_name: Model to use (gpt-4o-mini, gpt-4o, etc.)\n",
    "#         \"\"\"\n",
    "#         self.llm = ChatOpenAI(\n",
    "#             temperature=0.0,  # Zero temperature for maximum consistency\n",
    "#             model_name=model_name,\n",
    "#             openai_api_key=openai_api_key,\n",
    "#             max_tokens=2000\n",
    "#         )\n",
    "        \n",
    "#         # Setup output parser with Pydantic\n",
    "#         self.pydantic_parser = PydanticOutputParser(pydantic_object=ScottEntryResponse)\n",
    "        \n",
    "#         # Add fixing parser for robustness\n",
    "#         self.output_parser = OutputFixingParser.from_llm(\n",
    "#             parser=self.pydantic_parser,\n",
    "#             llm=self.llm,\n",
    "#             max_retries=2\n",
    "#         )\n",
    "        \n",
    "#         # Create the expert prompt template\n",
    "#         self.prompt = self._create_expert_prompt_template()\n",
    "        \n",
    "#         # Create the chain\n",
    "#         self.chain = self.prompt | self.llm | self.output_parser\n",
    "    \n",
    "#     def _create_expert_prompt_template(self) -> ChatPromptTemplate:\n",
    "#         \"\"\"Create the expert philatelic prompt template\"\"\"\n",
    "        \n",
    "#         system_message = \"\"\"You are a world-renowned philatelic expert specializing in Costa Rica stamps and the Scott catalog system. You have decades of experience parsing Scott catalog entries with perfect accuracy.\n",
    "\n",
    "# SCOTT CATALOG ENTRY STRUCTURE:\n",
    "# The standard format is: [Scott#] [Illustration#] [Denomination] [Color] [Mint$] [Used$]\n",
    "# Example: \"23 A8 1c rose 5.00 3.00\"\n",
    "\n",
    "# This means:\n",
    "# - Scott #23\n",
    "# - Uses illustration A8 (design type)\n",
    "# - 1 centavo denomination\n",
    "# - Rose color\n",
    "# - $5.00 mint value\n",
    "# - $3.00 used value\n",
    "\n",
    "# CRITICAL PARSING RULES:\n",
    "\n",
    "# 1. SCOTT NUMBER FORMATS:\n",
    "#    - Regular: \"1\", \"23\", \"146\" (just numbers)\n",
    "#    - Airmail: \"C1\", \"C146\" (C prefix)\n",
    "#    - Semi-postal: \"B1\" (B prefix)\n",
    "#    - Official: \"O1\" (O prefix)\n",
    "#    - Postage Due: \"J1\" (J prefix)\n",
    "#    - Major varieties: \"16A\", \"23B\" (capital letter suffix)\n",
    "#    - Minor varieties: \"146a\", \"23c\" (lowercase letter suffix)\n",
    "\n",
    "# 2. ILLUSTRATION NUMBERS:\n",
    "#    - Format: Letter(s) + Number like \"A1\", \"A8\", \"AP12\", \"D3\"\n",
    "#    - These are DESIGN TYPES, not individual stamps\n",
    "#    - Multiple stamps can share the same illustration number\n",
    "#    - Critical for identifying reprints and varieties\n",
    "\n",
    "# 3. DENOMINATION PARSING:\n",
    "#    - \"1c\" = 1 centavo\n",
    "#    - \"5c\" = 5 centavos\n",
    "#    - \"10c\" = 10 centavos (NOT centimos)\n",
    "#    - \"1r\" = 1 real\n",
    "#    - \"1/2r\" = medio real (0.5 real)\n",
    "#    - \"1p\" = 1 peso\n",
    "#    - \"₡1\" or \"1col\" = 1 colón\n",
    "\n",
    "# 4. COLOR DESCRIPTIONS:\n",
    "#    Standard Scott colors include:\n",
    "#    - Basic: black, blue, brown, green, red, violet, yellow, orange\n",
    "#    - Shades: deep, dark, light, pale, bright, dull\n",
    "#    - Specific: carmine, vermillion, ultramarine, emerald, scarlet\n",
    "#    - Compounds: \"blue & red\", \"green and black\"\n",
    "#    - Complex: \"carmine rose\", \"yellow brown\", \"blue green\"\n",
    "\n",
    "# 5. VALUE PARSING:\n",
    "#    - Format: [mint] [used] like \"5.00 3.00\"\n",
    "#    - Dash means no established value: \"5.00 -\"\n",
    "#    - Values are in USD without $ symbol\n",
    "#    - Handle ranges by taking first value\n",
    "\n",
    "# 6. PERFORATION FORMATS:\n",
    "#    - Simple: \"Perf. 12\", \"Perf. 14\"\n",
    "#    - Compound: \"Perf. 12x11½\", \"Perf. 14x13\"\n",
    "#    - Range: \"Perf. 14-16\"\n",
    "#    - Complex: \"Perf. 14-16 & Compound\"\n",
    "#    - Imperforate: \"Imperf\" or \"Imperforate\"\n",
    "\n",
    "# 7. OVERPRINT SECTIONS:\n",
    "#    - Headers like \"1889 Black Overprint\" indicate overprinted stamps\n",
    "#    - These are regular stamps with additional printing\n",
    "#    - Note overprint color and year\n",
    "#    - Watch for warnings about forgeries\n",
    "\n",
    "# 8. FIGURE DESCRIPTIONS:\n",
    "#    - Chunks with <::...::> contain stamp image descriptions\n",
    "#    - Extract illustration numbers (A10, A11, etc.)\n",
    "#    - Match denominations and colors from descriptions\n",
    "#    - These provide visual confirmation but aren't catalog entries themselves\n",
    "\n",
    "# 9. TABLE FORMAT:\n",
    "#    - Tables may have rowspan/colspan\n",
    "#    - Item column often has Scott# and Illustration#\n",
    "#    - Parse each row as separate entry\n",
    "\n",
    "# 10. WARNINGS AND NOTES:\n",
    "#    - \"Vertical and inverted overprints are fakes\" = forgery warning\n",
    "#    - \"For overprints see...\" = cross-reference\n",
    "#    - \"Nos. 1-4 exist imperforate...\" = variety note\n",
    "\n",
    "# 11. SECTION HEADERS:\n",
    "#    - Year headers: \"1889\", \"1901\"  \n",
    "#    - Type headers: \"Black Overprint\", \"Postage Due\", \"Air Post\"\n",
    "#    - Issue names: \"First Issue\", \"Coat of Arms\"\n",
    "\n",
    "# 12. CHUNK CLASSIFICATION:\n",
    "#    - stamp_listings: Contains actual catalog entries with Scott numbers\n",
    "#    - overprint_section: Overprinted stamp listings\n",
    "#    - figure_description: Visual descriptions of stamps\n",
    "#    - table_data: Tabular stamp data\n",
    "#    - notes: Explanatory text without listings\n",
    "#    - cross_reference: References to other sections\n",
    "#    - header: Section/year headers only\n",
    "#    - logo: Publisher logos (ignore)\n",
    "#    - other: Unrelated content\n",
    "\n",
    "# {format_instructions}\n",
    "\n",
    "# EXPERT GUIDELINES:\n",
    "# - Parse EXACTLY what's in the text - no assumptions\n",
    "# - Preserve all technical details precisely\n",
    "# - Flag any suspicious or ambiguous entries\n",
    "# - Note all warnings about forgeries or reprints\n",
    "# - Extract ALL cross-references for context\n",
    "# - If a chunk has a section header, always include it\n",
    "# - For figure descriptions, extract illustration mappings even if no catalog entries\n",
    "# - Convert fractional denominations: 1/2 = 0.5, 1/3 = 0.333, 1/4 = 0.25\n",
    "# - Maintain the distinction between centavos (monetary) and centimos (never used in Costa Rica)\"\"\"\n",
    "        \n",
    "#         user_message = \"\"\"Parse this Scott catalog chunk with expert precision:\n",
    "\n",
    "# CHUNK CONTENT:\n",
    "# {chunk_text}\n",
    "\n",
    "# Instructions:\n",
    "# 1. Identify the chunk type first\n",
    "# 2. Extract all stamp entries if present\n",
    "# 3. Note section headers, warnings, and cross-references\n",
    "# 4. For figures, map illustration numbers to descriptions\n",
    "# 5. Flag any parsing ambiguities in parsing_notes\"\"\"\n",
    "        \n",
    "#         return ChatPromptTemplate.from_messages([\n",
    "#             (\"system\", system_message),\n",
    "#             (\"user\", user_message)\n",
    "#         ]).partial(format_instructions=self.pydantic_parser.get_format_instructions())\n",
    "    \n",
    "#     def parse_chunk(self, chunk_data: Union[str, Dict[str, Any]], \n",
    "#                    chunk_id: Optional[str] = None) -> ScottEntryResponse:\n",
    "#         \"\"\"\n",
    "#         Parse a single chunk of catalog text\n",
    "        \n",
    "#         Args:\n",
    "#             chunk_data: Either raw text or chunk dict from ADE Parse\n",
    "#             chunk_id: Optional identifier for the chunk\n",
    "            \n",
    "#         Returns:\n",
    "#             ScottEntryResponse with extracted stamps or metadata\n",
    "#         \"\"\"\n",
    "#         # Extract text and metadata based on input type\n",
    "#         if isinstance(chunk_data, dict):\n",
    "#             chunk_text = chunk_data.get(\"markdown\", chunk_data.get(\"text\", \"\"))\n",
    "#             chunk_id = chunk_id or chunk_data.get(\"id\")\n",
    "#             # Get the actual ADE chunk type\n",
    "#             ade_chunk_type = chunk_data.get(\"type\", \"text\")\n",
    "#             try:\n",
    "#                 chunk_type = ChunkType(ade_chunk_type)\n",
    "#             except ValueError:\n",
    "#                 chunk_type = ChunkType.TEXT\n",
    "#         else:\n",
    "#             chunk_text = chunk_data\n",
    "#             chunk_type = ChunkType.TEXT\n",
    "        \n",
    "#         # Clean up markdown artifacts\n",
    "#         chunk_text = self._clean_chunk_text(chunk_text)\n",
    "        \n",
    "#         try:\n",
    "#             # Invoke the chain with the chunk text\n",
    "#             result = self.chain.invoke({\"chunk_text\": chunk_text})\n",
    "            \n",
    "#             # Override with actual chunk type from ADE\n",
    "#             result.chunk_type = chunk_type\n",
    "            \n",
    "#             # Post-process results\n",
    "#             result = self._post_process_results(result, chunk_id)\n",
    "            \n",
    "#             return result\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             # Return error response\n",
    "#             return ScottEntryResponse(\n",
    "#                 entries=[],\n",
    "#                 has_stamp_info=False,\n",
    "#                 chunk_type=chunk_type,\n",
    "#                 parsing_notes=f\"Parsing error: {str(e)}\"\n",
    "#             )\n",
    "    \n",
    "#     def _clean_chunk_text(self, text: str) -> str:\n",
    "#         \"\"\"Clean markdown and formatting artifacts\"\"\"\n",
    "#         # Remove anchor tags\n",
    "#         text = re.sub(r'<a id=[\\'\"][^\\'\"]+[\\'\"]></a>\\n*', '', text)\n",
    "#         # Remove figure markup but keep content\n",
    "#         text = re.sub(r'<::(.*?)::>', r'\\1', text, flags=re.DOTALL)\n",
    "#         # Clean excessive whitespace\n",
    "#         text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "#         return text.strip()\n",
    "    \n",
    "#     def _post_process_results(self, result: ScottEntryResponse, \n",
    "#                              chunk_id: Optional[str]) -> ScottEntryResponse:\n",
    "#         \"\"\"Post-process and validate parsing results\"\"\"\n",
    "        \n",
    "#         # Add chunk_id to metadata\n",
    "#         if chunk_id:\n",
    "#             for entry in result.entries:\n",
    "#                 if not hasattr(entry, 'metadata'):\n",
    "#                     entry.metadata = {}\n",
    "#                 entry.metadata[\"chunk_id\"] = chunk_id\n",
    "        \n",
    "#         # Validate Scott numbers\n",
    "#         for entry in result.entries:\n",
    "#             if not self._validate_scott_number(entry.scott_number):\n",
    "#                 if result.parsing_notes:\n",
    "#                     result.parsing_notes += f\"\\nWarning: Suspicious Scott number: {entry.scott_number}\"\n",
    "#                 else:\n",
    "#                     result.parsing_notes = f\"Warning: Suspicious Scott number: {entry.scott_number}\"\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def _validate_scott_number(self, scott_number: str) -> bool:\n",
    "#         \"\"\"Validate Scott number format\"\"\"\n",
    "#         pattern = r'^([A-Z]{0,4})?(\\d+)([A-Z])?([a-z]+)?$'\n",
    "#         return bool(re.match(pattern, scott_number))\n",
    "    \n",
    "#     def parse_chunks_batch(self, chunks: List[Union[str, Dict]], \n",
    "#                           show_progress: bool = True) -> Dict[str, Any]:\n",
    "#         \"\"\"\n",
    "#         Parse multiple chunks in batch with progress tracking\n",
    "        \n",
    "#         Args:\n",
    "#             chunks: List of chunks (text or dicts)\n",
    "#             show_progress: Show progress messages\n",
    "            \n",
    "#         Returns:\n",
    "#             Comprehensive results dictionary\n",
    "#         \"\"\"\n",
    "#         results = []\n",
    "#         errors = []\n",
    "#         statistics = {\n",
    "#             \"total_chunks\": len(chunks),\n",
    "#             \"chunks_with_stamps\": 0,\n",
    "#             \"total_stamps\": 0,\n",
    "#             \"total_varieties\": 0,\n",
    "#             \"text_chunks\": 0,\n",
    "#             \"table_chunks\": 0,\n",
    "#             \"figure_chunks\": 0,\n",
    "#             \"other_chunks\": 0\n",
    "#         }\n",
    "        \n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             if show_progress:\n",
    "#                 print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
    "            \n",
    "#             try:\n",
    "#                 response = self.parse_chunk(chunk)\n",
    "                \n",
    "#                 # Update chunk type statistics\n",
    "#                 if response.chunk_type == ChunkType.TEXT:\n",
    "#                     statistics[\"text_chunks\"] += 1\n",
    "#                 elif response.chunk_type == ChunkType.TABLE:\n",
    "#                     statistics[\"table_chunks\"] += 1\n",
    "#                 elif response.chunk_type == ChunkType.FIGURE:\n",
    "#                     statistics[\"figure_chunks\"] += 1\n",
    "#                 else:\n",
    "#                     statistics[\"other_chunks\"] += 1\n",
    "                \n",
    "#                 # Count stamps and varieties\n",
    "#                 if response.entries:\n",
    "#                     statistics[\"chunks_with_stamps\"] += 1\n",
    "#                     for entry in response.entries:\n",
    "#                         statistics[\"total_stamps\"] += 1\n",
    "#                         if entry.is_variety:\n",
    "#                             statistics[\"total_varieties\"] += 1\n",
    "                \n",
    "#                 # Store results if there's useful information\n",
    "#                 if response.entries or response.warnings or response.cross_references:\n",
    "#                     result_data = {\n",
    "#                         \"chunk_index\": i,\n",
    "#                         \"chunk_type\": response.chunk_type.value,\n",
    "#                         \"section_header\": response.section_header,\n",
    "#                         \"perforation\": response.perforation_info,\n",
    "#                         \"entries\": [entry.model_dump() for entry in response.entries],\n",
    "#                         \"cross_references\": response.cross_references,\n",
    "#                         \"warnings\": response.warnings,\n",
    "#                         \"summary\": response.summary_line,\n",
    "#                         \"notes\": response.parsing_notes\n",
    "#                     }\n",
    "                    \n",
    "#                     # Add chunk_id if available\n",
    "#                     if isinstance(chunk, dict) and \"id\" in chunk:\n",
    "#                         result_data[\"chunk_id\"] = chunk[\"id\"]\n",
    "                    \n",
    "#                     results.append(result_data)\n",
    "                    \n",
    "#                     if show_progress:\n",
    "#                         if response.entries:\n",
    "#                             print(f\"  ✓ Found {len(response.entries)} stamps\")\n",
    "#                         else:\n",
    "#                             print(f\"  ✓ Found metadata/references\")\n",
    "#                 else:\n",
    "#                     if show_progress:\n",
    "#                         print(f\"  - No stamp data (type: {response.chunk_type.value})\")\n",
    "                \n",
    "#             except Exception as e:\n",
    "#                 error_data = {\n",
    "#                     \"chunk_index\": i,\n",
    "#                     \"error\": str(e),\n",
    "#                     \"chunk_preview\": str(chunk)[:200] if isinstance(chunk, str) \n",
    "#                                     else chunk.get(\"markdown\", \"\")[:200]\n",
    "#                 }\n",
    "#                 errors.append(error_data)\n",
    "#                 if show_progress:\n",
    "#                     print(f\"  ✗ Error: {str(e)[:50]}...\")\n",
    "        \n",
    "#         return {\n",
    "#             \"results\": results,\n",
    "#             \"errors\": errors,\n",
    "#             \"statistics\": statistics,\n",
    "#             \"summary\": self._generate_summary(statistics)\n",
    "#         }\n",
    "    \n",
    "#     def _generate_summary(self, stats: Dict[str, Any]) -> str:\n",
    "#         \"\"\"Generate a human-readable summary\"\"\"\n",
    "#         return (\n",
    "#             f\"Processed {stats['total_chunks']} chunks:\\n\"\n",
    "#             f\"- Found {stats['total_stamps']} stamp entries total\\n\"\n",
    "#             f\"  - Main stamps: {stats['total_stamps'] - stats['total_varieties']}\\n\"\n",
    "#             f\"  - Varieties: {stats['total_varieties']}\\n\"\n",
    "#             f\"- Chunks with stamps: {stats['chunks_with_stamps']}\\n\"\n",
    "#             f\"- Chunk types: {stats['text_chunks']} text, {stats['table_chunks']} table, \"\n",
    "#             f\"{stats['figure_chunks']} figure, {stats['other_chunks']} other\"\n",
    "#         )\n",
    "    \n",
    "#     def save_results(self, results: Dict[str, Any], output_file: str):\n",
    "#         \"\"\"Save results to JSON with proper formatting\"\"\"\n",
    "        \n",
    "#         with open(output_file, 'w', encoding='utf-8') as f:\n",
    "#             json.dump(results, f, indent=2, ensure_ascii=False, default=str)\n",
    "        \n",
    "#         print(f\"\\n✓ Results saved to {output_file}\")\n",
    "#         print(\"\\nSummary:\")\n",
    "#         print(results.get(\"summary\", \"No summary available\"))\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # USAGE EXAMPLES\n",
    "# # ============================================================================\n",
    "\n",
    "# def test_with_real_chunks():\n",
    "#     \"\"\"Test with the real chunk examples provided\"\"\"\n",
    "#     OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "#     # Initialize parser\n",
    "#     parser = ScottCatalogExpertParser(\n",
    "#         openai_api_key=OPENAI_API_KEY,\n",
    "#         model_name=\"gpt-4o-mini\"\n",
    "#     )\n",
    "    \n",
    "#     # Real chunks from ADE Parse\n",
    "#     test_chunks = [\n",
    "                \n",
    "#         {\n",
    "#             \"id\": \"ca72a5ed-984e-4264-9d63-f1ebf2fdc47e\",\n",
    "#             \"grounding\": {\n",
    "#             \"box\": {\n",
    "#                 \"bottom\": 0.8415362238883972,\n",
    "#                 \"left\": 0.05093258619308472,\n",
    "#                 \"right\": 0.268806517124176,\n",
    "#                 \"top\": 0.6473554968833923\n",
    "#             },\n",
    "#             \"page\": 1\n",
    "#             },\n",
    "#             \"markdown\": \"<a id='ca72a5ed-984e-4264-9d63-f1ebf2fdc47e'></a>\\n\\n1901, Jan.                                Perf. 12-15½\\n45 A30    1c green & blk                  3.25    .30\\na.        Horiz. pair, imperf. btwn.      150.00\\n46 A31    2c ver & blk                    1.25    .30\\n47 A32    5c gray blue &\\n          blk                             3.25    .30\\na.        Vert. pair, imperf. btwn.       —       300.00\\n48 A33    10c ocher & blk                 3.25    .35\\n49 A34    20c lake & blk                  22.50   .25\\na.        Vert. pair, imperf. btwn.       1,000.\\n50 A35    50c dull lil & dk bl            5.50    1.00\\n51 A36    1col ol bis & blk               110.00  3.50\\n52 A37    2col car rose & dk\\n          grn                             16.00   3.00\\n53 A38    5col brown & blk                75.00   3.50\\n54 A39    10col yel grn & brn\\n          red                             29.00   3.00\\nNos. 45-54 (10)                           269.00  15.50\\n\\nThe 2c exists with center inverted. Value\\n$77,500.\\nNos. 45-57 in other colors are private\\nreprints made in 1948. They have little value.\\nFor surcharge and overprints see Nos. 58,\\n78, O37-O44.\",\n",
    "#             \"type\": \"text\"\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Process chunks\n",
    "#     results = parser.parse_chunks_batch(test_chunks)\n",
    "    \n",
    "#     # Save results\n",
    "#     parser.save_results(results, \"costa_rica_stamps_test.json\")\n",
    "    \n",
    "#     # Print detailed results\n",
    "#     for result in results[\"results\"]:\n",
    "#         print(f\"\\n{'='*50}\")\n",
    "#         print(f\"Chunk Type: {result['chunk_type']}\")\n",
    "#         if result['section_header']:\n",
    "#             print(f\"Section: {result['section_header']}\")\n",
    "#         print(f\"Stamps found: {len(result['entries'])}\")\n",
    "        \n",
    "#         for entry in result['entries']:\n",
    "#             print(f\"  - Scott #{entry['scott_number']}: {entry['denomination']} {entry['color']}\")\n",
    "#             if entry.get('mint_value'):\n",
    "#                 print(f\"    Values: ${entry['mint_value']} mint, ${entry.get('used_value', '-')} used\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Test with real chunks\n",
    "#     test_with_real_chunks()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
