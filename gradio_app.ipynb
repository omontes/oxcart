{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philatelic Gradio App with Weaviate\n",
    "Interactive web interface for CR Philately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Load all the modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from datetime import datetime\n",
    "import weaviate\n",
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_weaviate import WeaviateVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.retrievers import MultiQueryRetriever, EnsembleRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "from philatelic_weaviate import *\n",
    "\n",
    "from philatelic_chunk_schema import *\n",
    "\n",
    "print(\"‚úÖ Basic imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WEAVIATE_URL = os.getenv('WEAVIATE_URL', 'http://localhost:8083')\n",
    "PHILATELIC_JSONS_DIR = os.getenv('PHILATELIC_JSONS_DIR', './results/final_jsons')\n",
    "COLLECTION_NAME = os.getenv('WEAVIATE_COLLECTION_NAME', 'Oxcart')\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   ‚Ä¢ Weaviate URL: {WEAVIATE_URL}\")\n",
    "print(f\"   ‚Ä¢ JSONs Directory: {PHILATELIC_JSONS_DIR}\")\n",
    "print(f\"   ‚Ä¢ Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"   ‚Ä¢ OpenAI API Key: {'‚úÖ Configured' if OPENAI_API_KEY else '‚ùå Missing configuration'}\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"\\\\n‚ö†Ô∏è  IMPORTANT: Configure your OPENAI_API_KEY in the .env file\")\n",
    "    print(\"   Copy .env.example to .env and add your API key\")\n",
    "\n",
    "# Verify that the JSONs directory exists\n",
    "if not os.path.exists(PHILATELIC_JSONS_DIR):\n",
    "    print(f\"\\\\n‚ö†Ô∏è  Directory {PHILATELIC_JSONS_DIR} not found\")\n",
    "    print(\"   Make sure you have processed documents with the Dolphin parser\")\n",
    "else:\n",
    "    json_files = glob.glob(os.path.join(PHILATELIC_JSONS_DIR, '*_final.json'))\n",
    "    print(f\"\\\\nüìÅ Found {len(json_files)} philatelic JSON files\")\n",
    "    if json_files:\n",
    "        print(\"   Examples:\")\n",
    "        for file in json_files[:3]:\n",
    "            print(f\"   ‚Ä¢ {os.path.basename(file)}\")\n",
    "        if len(json_files) > 3:\n",
    "            print(f\"   ‚Ä¢ ... and {len(json_files) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            model=\"gpt-5-nano\", \n",
    "            api_key=OPENAI_API_KEY, \n",
    "            temperature=1,  # obligatorio para gpt-5-nano\n",
    "            timeout=120.0,\n",
    "            #max_completion_tokens=2500,\n",
    "            model_kwargs={\n",
    "                \"verbosity\": \"medium\",\n",
    "                \"reasoning_effort\" : \"low\"\n",
    "            })\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üìù RAG PROMPT TEMPLATE - Professional Philatelic Consultation\n",
    "# ========================================================================================\n",
    "\n",
    "philatelic_rag_template = \"\"\"You are a senior philatelic researcher and catalog specialist with expertise in Costa Rican stamps and postal history. Provide comprehensive, well-structured analysis based strictly on the source materials provided.\n",
    "\n",
    "SOURCE MATERIALS:\n",
    "{context}\n",
    "\n",
    "RESEARCH QUERY: {question}\n",
    "\n",
    "RESPONSE REQUIREMENTS:\n",
    "\n",
    "FORMATTING & STRUCTURE:\n",
    "‚Ä¢ Use clear hierarchical organization with descriptive headers using markdown\n",
    "‚Ä¢ Group related information under logical categories using ## and **bold subheadings**\n",
    "‚Ä¢ Use bullet points (‚Ä¢) for individual facts and varieties\n",
    "‚Ä¢ Include relevant emojis for major sections (üîç üìÆ üìö üéØ) to enhance readability\n",
    "‚Ä¢ Bold key terms, catalog numbers, and important details\n",
    "‚Ä¢ KEEP SECTIONS CONCISE - avoid excessive repetition or overly detailed explanations\n",
    "‚Ä¢ Your output is in markdown format\n",
    "\n",
    "CITATION FORMAT:\n",
    "‚Ä¢ Every factual statement must include the name of the document (doc_id) and its page numeber like this example: (CRF 100, page 15)\n",
    "‚Ä¢ Multiple sources: (doc_id, page number; doc_id, page number) Example: (OXCART 123, page 24 ; OXCART 25, page 15)\n",
    "‚Ä¢ Always cite catalog numbers (scott, yvert, michell, etc), varieties, dates, quantities, and technical specifications\n",
    "‚Ä¢ When quoting directly, use quotation marks around quoted text\n",
    "\n",
    "CONTENT ORGANIZATION:\n",
    "‚Ä¢ Lead with the most direct answer to the query\n",
    "‚Ä¢ Organize by catalog numbers, chronological order, or logical categories as appropriate\n",
    "‚Ä¢ Include technical specifications: dates, quantities, colors, perforations, varieties\n",
    "‚Ä¢ Provide brief historical context and collecting significance\n",
    "‚Ä¢ Note relationships between issues, varieties, or catalog entries\n",
    "‚Ä¢ Address valuation or rarity when relevant to the query\n",
    "\n",
    "RESPONSE LENGTH:\n",
    "‚Ä¢ Aim for clear, informative responses that are thorough but not excessive\n",
    "‚Ä¢ Eliminate redundant information and repetitive explanations\n",
    "‚Ä¢ Focus on the most relevant information that directly answers the query\n",
    "‚Ä¢ If information is extensive, prioritize the most important catalog entries and varieties\n",
    "\n",
    "TECHNICAL STANDARDS:\n",
    "‚Ä¢ Use precise philatelic terminology (definitive, commemorative, variety, error, overprint, etc.)\n",
    "‚Ä¢ Specify exact catalog numbers with proper formatting (Scott C216, not just C216)\n",
    "‚Ä¢ Include denomination and color details when available\n",
    "‚Ä¢ Note printing quantities, dates, and technical varieties\n",
    "‚Ä¢ Distinguish between verified catalog facts and expert opinions\n",
    "‚Ä¢ Flag incomplete or uncertain information clearly\n",
    "\n",
    "RESEARCH COMPLETENESS:\n",
    "‚Ä¢ If source materials are insufficient, state: \"The provided documents do not contain sufficient information about...\"\n",
    "‚Ä¢ Suggest what additional sources or information would be needed\n",
    "‚Ä¢ Note any gaps in catalog coverage or missing details\n",
    "\n",
    "PROFESSIONAL TONE:\n",
    "‚Ä¢ Maintain authoritative but accessible language\n",
    "‚Ä¢ Present information objectively without unnecessary qualifiers\n",
    "‚Ä¢ Use active voice and clear, direct statements\n",
    "‚Ä¢ Avoid speculation beyond what sources support\n",
    "\n",
    "RESPONSE:\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=philatelic_rag_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# üìÑ OPTIMIZED DOCUMENT FORMATTING - For Academic Citation Style\n",
    "# ========================================================================================\n",
    "\n",
    "def format_docs_for_rag(docs_results: List[Dict]) -> str:\n",
    "    \"\"\"Efficient document formatting optimized for academic citation style (Document Name, p. Page)\"\"\"\n",
    "    \n",
    "    if not docs_results:\n",
    "        return \"\\nNo source documents available.\"\n",
    "    \n",
    "    # Group and sort documents by authority\n",
    "    #doc_groups = {'catalog': [], 'literature': [], 'collection': [], 'reference': []}\n",
    "    docs = []\n",
    "    \n",
    "    for i, doc in enumerate(docs_results, 1):\n",
    "        #category, reliability = classify_document_authority(doc.metadata.get('doc_id', 'Unknown'))\n",
    "        \n",
    "        doc_info = {\n",
    "            'doc_num': i,\n",
    "            'doc_id': doc.metadata.get('doc_id', 'Unknown'),\n",
    "            'page': doc.metadata.get('page_number', 'N/A'),\n",
    "            'content': doc.page_content,\n",
    "        }\n",
    "        #doc_groups[category].append(doc_info)\n",
    "        docs.append(doc_info)\n",
    "    return docs\n",
    "\n",
    "def create_rag_response(retriever_results: List[Dict], query: str) -> Dict:\n",
    "    \"\"\"Streamlined RAG chain execution with academic citation style and token tracking\"\"\"\n",
    "    \n",
    "    if not retriever_results:\n",
    "        return {\n",
    "            \"response\": \"No documents found for this query.\", \n",
    "            \"generation_time\": 0,\n",
    "            \"context_docs_count\": 0,\n",
    "            \"context_length\": 0,\n",
    "            \"token_usage\": {\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"total_tokens\": 0\n",
    "            },\n",
    "            \"cost_info\": {\n",
    "                \"estimated_cost_usd\": 0,\n",
    "                \"input_cost\": 0,\n",
    "                \"output_cost\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Format context efficiently for academic citations\n",
    "    context = format_docs_for_rag(retriever_results)\n",
    "    \n",
    "    # Execute RAG chain with OpenAI callback for token tracking\n",
    "    rag_chain = (\n",
    "        {\"context\": lambda x: context, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use OpenAI callback to track token usage\n",
    "    with get_openai_callback() as cb:\n",
    "        response = rag_chain.invoke(query)\n",
    "        # Get token counts from callback\n",
    "        input_tokens = cb.prompt_tokens\n",
    "        output_tokens = cb.completion_tokens\n",
    "        total_tokens = cb.total_tokens\n",
    "        \n",
    "        # OpenAI callback provides cost directly, but we'll calculate our own\n",
    "        # based on GPT-5-nano pricing\n",
    "    \n",
    "    generation_time = round(time.time() - start_time, 2)\n",
    "    \n",
    "    # Calculate costs for GPT-5-nano\n",
    "    # $0.05 per 1M input tokens, $0.40 per 1M output tokens\n",
    "    cost_per_1m_input = 0.05\n",
    "    cost_per_1m_output = 0.40\n",
    "    \n",
    "    # Convert to cost per token\n",
    "    cost_per_input_token = cost_per_1m_input / 1_000_000\n",
    "    cost_per_output_token = cost_per_1m_output / 1_000_000\n",
    "    \n",
    "    input_cost = input_tokens * cost_per_input_token\n",
    "    output_cost = output_tokens * cost_per_output_token\n",
    "    estimated_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"generation_time\": generation_time,\n",
    "        \"context_docs_count\": len(retriever_results),\n",
    "        \"context_length\": len(context),        \n",
    "        \"token_usage\": {\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens\n",
    "        },\n",
    "        \"cost_info\": {\n",
    "            \"estimated_cost_usd\": round(estimated_cost, 6),\n",
    "            \"input_cost\": round(input_cost, 6),\n",
    "            \"output_cost\": round(output_cost, 6)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate\n",
    "print(\"üîå Connecting to Weaviate...\")\n",
    "\n",
    "try:\n",
    "    client = create_weaviate_client(WEAVIATE_URL, OPENAI_API_KEY)\n",
    "    print(\"‚úÖ Connection successful\")\n",
    "    \n",
    "    # Verify that Weaviate is working\n",
    "    meta = client.get_meta()\n",
    "    print(f\"üìä Weaviate version: {meta.get('version', 'unknown')}\")\n",
    "    \n",
    "    # Verify if collection exists\n",
    "    try:\n",
    "        collections = client.collections.list_all()\n",
    "        collection_names = [col.name for col in collections]\n",
    "        \n",
    "        if COLLECTION_NAME in collection_names:\n",
    "            collection = client.collections.get(COLLECTION_NAME)\n",
    "            total_objects = collection.aggregate.over_all(total_count=True).total_count\n",
    "            print(f\"üìä Collection '{COLLECTION_NAME}' exists with {total_objects} documents\")\n",
    "        else:\n",
    "            print(f\"üìù Collection '{COLLECTION_NAME}' does not exist (will be created during indexing)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not verify collections: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Weaviate: {e}\")\n",
    "    print(\"üí° Make sure Weaviate is running:\")\n",
    "    print(\"   docker-compose up -d\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weaviate Search Tests\n",
    "\n",
    "Test the function search_chunks_semantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_chunks_semantic(\n",
    "                client, \n",
    "                \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\", \n",
    "                \"Oxcart\", \n",
    "                limit=50,\n",
    "                filters=[],\n",
    "                mode = \"hybrid\",\n",
    "                alpha= 0.35\n",
    "                \n",
    "            )\n",
    "            \n",
    "print(f\"   üìä Resultados: {len(results)}\")\n",
    "\n",
    "for j, result in enumerate(results, 1):\n",
    "    print(f\"\\n      üè∑Ô∏è #{j} (Score: {result['score']:.3f})\")\n",
    "    print(f\"         üìÑ Documento: {result['doc_id']}\")\n",
    "    print(f\"         üìã Tipo: {result['chunk_type']}\")\n",
    "    print(f\"         üìÑ P√°gina: {result['page_number']}\")\n",
    "    \n",
    "    # Mostrar metadatos relevantes\n",
    "    if result.get('catalog_systems'):\n",
    "        print(f\"         üìñ Cat√°logos: {result['catalog_systems']}\")\n",
    "    if result.get('scott_numbers'):\n",
    "        print(f\"         üî¢ Scott: {result['scott_numbers']}\")\n",
    "    if result.get('years'):\n",
    "        print(f\"         üìÖ A√±os: {result['years']}\")\n",
    "    if result.get('colors'):\n",
    "        print(f\"         üé® Colores: {result['colors']}\")\n",
    "    if result.get('variety_classes'):\n",
    "        print(f\"         üîÄ Variedades: {result['variety_classes']}\")\n",
    "    \n",
    "    # Texto truncado\n",
    "    text = result.get('text', '')\n",
    "    # if len(text) > 200:\n",
    "    #     text = text[:200] + \"...\"\n",
    "    print(f\"         üìù Texto: {text}\")\n",
    "    print(\"**********************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advance Retriever Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "def compress_documents_simple(documents: List[Document], query: str, llm) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Simple document compression using LangChain's native batch processing.\n",
    "    Each document is processed individually with the same prompt.\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    # Simple compression prompt for individual documents\n",
    "    compress_prompt_template = \"\"\"You are a philatelic expert. Extract and compress ONLY the information relevant to this query from the document below.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "DOCUMENT:\n",
    "{document}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Extract only information directly relevant to the query\n",
    "- Preserve exact catalog numbers, dates, denominations, and technical specifications\n",
    "- Keep Scott numbers, Michel numbers, and other catalog references intact\n",
    "- Maintain variety descriptions and error information\n",
    "- Remove irrelevant content but preserve context for understanding\n",
    "\n",
    "If the document contains no relevant information, respond with exactly: NO_RELEVANT_CONTENT\n",
    "\n",
    "COMPRESSED CONTENT:\"\"\"\n",
    "\n",
    "    # Create individual prompts for each document\n",
    "    prompts = []\n",
    "    for doc in documents:\n",
    "        prompt_text = compress_prompt_template.format(\n",
    "            query=query, \n",
    "            document=doc.page_content\n",
    "        )\n",
    "        prompts.append([(\"user\", prompt_text)])\n",
    "    \n",
    "    # Use LangChain's native batch processing with concurrency control\n",
    "    config = RunnableConfig(max_concurrency=10)  # Process 10 documents concurrently\n",
    "    \n",
    "    try:\n",
    "        responses = llm.batch(prompts, config=config)\n",
    "        \n",
    "        # Filter and create compressed documents\n",
    "        compressed_docs = []\n",
    "        for i, response in enumerate(responses):\n",
    "            content = response.content.strip() if hasattr(response, 'content') else str(response).strip()\n",
    "            \n",
    "            # Only include documents that have relevant content\n",
    "            if content and content != \"NO_RELEVANT_CONTENT\":\n",
    "                compressed_doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata=documents[i].metadata\n",
    "                )\n",
    "                compressed_docs.append(compressed_doc)\n",
    "        \n",
    "        return compressed_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch compression: {e}\")\n",
    "        # Fallback: return original documents\n",
    "        return documents\n",
    "\n",
    "def search_stamps_with_compression(query, client, embeddings, llm, limit=100, \n",
    "                                 alpha=0.30, diversity_lambda=0.75):\n",
    "    \"\"\"\n",
    "    Optimized philatelic search with simple batch document compression using LangChain's native batch processing.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The stamp query\n",
    "        client: Weaviate client\n",
    "        embeddings: Embedding model\n",
    "        llm: Language model\n",
    "        limit (int): Maximum documents to retrieve\n",
    "        alpha (float): Hybrid search factor (0.30 = 30% vector, 70% keywords)\n",
    "        diversity_lambda (float): MMR diversity factor (0.75 = good diversity)\n",
    "    \n",
    "    Returns:\n",
    "        list: Compressed and optimized documents for philatelic queries\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = WeaviateVectorStore(\n",
    "        client=client,\n",
    "        index_name=COLLECTION_NAME,\n",
    "        text_key=\"text\",\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Try to create hybrid retriever\n",
    "    hybrid_kwargs = {\"k\": limit // 2}\n",
    "    if alpha is not None:\n",
    "        hybrid_kwargs[\"alpha\"] = alpha\n",
    "    \n",
    "    # 1. Precision hybrid retriever (captures exact numbers + context)\n",
    "    precision_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs=hybrid_kwargs\n",
    "    )\n",
    "    \n",
    "    # 2. Diversity MMR retriever (avoids duplicate stamps)\n",
    "    diversity_retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": limit // 2, \"lambda_mult\": diversity_lambda}\n",
    "    )\n",
    "    \n",
    "    # 3. Ensemble with dual strategy\n",
    "    base_retriever = EnsembleRetriever(\n",
    "        retrievers=[precision_retriever, diversity_retriever],\n",
    "        weights=[0.7, 0.3]  # 70% precision + 30% diversity\n",
    "    )\n",
    "    \n",
    "    # Specialized prompt for philatelic multi-query generation\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are a specialized philatelic researcher expert in stamp catalogues and varieties.\n",
    "Generate 3 strategically different versions of the question to capture comprehensive stamp information:\n",
    "\n",
    "ORIGINAL: {question}\n",
    "\n",
    "Create variations that target:\n",
    "1. CATALOG PRECISION: Focus on exact catalog numbers, dates, and technical specifications\n",
    "2. CONTEXTUAL SEARCH: Include related series, printings, varieties, and historical context  \n",
    "3. TERMINOLOGY ALTERNATIVES: Use alternative philatelic terms and synonyms\n",
    "\n",
    "Consider these philatelic elements:\n",
    "- Catalog systems: Scott, Michel, Yvert, SG, local catalogs\n",
    "- Technical terms: definitive/commemorative, variety/error, overprint/surcharge\n",
    "- Time references: issue dates, printing dates, first day covers\n",
    "- Denominations: face values, colors, perforations\n",
    "\n",
    "Alternative searches:\n",
    "1.\n",
    "2. \n",
    "3.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # MultiQueryRetriever with specialized prompt\n",
    "    multi_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=base_retriever,\n",
    "        llm=llm,\n",
    "        prompt=query_prompt,\n",
    "        parser_key=\"lines\"\n",
    "    )\n",
    "    \n",
    "    # Execute initial retrieval\n",
    "    initial_results = multi_retriever.invoke(query)\n",
    "       \n",
    "    compression_llm = ChatOpenAI(\n",
    "            model=\"gpt-5-nano\", \n",
    "            api_key=OPENAI_API_KEY, \n",
    "            temperature=1,  # obligatorio para gpt-5-nano\n",
    "            timeout=120.0,\n",
    "            model_kwargs={\n",
    "                \"verbosity\": \"medium\",\n",
    "                \"reasoning_effort\" : \"low\"\n",
    "            })\n",
    "    \n",
    "    # Simple batch compression using LangChain's native batch processing\n",
    "    compressed_results = compress_documents_simple(initial_results, query, compression_llm)\n",
    "    \n",
    "    # Reorder by quality_score if it exists\n",
    "    def get_quality_score(doc):\n",
    "        return getattr(doc, 'metadata', {}).get('quality_score', 0.0)\n",
    "    \n",
    "    sorted_results = sorted(compressed_results, key=get_quality_score, reverse=True)\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the method search_stamps_with_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the optimized search_stamps_with_compression with batch processing\n",
    "# print(\"üß™ Testing optimized batch compression...\")\n",
    "\n",
    "# # Test query focused on specific stamps\n",
    "# test_query = \"Costa Rica 1907 2 colones stamp with original gum Scott 68\"\n",
    "\n",
    "# print(f\"üîç Query: {test_query}\")\n",
    "# print(\"‚è±Ô∏è Starting optimized search with batch compression...\")\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# try:\n",
    "#     compressed_docs = search_stamps_with_compression(\n",
    "#         query=test_query,\n",
    "#         client=client, \n",
    "#         embeddings=embeddings, \n",
    "#         limit=30,\n",
    "#         llm=llm,\n",
    "#         alpha=0.30,  # 30% vectorial, 70% keywords for exact numbers\n",
    "#         diversity_lambda=0.75  # 75% relevance, 25% diversity\n",
    "#     )\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     execution_time = end_time - start_time\n",
    "    \n",
    "#     print(f\"‚úÖ Batch compression completed in {execution_time:.2f} seconds\")\n",
    "#     print(f\"üìä Retrieved and compressed {len(compressed_docs)} documents\")\n",
    "    \n",
    "#     # Show sample results\n",
    "#     for i, doc in enumerate(compressed_docs[:3], 1):\n",
    "#         print(f\"\\\\nüìÑ Document {i}:\")\n",
    "#         print(f\"   Metadata: {getattr(doc, 'metadata', {})}\")\n",
    "#         content = getattr(doc, 'page_content', str(doc))\n",
    "#         preview = content[:200] + \"...\" if len(content) > 200 else content\n",
    "#         print(f\"   Content: {preview}\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error during batch compression test: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_info() -> str:\n",
    "    \"\"\"\n",
    "    Get collection information to display in the interface.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return \"‚ùå No Weaviate connection\"\n",
    "    \n",
    "    try:\n",
    "        stats = get_collection_stats(client, \"Oxcart\")\n",
    "        if stats:\n",
    "            info = f\"üìä **Oxcart Collection Statistics:**\\\\n\\\\n\"\n",
    "            info += f\"üì¶ **Total chunks:** {stats['total_chunks']:,}\\\\n\"\n",
    "            info += f\"üìÑ **Documents:** {stats['total_documents']}\\\\n\\\\n\"\n",
    "            \n",
    "            if stats.get('documents'):\n",
    "                info += \"**Indexed documents:**\\\\n\"\n",
    "                for doc_id, count in stats['documents'].items():\n",
    "                    info += f\"‚Ä¢ {doc_id}: {count:,} chunks\\\\n\"\n",
    "            \n",
    "            return info\n",
    "        else:\n",
    "            return \"‚ùå Could not retrieve statistics\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "print(\"‚úÖ RAG functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_collection_stats(client, \"Oxcart\")\n",
    "stats['total_documents']\n",
    "stats['total_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura que usan tus funciones de b√∫squeda/respuesta\n",
    "rag_system = {\n",
    "    \"success\": True,\n",
    "    \"client\": client,                    # para que search_and_answer pueda consultar\n",
    "    \"collection_name\": COLLECTION_NAME,  # nombre de la colecci√≥n\n",
    "    \"weaviate_url\": WEAVIATE_URL,        # info para la UI\n",
    "    \"total_documents\": stats['total_documents'],       # para mostrar estado\n",
    "    \"total_chunks\": stats['total_chunks'],        # opcional en la UI\n",
    "    \"embeddings\":embeddings,\n",
    "    \"llm\":llm,\n",
    "    # puedes a√±adir m√°s campos que tu search_and_answer necesite\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_answer_basic(\n",
    "    query: str,\n",
    "    rag_system: Dict[str, Any],\n",
    "    year_start: Optional[int] = None,\n",
    "    year_end: Optional[int] = None,\n",
    "    scott_numbers: Optional[List[str]] = None,\n",
    "    max_results: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Basic hybrid search approach with improved philatelic filters.\n",
    "    All filters are OPTIONAL - only applied when provided.\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    if not rag_system or not rag_system.get(\"client\"):\n",
    "        return {\n",
    "            \"answer\": \"‚ùå Error: No Weaviate connection\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\"error\": \"No Weaviate connection\"}\n",
    "        }\n",
    "    \n",
    "    client_wv = rag_system[\"client\"]\n",
    "    collection_name = rag_system.get(\"collection_name\", \"Oxcart\")\n",
    "    \n",
    "    # Build philatelic filters only if values are provided\n",
    "    filters = {}\n",
    "    \n",
    "    # Year range filter - ONLY if both years are provided and valid\n",
    "    if year_start is not None and year_end is not None:\n",
    "        try:\n",
    "            # Ensure both are integers and valid\n",
    "            start = int(year_start)\n",
    "            end = int(year_end)\n",
    "            # Ensure start <= end\n",
    "            if start > end:\n",
    "                start, end = end, start\n",
    "            filters[\"year_range\"] = (start, end)\n",
    "            print(f\"[DEBUG] Year filter applied: {start}-{end}\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"[WARNING] Invalid year values, skipping year filter: {e}\")\n",
    "    \n",
    "    # Scott numbers ONLY if provided and not empty\n",
    "    if scott_numbers:\n",
    "        print(\"Scott Numbers: \",scott_numbers)\n",
    "        filters[\"catalog_system\"] = \"Scott\"\n",
    "        filters[\"scott_numbers\"] = scott_numbers        \n",
    "    \n",
    "    # Log final filter status\n",
    "    if not filters:\n",
    "        print(\"[DEBUG] No filters applied - searching all documents\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] Filters being used: {filters}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Basic semantic search with philatelic filters\n",
    "        # Pass None if no filters, not empty dict\n",
    "        results = search_chunks_semantic(\n",
    "            client=client_wv,\n",
    "            query=query,\n",
    "            collection_name=collection_name,\n",
    "            limit=int(max_results),\n",
    "            filters=filters if filters else None,  # Pass None if no filters\n",
    "            mode=\"hybrid\",\n",
    "            alpha=0.35\n",
    "        )\n",
    "        \n",
    "        # Convert to LangChain document format for RAG\n",
    "        docs_for_rag = []\n",
    "        for r in results:\n",
    "            doc = type('Document', (), {\n",
    "                'page_content': r.get('text', ''),\n",
    "                'metadata': {\n",
    "                    'doc_id': r.get('doc_id', 'N/A'),\n",
    "                    'page_number': r.get('page_number', 'N/A'),\n",
    "                    'chunk_type': r.get('chunk_type', 'N/A'),\n",
    "                    'score': r.get('score', 0.0),\n",
    "                    'scott_numbers': r.get('scott_numbers', []),\n",
    "                    'years': r.get('years', []),\n",
    "                    'catalog_systems': r.get('catalog_systems', [])\n",
    "                }\n",
    "            })()\n",
    "            docs_for_rag.append(doc)\n",
    "        \n",
    "        # Generate RAG response using LangChain\n",
    "        rag_response = create_rag_response(docs_for_rag, query)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Build metadata with actual filters used\n",
    "        metadata = {\n",
    "            \"approach\": \"Basic Hybrid Search\",\n",
    "            \"query\": query,\n",
    "            \"total_results\": len(results),\n",
    "            \"max_results\": int(max_results),\n",
    "            \"filters_used\": filters if filters else \"None (searching all documents)\",\n",
    "            \"generation_time\": execution_time,\n",
    "            \"context_docs_count\": rag_response.get(\"context_docs_count\", len(docs_for_rag)),\n",
    "            \"context_length\": sum(len(d.page_content) for d in docs_for_rag),\n",
    "            \"token_usage\": rag_response.get(\"token_usage\", {}),\n",
    "            \"cost_info\": rag_response.get(\"cost_info\", {}),\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"answer\": rag_response.get(\"response\", \"No response generated\"),\n",
    "            \"results\": results,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        print(f\"[ERROR] Basic search error: {str(e)}\")\n",
    "        print(f\"[ERROR] Full traceback: {error_details}\")\n",
    "        print(f\"[ERROR] Filters attempted: {filters}\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": f\"‚ùå Basic search error: {str(e)}\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\n",
    "                \"error\": str(e),\n",
    "                \"generation_time\": 0,\n",
    "                \"filters_attempted\": filters if filters else \"None\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_answer_advanced(\n",
    "    query: str,\n",
    "    rag_system: Dict[str, Any],\n",
    "    max_results: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Advanced compression search approach - filters NOT applied (as requested).\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    if not rag_system or not rag_system.get(\"client\"):\n",
    "        return {\n",
    "            \"answer\": \"‚ùå Error: No Weaviate connection\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\"error\": \"No Weaviate connection\"}\n",
    "        }\n",
    "\n",
    "    client_wv = rag_system[\"client\"]\n",
    "    embeddings = rag_system.get(\"embeddings\")\n",
    "    llm = rag_system.get(\"llm\")\n",
    "    \n",
    "    # NOTE: Advanced search does not apply filters as requested by user\n",
    "    # This approach uses ensemble retrieval and compression instead\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Advanced search with compression (no filters applied)\n",
    "        compressed_docs = search_stamps_with_compression(\n",
    "            query=query,\n",
    "            client=client_wv,\n",
    "            embeddings=embeddings,\n",
    "            llm=llm,\n",
    "            limit=max_results,\n",
    "            alpha=0.30,\n",
    "            diversity_lambda=0.75\n",
    "        )\n",
    "        \n",
    "        # Generate RAG response using LangChain\n",
    "        rag_response = create_rag_response(compressed_docs, query)\n",
    "        \n",
    "        # Extraer y preservar figuras de los documentos originales\n",
    "        figure_pattern = r'(!\\[([^\\]]*)\\]\\([^)]+\\))'\n",
    "\n",
    "        for doc in compressed_docs:\n",
    "            # Buscar figuras en el contenido original si est√° disponible\n",
    "            original_content = doc.metadata.get('text_original', doc.page_content)\n",
    "            \n",
    "            # Extraer todas las figuras del contenido original\n",
    "            figures = re.findall(figure_pattern, original_content)\n",
    "            \n",
    "            # Eliminar duplicados manteniendo el orden\n",
    "            seen_figures = set()\n",
    "            unique_figures = []\n",
    "            for fig in figures:\n",
    "                # Usar el path de la imagen como identificador √∫nico (ignorando el alt text)\n",
    "                img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                if img_path:\n",
    "                    img_identifier = img_path.group(1)\n",
    "                    if img_identifier not in seen_figures:\n",
    "                        seen_figures.add(img_identifier)\n",
    "                        unique_figures.append(fig)\n",
    "            \n",
    "            # Verificar qu√© figuras ya est√°n en el contenido comprimido\n",
    "            existing_figures = set()\n",
    "            for fig in unique_figures:\n",
    "                if fig[0] in doc.page_content:\n",
    "                    img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                    if img_path:\n",
    "                        existing_figures.add(img_path.group(1))\n",
    "            \n",
    "            # Agregar solo las figuras que faltan\n",
    "            missing_figures = []\n",
    "            for fig in unique_figures:\n",
    "                img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                if img_path and img_path.group(1) not in existing_figures:\n",
    "                    missing_figures.append(fig[0])\n",
    "            \n",
    "            # Si hay figuras faltantes, agregarlas al final\n",
    "            if missing_figures:\n",
    "                figures_text = \"\\n\\n\" + \"\\n\".join(missing_figures)\n",
    "                doc.page_content = doc.page_content + figures_text\n",
    "            \n",
    "            # Guardar las figuras √∫nicas en metadata para acceso r√°pido\n",
    "            doc.metadata['figures'] = [fig[0] for fig in unique_figures] if unique_figures else []\n",
    "            doc.metadata['has_figures'] = len(unique_figures) > 0\n",
    "        \n",
    "        # Convert compressed docs to results format for display\n",
    "        results = []\n",
    "        for i, doc in enumerate(compressed_docs):\n",
    "            result = {\n",
    "                'doc_id': doc.metadata.get('doc_id', 'N/A'),\n",
    "                'page_number': doc.metadata.get('page_number', 'N/A'),\n",
    "                'chunk_type': doc.metadata.get('chunk_type', 'N/A'),\n",
    "                'text': doc.page_content,\n",
    "                'score': doc.metadata.get('quality_score', 0.0),\n",
    "                'catalog_systems': doc.metadata.get('catalog_systems', []),\n",
    "                'scott_numbers': doc.metadata.get('scott_numbers', []),\n",
    "                'years': doc.metadata.get('years', []),\n",
    "                'colors': doc.metadata.get('colors', []),\n",
    "                'variety_classes': doc.metadata.get('variety_classes', []),\n",
    "                'has_figures': doc.metadata.get('has_figures', False),  \n",
    "                'figures': doc.metadata.get('figures', [])  \n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        metadata = {\n",
    "            \"approach\": \"Advanced Compression Search\",\n",
    "            \"query\": query,\n",
    "            \"total_results\": len(results),\n",
    "            \"compressed_docs\": len(compressed_docs),\n",
    "            \"filters_used\": \"No filters (advanced approach)\",\n",
    "            \"generation_time\": execution_time,\n",
    "            \"context_docs_count\": rag_response[\"context_docs_count\"],\n",
    "            \"docs_with_figures\": sum(1 for r in results if r.get('has_figures', False)),\n",
    "            \"token_usage\": rag_response.get(\"token_usage\", {}),\n",
    "            \"cost_info\": rag_response.get(\"cost_info\", {}),\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"answer\": rag_response.get(\"response\", \"No response generated\"),\n",
    "            \"results\": results,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"‚ùå Advanced search error: {str(e)}\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\n",
    "                \"error\": str(e),\n",
    "                \"generation_time\": 0,\n",
    "                \"filters_attempted\": \"None\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\"\n",
    "\n",
    "# results = search_and_answer_basic(\n",
    "#     query,\n",
    "#     rag_system,\n",
    "#     None,\n",
    "#     None,\n",
    "#     [\"1\",\"2\"],\n",
    "#     10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Basic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced test of search_chunks_semantic function\n",
    "# def display_search_results(results, query, filters_used=None):\n",
    "#     \"\"\"\n",
    "#     Enhanced display function for search results\n",
    "#     \"\"\"\n",
    "#     print(f\"üîç SEARCH RESULTS\")\n",
    "#     print(f\"=\" * 60)\n",
    "#     print(f\"üìù Query: '{query}'\")\n",
    "#     if filters_used:\n",
    "#         print(f\"üîß Filters applied: {filters_used}\")\n",
    "#     print(f\"üìä Total results: {len(results)}\")\n",
    "#     print(f\"=\" * 60)\n",
    "    \n",
    "#     if not results:\n",
    "#         print(\"‚ùå No results found\")\n",
    "#         return\n",
    "    \n",
    "#     for j, result in enumerate(results[:5], 1):  # Show top 5 results\n",
    "#         print(f\"\\nüè∑Ô∏è RESULT #{j} (Score: {result['score']:.4f})\")\n",
    "#         print(f\"   üìÑ Document: {result['doc_id']}\")\n",
    "#         print(f\"   üìã Chunk Type: {result['chunk_type']}\")\n",
    "#         print(f\"   üìÑ Page: {result['page_number']}\")\n",
    "        \n",
    "#         # Show metadata if available\n",
    "#         metadata_items = [\n",
    "#             ('üìñ Catalog Systems', result.get('catalog_systems', [])),\n",
    "#             ('üî¢ Scott Numbers', result.get('scott_numbers', [])),\n",
    "#             ('üìÖ Years', result.get('years', [])),\n",
    "#             ('üé® Colors', result.get('colors', [])),\n",
    "#             ('üîÄ Variety Classes', result.get('variety_classes', [])),\n",
    "#         ]\n",
    "        \n",
    "#         for label, data in metadata_items:\n",
    "#             if data:\n",
    "#                 display_data = ', '.join(str(item) for item in data) if isinstance(data, list) else str(data)\n",
    "#                 print(f\"   {label}: {display_data}\")\n",
    "        \n",
    "#         # Boolean flags\n",
    "#         if result.get('has_varieties'):\n",
    "#             print(f\"   ‚úÖ Has varieties\")\n",
    "#         if result.get('is_guanacaste'):\n",
    "#             print(f\"   üåé Guanacaste province\")\n",
    "#         if result.get('has_technical_specs'):\n",
    "#             print(f\"   üîß Has technical specs\")\n",
    "            \n",
    "#         # Text preview\n",
    "#         text = result.get('text', '')\n",
    "#         preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "#         print(f\"   üìù Text preview: {preview}\")\n",
    "#         print(f\"   {'‚îÄ' * 50}\")\n",
    "\n",
    "# # Test 1: Basic search without filters (original test enhanced)\n",
    "# print(\"üß™ TEST 1: Basic Hybrid Search (No Filters)\")\n",
    "# query = \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\"\n",
    "\n",
    "# results = search_chunks_semantic(\n",
    "#     client=client, \n",
    "#     query=query, \n",
    "#     collection_name=\"Oxcart\", \n",
    "#     limit=20,\n",
    "#     filters=[],  # No filters\n",
    "#     mode=\"hybrid\",\n",
    "#     alpha=0.35\n",
    "# )\n",
    "\n",
    "# display_search_results(results, query)\n",
    "\n",
    "# print(f\"\\nüí° This test shows unfiltered results. Now let's test with specific filters...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test: Combined Filters (Advanced Testing) - UPDATED with Multiple Scott Numbers\n",
    "# print(\"üß™ TEST 6: Combined Filters (Advanced Testing)\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # Test complex filter combinations for precise searches\n",
    "# combined_tests = [\n",
    "#     {\n",
    "#         \"name\": \"1907 stamps with varieties\",\n",
    "#         \"query\": \"1907 Costa Rica stamps with varieties or errors\",\n",
    "#         \"filters\": {\n",
    "#             \"year_range\": (1907, 1907)\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"1934 Costa Rica stamps\",\n",
    "#         \"query\": \"List all 1934 Costa Rica Stamps\",\n",
    "#         \"filters\": {\n",
    "#             \"year_range\": (1934, 1934)\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Costa Rica First Issue Scott 1-5 (MULTIPLE SCOTT NUMBERS TEST)\",\n",
    "#         \"query\": \"Costa Rica First Issue Scott 1 2 3 4 5\",\n",
    "#         \"filters\": {\n",
    "#             \"catalog_system\": \"Scott\",\n",
    "#             \"scott_numbers\": [\"1\", \"2\", \"3\", \"4\", \"5\"]  # TEST: Multiple Scott numbers as list\n",
    "#         }\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# for i, test in enumerate(combined_tests, 1):\n",
    "#     print(f\"\\nüî¨ COMBINED TEST {i}: {test['name']}\")\n",
    "#     print(f\"{'‚îÄ' * 60}\")\n",
    "#     print(f\"üéØ Filters: {test['filters']}\")\n",
    "    \n",
    "#     # Special logging for multiple Scott numbers test\n",
    "#     if \"scott_number\" in test['filters'] and isinstance(test['filters']['scott_number'], list):\n",
    "#         print(f\"üî¢ TESTING MULTIPLE SCOTT NUMBERS: {test['filters']['scott_number']}\")\n",
    "#         print(f\"üìù Expected: Should find documents with ANY of these Scott numbers (OR logic)\")\n",
    "    \n",
    "#     # Execute search with combined filters\n",
    "#     results = search_chunks_semantic(\n",
    "#         client=client,\n",
    "#         query=test['query'],\n",
    "#         collection_name=\"Oxcart\",\n",
    "#         limit=15,  # Increased limit for multiple Scott test\n",
    "#         filters=test['filters'],\n",
    "#         mode=\"hybrid\",\n",
    "#         alpha=0.35\n",
    "#     )\n",
    "    \n",
    "#     display_search_results(results, test['query'], filters_used=test['filters'])\n",
    "    \n",
    "#     # Detailed validation of filter application\n",
    "#     if results:\n",
    "#         print(f\"\\n   üîç FILTER VALIDATION:\")\n",
    "#         for filter_key, filter_value in test['filters'].items():\n",
    "#             validation_count = 0\n",
    "            \n",
    "#             # Special handling for multiple Scott numbers\n",
    "#             if filter_key == \"scott_numbers\" and isinstance(filter_value, list):\n",
    "#                 print(f\"      üî¢ Checking for ANY Scott number from: {filter_value}\")\n",
    "#                 for result in results:\n",
    "#                     result_scotts = result.get('scott_numbers', [])\n",
    "#                     # Check if ANY of the requested Scott numbers is in the result\n",
    "#                     if any(scott_num in result_scotts for scott_num in filter_value):\n",
    "#                         validation_count += 1\n",
    "#                 print(f\"      ‚úÖ Documents with ANY requested Scott number: {validation_count}/{len(results)}\")\n",
    "                \n",
    "#                 # Show which specific Scott numbers were found\n",
    "#                 found_scotts = set()\n",
    "#                 for result in results:\n",
    "#                     found_scotts.update(result.get('scott_numbers', []))\n",
    "#                 matching_scotts = [s for s in filter_value if s in found_scotts]\n",
    "#                 print(f\"      üìã Requested Scott numbers found: {matching_scotts}\")\n",
    "#                 print(f\"      üìã All Scott numbers in results: {sorted(found_scotts)}\")\n",
    "                \n",
    "#             elif filter_key == \"year_range\":\n",
    "#                 result_years = result.get('years', [])\n",
    "#                 if any(filter_value[0] <= year <= filter_value[1] for year in result_years):\n",
    "#                     validation_count += 1\n",
    "#             elif filter_key == \"catalog_system\":\n",
    "#                 if filter_value in result.get('catalog_systems', []):\n",
    "#                     validation_count += 1\n",
    "#             elif filter_key == \"chunk_type\":\n",
    "#                 if result.get('chunk_type') == filter_value:\n",
    "#                     validation_count += 1\n",
    "#             elif filter_key in [\"has_varieties\", \"is_guanacaste\", \"has_technical_specs\"]:\n",
    "#                 if result.get(filter_key) == filter_value:\n",
    "#                     validation_count += 1\n",
    "            \n",
    "#             # Show validation for non-Scott filters\n",
    "#             if filter_key != \"scott_number\":\n",
    "#                 print(f\"      ‚úÖ {filter_key}: {validation_count}/{len(results)} results match\")\n",
    "    \n",
    "#     print(f\"\\n{'‚ïê' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import markdown\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "def create_gradio_interface(rag_system: Dict[str, Any]) -> gr.Blocks:\n",
    "    \"\"\"\n",
    "    Creates the Gradio interface with improved philatelic filters.\n",
    "    Uses HTML for markdown content and Textbox for timing display.\n",
    "    Includes token usage and cost metrics display.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Funci√≥n para convertir Markdown a HTML con correcci√≥n de rutas de im√°genes\n",
    "    \n",
    "    def markdown_to_html(text):\n",
    "        \"\"\"Convert markdown text to HTML with lazy base64 loading\"\"\"\n",
    "        if not text:\n",
    "            return \"<p><em>No content</em></p>\"\n",
    "        \n",
    "        import re\n",
    "        import os\n",
    "        import base64\n",
    "        \n",
    "        base_path = r\"C:\\Users\\VM-SERVER\\Desktop\\Oxcart RAG\\results\\markdown\\figures\"\n",
    "        \n",
    "        def image_to_base64_lazy(match):\n",
    "            alt_text = match.group(1)\n",
    "            filename = match.group(2).split('/')[-1].split('\\\\')[-1]\n",
    "            full_path = os.path.join(base_path, filename)\n",
    "            \n",
    "            if os.path.exists(full_path):\n",
    "                try:\n",
    "                    with open(full_path, \"rb\") as img_file:\n",
    "                        b64_string = base64.b64encode(img_file.read()).decode()\n",
    "                        ext = filename.split('.')[-1].lower()\n",
    "                        mime_type = f\"image/{ext}\" if ext != 'jpg' else \"image/jpeg\"\n",
    "                        return f'<img style=\"max-width: 100%; height: auto; display: block; margin: 10px auto; border: 1px solid #ddd; border-radius: 4px;\" alt=\"{alt_text}\" src=\"data:{mime_type};base64,{b64_string}\" />'\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {filename}: {e}\")\n",
    "                    return f'<p>[Image not found: {filename}]</p>'\n",
    "            else:\n",
    "                return f'<p>[Image not found: {filename}]</p>'\n",
    "        \n",
    "        # Primero convertir markdown a HTML\n",
    "        html = markdown.markdown(text, extensions=['tables', 'fenced_code'])\n",
    "        \n",
    "        # Luego reemplazar las im√°genes en el HTML\n",
    "        html = re.sub(\n",
    "            r'<img[^>]*alt=\"([^\"]*)\"[^>]*src=\"[^\"]*?([^/\\\\\">]+\\.(?:png|jpg|jpeg|gif))\"[^>]*>',\n",
    "            image_to_base64_lazy,\n",
    "            html\n",
    "        )\n",
    "        \n",
    "        return html\n",
    "\n",
    "    def gradio_sequential_search(query, year_start, year_end, scott_numbers, max_results):\n",
    "        \"\"\"\n",
    "        Sequential search with improved philatelic filters.\n",
    "        Now with OPTIONAL filters - only used if provided by user.\n",
    "        \"\"\"\n",
    "        if not rag_system:\n",
    "            error_msg = \"‚ùå RAG system not configured\"\n",
    "            error_html = markdown_to_html(error_msg)\n",
    "            yield error_html, \"\", \"\", error_html, \"\", \"\", \"No timing data available\"\n",
    "            return\n",
    "            \n",
    "        if not query or not query.strip():\n",
    "            error_msg = \"‚ùå Please enter a query\"\n",
    "            error_html = markdown_to_html(error_msg)\n",
    "            yield error_html, \"\", \"\", error_html, \"\", \"\", \"No timing data available\"\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            start_total_time = time.time()\n",
    "            \n",
    "            # Process OPTIONAL filters - convert empty strings to None\n",
    "            # Years: only use if both are provided and valid\n",
    "            processed_year_start = None\n",
    "            processed_year_end = None\n",
    "            \n",
    "            # Check if years are provided as strings and convert\n",
    "            if year_start and year_end:\n",
    "                try:\n",
    "                    # Strip whitespace and check if not empty\n",
    "                    year_start_str = str(year_start).strip()\n",
    "                    year_end_str = str(year_end).strip()\n",
    "                    \n",
    "                    if year_start_str and year_end_str:\n",
    "                        # Try to convert to integers\n",
    "                        year_start_int = int(year_start_str)\n",
    "                        year_end_int = int(year_end_str)\n",
    "                        \n",
    "                        # Validate year range\n",
    "                        if 1800 <= year_start_int <= 2025 and 1800 <= year_end_int <= 2025:\n",
    "                            processed_year_start = year_start_int\n",
    "                            processed_year_end = year_end_int\n",
    "                        else:\n",
    "                            print(f\"[WARNING] Years out of valid range (1800-2025): {year_start_int}-{year_end_int}\")\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"[WARNING] Could not parse years: {year_start}, {year_end} - {e}\")\n",
    "                    # Years are invalid, will proceed without year filter\n",
    "            \n",
    "            # Scott numbers: only use if provided and not empty\n",
    "            processed_scott_numbers = None\n",
    "            if scott_numbers and scott_numbers.strip():\n",
    "                # Clean and split the scott numbers\n",
    "                processed_scott_numbers = [s.strip() for s in scott_numbers.split(',') if s.strip()]\n",
    "                # If list is empty after cleaning, set to None\n",
    "                if not processed_scott_numbers:\n",
    "                    processed_scott_numbers = None\n",
    "            \n",
    "            # Log what filters are being used\n",
    "            filters_status = []\n",
    "            if processed_year_start and processed_year_end:\n",
    "                filters_status.append(f\"Years: {processed_year_start}-{processed_year_end}\")\n",
    "            if processed_scott_numbers:\n",
    "                filters_status.append(f\"Scott: {', '.join(processed_scott_numbers)}\")\n",
    "            \n",
    "            filter_msg = \"Filters applied: \" + (\", \".join(filters_status) if filters_status else \"None (searching all documents)\")\n",
    "            print(f\"[DEBUG] {filter_msg}\")\n",
    "            \n",
    "            # ============= STEP 1: Execute Basic Search =============\n",
    "            # Mensajes de estado inicial\n",
    "            loading_basic = markdown_to_html(f\"*üîÑ Running Basic Hybrid Search...*\\n\\n{filter_msg}\")\n",
    "            loading_advanced = markdown_to_html(\"*‚è≥ Waiting for Basic search to complete...*\")\n",
    "            \n",
    "            yield (\n",
    "                loading_basic, \n",
    "                \"\", \n",
    "                \"\", \n",
    "                loading_advanced, \n",
    "                \"\", \n",
    "                \"\", \n",
    "                f\"‚è±Ô∏è Basic search in progress...\\n{filter_msg}\"\n",
    "            )\n",
    "            \n",
    "            # Call basic search function with optional filters\n",
    "            basic_results_data = search_and_answer_basic(\n",
    "                query=query,\n",
    "                rag_system=rag_system,\n",
    "                year_start=processed_year_start,  # Pass None if not provided\n",
    "                year_end=processed_year_end,      # Pass None if not provided\n",
    "                scott_numbers=processed_scott_numbers,  # Pass None if not provided\n",
    "                max_results=int(max_results),\n",
    "            )\n",
    "            \n",
    "            # Format Basic Results\n",
    "            basic_answer = basic_results_data[\"answer\"]  # Ya viene en Markdown\n",
    "            basic_answer_html = markdown_to_html(basic_answer)\n",
    "            \n",
    "            basic_results = basic_results_data[\"results\"]\n",
    "            basic_metadata = basic_results_data[\"metadata\"]\n",
    "            basic_execution_time = basic_metadata.get(\"generation_time\", 0)\n",
    "            \n",
    "            basic_search_output = format_search_results(basic_results, \"Basic Hybrid Search\")\n",
    "            basic_search_html = markdown_to_html(basic_search_output)\n",
    "            \n",
    "            basic_metadata_output = format_metadata(basic_metadata, basic_execution_time)\n",
    "            basic_metadata_html = markdown_to_html(basic_metadata_output)\n",
    "            \n",
    "            # Timing parcial\n",
    "            timing_partial = f\"\"\"‚è±Ô∏è EXECUTION TIMING (Partial)\n",
    "            \n",
    "Basic Hybrid Search: ‚úÖ COMPLETED\n",
    "‚Ä¢ Time: {basic_execution_time:.2f}s\n",
    "‚Ä¢ Results: {len(basic_results)}\n",
    "‚Ä¢ {filter_msg}\n",
    "\n",
    "Advanced Search: ‚è≥ STARTING...\n",
    "\"\"\"\n",
    "            \n",
    "            loading_advanced_2 = markdown_to_html(f\"*üîÑ Starting Advanced Compression Search...*\\n\\n{filter_msg}\")\n",
    "            \n",
    "            yield (\n",
    "                basic_answer_html,\n",
    "                basic_search_html,\n",
    "                basic_metadata_html,\n",
    "                loading_advanced_2,\n",
    "                \"\",\n",
    "                \"\",\n",
    "                timing_partial\n",
    "            )\n",
    "            \n",
    "            # ============= STEP 2: Execute Advanced Search =============\n",
    "            advanced_results_data = search_and_answer_advanced(\n",
    "                query=query,\n",
    "                rag_system=rag_system,\n",
    "                max_results=int(max_results),\n",
    "            )\n",
    "            \n",
    "            # Format Advanced Results\n",
    "            advanced_answer = advanced_results_data[\"answer\"]  # Ya viene en Markdown\n",
    "            advanced_answer_html = markdown_to_html(advanced_answer)\n",
    "            \n",
    "            advanced_results = advanced_results_data[\"results\"]\n",
    "            advanced_metadata = advanced_results_data[\"metadata\"]\n",
    "            advanced_execution_time = advanced_metadata.get(\"generation_time\", 0)\n",
    "            \n",
    "            advanced_search_output = format_search_results(advanced_results, \"Advanced Compression Search\")\n",
    "            advanced_search_html = markdown_to_html(advanced_search_output)\n",
    "            \n",
    "            advanced_metadata_output = format_metadata(advanced_metadata, advanced_execution_time)\n",
    "            advanced_metadata_html = markdown_to_html(advanced_metadata_output)\n",
    "            \n",
    "            # Calculate total execution time\n",
    "            total_execution_time = time.time() - start_total_time\n",
    "            \n",
    "            # Final timing information WITH METADATA for costs\n",
    "            timing_final = format_timing_display(\n",
    "                basic_execution_time,\n",
    "                advanced_execution_time,\n",
    "                total_execution_time,\n",
    "                len(basic_results),\n",
    "                len(advanced_results),\n",
    "                filter_msg,\n",
    "                basic_metadata,    # Pass the full metadata\n",
    "                advanced_metadata  # Pass the full metadata\n",
    "            )\n",
    "            \n",
    "            # Yield final complete results\n",
    "            yield (\n",
    "                basic_answer_html,\n",
    "                basic_search_html,\n",
    "                basic_metadata_html,\n",
    "                advanced_answer_html,\n",
    "                advanced_search_html,\n",
    "                advanced_metadata_html,\n",
    "                timing_final\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"‚ùå Error during search: {str(e)}\"\n",
    "            error_html = markdown_to_html(error_msg)\n",
    "            yield error_html, \"\", \"\", error_html, \"\", \"\", f\"‚ùå Error occurred - no timing data\"\n",
    "\n",
    "    def format_search_results(results, approach_name):\n",
    "        \"\"\"Format search results for display in Markdown with figure handling\"\"\"\n",
    "        if not results:\n",
    "            return f\"*No results found with {approach_name}*\"\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"### {approach_name} Results\")\n",
    "        lines.append(f\"**Found {len(results)} documents**\\n\")\n",
    "        lines.append(\"---\")\n",
    "        \n",
    "        for i, r in enumerate(results):\n",
    "            doc_id = r.get(\"doc_id\", \"N/A\")\n",
    "            chunk_type_val = r.get(\"chunk_type\", \"N/A\")\n",
    "            page_number = r.get(\"page_number\", \"N/A\")\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            catalogs = r.get(\"catalog_systems\", [])\n",
    "            scotts = r.get(\"scott_numbers\", [])\n",
    "            years = r.get(\"years\", [])\n",
    "\n",
    "            # Get full text (including figures)\n",
    "            text = r.get(\"text\", \"\")\n",
    "            \n",
    "            # Check if text contains figures\n",
    "            has_figures = \"![Figure]\" in text or \"![\" in text\n",
    "            \n",
    "            # Extract just the text preview (without figures)\n",
    "            import re\n",
    "            text_without_figures = re.sub(r'!\\[([^\\]]*)\\]\\([^)]+\\)', '', text).strip()\n",
    "            preview = (text_without_figures[:300] + \"...\") if len(text_without_figures) > 300 else text_without_figures\n",
    "            \n",
    "            # Extract all figure references\n",
    "            figure_pattern = r'(!\\[([^\\]]*)\\]\\([^)]+\\))'\n",
    "            figures = re.findall(figure_pattern, text)\n",
    "\n",
    "            lines.append(f\"\\n#### üìÑ Result {i+1}\")\n",
    "            lines.append(f\"**Score:** `{score:.3f}`\")\n",
    "            \n",
    "            if has_figures:\n",
    "                lines.append(\"üñºÔ∏è **This result contains figures**\\n\")\n",
    "            \n",
    "            # Create a table for metadata\n",
    "            lines.append(\"| Field | Value |\")\n",
    "            lines.append(\"|-------|-------|\")\n",
    "            lines.append(f\"| Document | `{doc_id}` |\")\n",
    "            lines.append(f\"| Type | {chunk_type_val} |\")\n",
    "            lines.append(f\"| Page | {page_number} |\")\n",
    "            \n",
    "            if catalogs:\n",
    "                lines.append(f\"| Catalogs | {', '.join(catalogs)} |\")\n",
    "            if scotts:\n",
    "                lines.append(f\"| Scott Numbers | **{', '.join(scotts)}** |\")\n",
    "            if years:\n",
    "                lines.append(f\"| Years | {', '.join(str(y) for y in years)} |\")\n",
    "            \n",
    "            # Always show preview\n",
    "            lines.append(f\"\\n**Preview:**\")\n",
    "            lines.append(f\"> {preview}\")\n",
    "            \n",
    "            # Always show figures if they exist\n",
    "            if has_figures and figures:\n",
    "                lines.append(f\"\\n**Figures in this result:**\\n\")\n",
    "                for figure_match in figures:\n",
    "                    lines.append(figure_match[0])  # Add the complete figure markdown\n",
    "            \n",
    "            lines.append(\"\\n---\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def format_metadata(metadata, execution_time):\n",
    "        \"\"\"Format metadata for display in Markdown including token usage and costs (robust casting).\"\"\"\n",
    "        if not metadata:\n",
    "            return \"*No metadata available*\"\n",
    "\n",
    "        # Helpers\n",
    "        def as_float(x, default=None):\n",
    "            try:\n",
    "                return float(x)\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        def as_int(x, default=None):\n",
    "            try:\n",
    "                return int(float(x))\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        out = []\n",
    "        out.append(\"### Search Metadata\\n\")\n",
    "\n",
    "        # Basic info\n",
    "        out.append(\"#### üìä Search Information\")\n",
    "        out.append(\"| Property | Value |\")\n",
    "        out.append(\"|----------|-------|\")\n",
    "        out.append(f\"| **Approach** | {metadata.get('approach', 'Unknown')} |\")\n",
    "        query_val = str(metadata.get('query', 'N/A')).replace(\"\\n\", \" \")\n",
    "        # (opcional) escapar pipes para no romper la tabla\n",
    "        query_val = query_val.replace(\"|\", \"\\\\|\")\n",
    "        out.append(f\"| **Query** | `{query_val}` |\")\n",
    "\n",
    "        total_results = as_int(metadata.get('total_results'), 0)\n",
    "        out.append(f\"| **Results found** | {total_results} |\")\n",
    "\n",
    "        context_docs_count = as_int(metadata.get('context_docs_count'))\n",
    "        out.append(f\"| **Context docs** | {context_docs_count if context_docs_count is not None else 'N/A'} |\")\n",
    "\n",
    "        context_length = as_int(metadata.get('context_length'))\n",
    "        out.append(\n",
    "            f\"| **Context length** | {context_length:,} chars |\"\n",
    "            if context_length is not None else\n",
    "            \"| **Context length** | N/A |\"\n",
    "        )\n",
    "\n",
    "        if metadata.get('filters_used'):\n",
    "            filters_str = str(metadata['filters_used']).replace('{', '').replace('}', '')\n",
    "            filters_str = filters_str.replace(\"|\", \"\\\\|\")\n",
    "            out.append(f\"| **Filters** | `{filters_str}` |\")\n",
    "\n",
    "        if 'compressed_docs' in metadata:\n",
    "            out.append(f\"| **Compressed docs** | {metadata['compressed_docs']} |\")\n",
    "\n",
    "        # Token usage\n",
    "        token_usage = metadata.get('token_usage') or {}\n",
    "        if token_usage:\n",
    "            in_tok  = as_int(token_usage.get('input_tokens'), 0)\n",
    "            out_tok = as_int(token_usage.get('output_tokens'), 0)\n",
    "            tot_tok = as_int(token_usage.get('total_tokens'), (in_tok or 0) + (out_tok or 0))\n",
    "\n",
    "            out.append(\"\\n#### üéØ Token Usage\")\n",
    "            out.append(\"| Token Type | Count |\")\n",
    "            out.append(\"|------------|-------|\")\n",
    "            out.append(f\"| **Input tokens** | {in_tok:,} |\")\n",
    "            out.append(f\"| **Output tokens** | {out_tok:,} |\")\n",
    "            out.append(f\"| **Total tokens** | {tot_tok:,} |\")\n",
    "\n",
    "        # Cost info\n",
    "        cost_info = metadata.get('cost_info') or {}\n",
    "        if cost_info:\n",
    "            in_cost  = as_float(cost_info.get('input_cost'), 0.0)\n",
    "            out_cost = as_float(cost_info.get('output_cost'), 0.0)\n",
    "            est_cost = as_float(cost_info.get('estimated_cost_usd'), (in_cost or 0.0) + (out_cost or 0.0))\n",
    "\n",
    "            out.append(\"\\n#### üí∞ Cost Analysis\")\n",
    "            out.append(\"| Cost Component | USD |\")\n",
    "            out.append(\"|----------------|-----|\")\n",
    "            out.append(f\"| **Input cost** | ${in_cost:.6f} |\")\n",
    "            out.append(f\"| **Output cost** | ${out_cost:.6f} |\")\n",
    "            out.append(f\"| **Total cost** | **${est_cost:.6f}** |\")\n",
    "\n",
    "        # Performance\n",
    "        out.append(\"\\n#### ‚è±Ô∏è Performance\")\n",
    "        out.append(\"| Metric | Time |\")\n",
    "        out.append(\"|--------|------|\")\n",
    "\n",
    "        gen_time = as_float(metadata.get('generation_time'))\n",
    "        exec_time = as_float(execution_time)\n",
    "\n",
    "        out.append(\n",
    "            f\"| **Generation time** | {gen_time:.2f} seconds |\"\n",
    "            if gen_time is not None else\n",
    "            \"| **Generation time** | N/A |\"\n",
    "        )\n",
    "        out.append(\n",
    "            f\"| **Total execution** | {exec_time:.2f} seconds |\"\n",
    "            if exec_time is not None else\n",
    "            \"| **Total execution** | N/A |\"\n",
    "        )\n",
    "\n",
    "        if token_usage and gen_time and gen_time > 0:\n",
    "            # usa out_tok ya casteado arriba; si no hay token_usage, se salta este bloque\n",
    "            tps = (as_int(token_usage.get('output_tokens'), 0) or 0) / gen_time\n",
    "            out.append(f\"| **Generation speed** | {tps:.1f} tokens/sec |\")\n",
    "\n",
    "        if metadata.get('error'):\n",
    "            err = str(metadata['error']).replace(\"`\", \"'\")\n",
    "            out.append(f\"\\n‚ö†Ô∏è **Error:** `{err}`\")\n",
    "\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "    def format_timing_display(\n",
    "        basic_time, advanced_time, total_time,\n",
    "        basic_results, advanced_results,\n",
    "        filter_msg=\"\",\n",
    "        basic_metadata=None, advanced_metadata=None\n",
    "    ):\n",
    "        \"\"\"Enhanced timing display with cost comparison (robust casting)\"\"\"\n",
    "        try:\n",
    "            # --- helpers seguros ---\n",
    "            def as_float(x, default=0.0):\n",
    "                try:\n",
    "                    return float(x)\n",
    "                except (TypeError, ValueError):\n",
    "                    return default\n",
    "\n",
    "            def as_int(x, default=0):\n",
    "                try:\n",
    "                    # evita ints tipo '1_234' si viniera as√≠\n",
    "                    return int(float(x))\n",
    "                except (TypeError, ValueError):\n",
    "                    return default\n",
    "\n",
    "            # tiempos\n",
    "            basic_time = as_float(basic_time)\n",
    "            advanced_time = as_float(advanced_time)\n",
    "            total_time = as_float(total_time)\n",
    "\n",
    "            # costos\n",
    "            basic_cost = 0.0\n",
    "            advanced_cost = 0.0\n",
    "            if basic_metadata and 'cost_info' in basic_metadata:\n",
    "                basic_cost = as_float(basic_metadata['cost_info'].get('estimated_cost_usd', 0))\n",
    "            if advanced_metadata and 'cost_info' in advanced_metadata:\n",
    "                advanced_cost = as_float(advanced_metadata['cost_info'].get('estimated_cost_usd', 0))\n",
    "            total_cost = basic_cost + advanced_cost\n",
    "\n",
    "            # tokens (¬°forzar int!)\n",
    "            basic_tokens = 0\n",
    "            advanced_tokens = 0\n",
    "            if basic_metadata and 'token_usage' in basic_metadata:\n",
    "                basic_tokens = as_int(basic_metadata['token_usage'].get('total_tokens', 0))\n",
    "            if advanced_metadata and 'token_usage' in advanced_metadata:\n",
    "                advanced_tokens = as_int(advanced_metadata['token_usage'].get('total_tokens', 0))\n",
    "            total_tokens = basic_tokens + advanced_tokens\n",
    "\n",
    "            # faster\n",
    "            if basic_time > 0 and advanced_time > 0:\n",
    "                if basic_time < advanced_time:\n",
    "                    faster = f\"üèÜ Basic search was {advanced_time/basic_time:.1f}x faster\"\n",
    "                elif advanced_time < basic_time:\n",
    "                    faster = f\"üèÜ Advanced search was {basic_time/advanced_time:.1f}x faster\"\n",
    "                else:\n",
    "                    faster = \"‚ö° Both approaches took similar time\"\n",
    "            else:\n",
    "                faster = \"‚è±Ô∏è Timing comparison not available\"\n",
    "\n",
    "            # cost effectiveness\n",
    "            cost_comparison = \"\"\n",
    "            if basic_cost > 0 and advanced_cost > 0:\n",
    "                if basic_cost < advanced_cost:\n",
    "                    cost_comparison = f\"üíµ Basic search was ${advanced_cost - basic_cost:.6f} cheaper\"\n",
    "                elif advanced_cost < basic_cost:\n",
    "                    cost_comparison = f\"üíµ Advanced search was ${basic_cost - advanced_cost:.6f} cheaper\"\n",
    "                else:\n",
    "                    cost_comparison = \"üíµ Both approaches had similar costs\"\n",
    "\n",
    "            # speeds\n",
    "            basic_speed = f\"{basic_results/basic_time:.1f}\" if basic_time > 0 else \"N/A\"\n",
    "            advanced_speed = f\"{advanced_results/advanced_time:.1f}\" if advanced_time > 0 else \"N/A\"\n",
    "\n",
    "            timing_display = f\"\"\"‚è±Ô∏è EXECUTION TIMING & COST COMPARISON\n",
    "                ================================================\n",
    "\n",
    "                üìã SEARCH CONFIGURATION\n",
    "                ‚Ä¢ {filter_msg}\n",
    "\n",
    "                üîç BASIC HYBRID SEARCH\n",
    "                ‚Ä¢ Processing Time: {basic_time:.2f} seconds\n",
    "                ‚Ä¢ Documents Found: {basic_results}\n",
    "                ‚Ä¢ Speed: {basic_speed} docs/sec\n",
    "                ‚Ä¢ Tokens Used: {basic_tokens:,}\n",
    "                ‚Ä¢ Cost: ${basic_cost:.6f}\n",
    "                ‚Ä¢ Status: ‚úÖ Complete\n",
    "\n",
    "                üöÄ ADVANCED COMPRESSION SEARCH\n",
    "                ‚Ä¢ Processing Time: {advanced_time:.2f} seconds\n",
    "                ‚Ä¢ Documents Found: {advanced_results}\n",
    "                ‚Ä¢ Speed: {advanced_speed} docs/sec\n",
    "                ‚Ä¢ Tokens Used: {advanced_tokens:,}\n",
    "                ‚Ä¢ Cost: ${advanced_cost:.6f}\n",
    "                ‚Ä¢ Status: ‚úÖ Complete\n",
    "\n",
    "                üìä OVERALL PERFORMANCE\n",
    "                ‚Ä¢ Total Execution: {total_time:.2f} seconds\n",
    "                ‚Ä¢ Total Tokens: {total_tokens:,}\n",
    "                ‚Ä¢ Total Cost: ${total_cost:.6f}\n",
    "                ‚Ä¢ Execution Mode: Sequential (Basic ‚Üí Advanced)\n",
    "                ‚Ä¢ {faster}\n",
    "                ‚Ä¢ {cost_comparison}\n",
    "\n",
    "                üí° PERFORMANCE NOTES:\n",
    "                ‚Ä¢ Basic search: Fast initial results, lower cost\n",
    "                ‚Ä¢ Advanced search: Enhanced quality, higher token usage\n",
    "                ‚Ä¢ Costs shown are for GPT-5-Nano model\n",
    "                ‚Ä¢ Sequential execution allows progressive viewing\n",
    "                ‚Ä¢ Filters are optional and only applied when provided\"\"\"\n",
    "            return timing_display\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error formatting timing data: {e}\"\n",
    "\n",
    "\n",
    "    # Set example query functions\n",
    "    def set_example_1():\n",
    "        return \"Tell me all about the costa rica 1907 inverted centers?\"\n",
    "    \n",
    "    def set_example_2():\n",
    "        return \"Show me Costa Rica overprinted stamps with varieties or errors\"\n",
    "    \n",
    "    def set_example_3():\n",
    "        return \"1934 airmail definitive issue with catalog values C15-27\"\n",
    "    \n",
    "    def set_example_4():\n",
    "        return \"Tell me all about the first issue crack plate?\"\n",
    "    \n",
    "    def set_example_5():\n",
    "        return \"Costa Rica stamps with perforation errors or printing varieties\"\n",
    "    \n",
    "    def set_example_6():\n",
    "        return \"Research about all the mirror impression stamps of Costa Rica\"\n",
    "\n",
    "    # System information\n",
    "    collection_name = rag_system.get(\"collection_name\", \"Oxcart\")\n",
    "    total_docs = rag_system.get(\"total_documents\", 0)\n",
    "    total_chunks = rag_system.get(\"total_chunks\", 0)\n",
    "\n",
    "    # --- UI with improved philatelic filters ---\n",
    "    with gr.Blocks(\n",
    "        title=\"OXCART RAG - Costa Rica Philatelic System\",\n",
    "        css=\"\"\"\n",
    "        .markdown-text {\n",
    "            font-family: 'Inter', sans-serif;\n",
    "        }\n",
    "        table {\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid #ddd;\n",
    "            padding: 8px;\n",
    "            text-align: left;\n",
    "        }\n",
    "        \"\"\"\n",
    "    ) as interface:\n",
    "        gr.Markdown(\n",
    "            \"# üá®üá∑ OXCART RAG - Costa Rica Philatelic System\\n\\n\"\n",
    "            \"Advanced search for Costa Rican stamps and postal history with sequential dual AI approaches.\"\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Your Costa Rica philatelic query\",\n",
    "                    placeholder=\"e.g., What Costa Rica stamps from 1907 have Scott number 68?\",\n",
    "                    lines=2,\n",
    "                )\n",
    "\n",
    "                search_btn = gr.Button(\"üîç Search with Both Approaches (Sequential)\", variant=\"primary\")\n",
    "\n",
    "                gr.Markdown(\"**Example Queries:**\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    btn1 = gr.Button(\"üìÆ 1907 inverted centers\", variant=\"secondary\")\n",
    "                    btn2 = gr.Button(\"üìÆ Overprinted varieties\", variant=\"secondary\")\n",
    "                    btn3 = gr.Button(\"üìÆ 1934 airmail stamps\", variant=\"secondary\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    btn4 = gr.Button(\"üìÆ First issue crack plate\", variant=\"secondary\")\n",
    "                    btn5 = gr.Button(\"üìÆ Perforation errors\", variant=\"secondary\")\n",
    "                    btn6 = gr.Button(\"üìÆ Mirror impression stamps\", variant=\"secondary\")\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"**üîß Optional Philatelic Filters**\")\n",
    "                gr.Markdown(\"*Leave empty to search all documents*\")\n",
    "\n",
    "                year_start = gr.Textbox(\n",
    "                    label=\"Start Year (optional)\", \n",
    "                    value=\"\", \n",
    "                    placeholder=\"e.g., 1907\",\n",
    "                    info=\"Leave empty for no year filter\"\n",
    "                )\n",
    "                year_end = gr.Textbox(\n",
    "                    label=\"End Year (optional)\", \n",
    "                    value=\"\", \n",
    "                    placeholder=\"e.g., 1910\",\n",
    "                    info=\"Leave empty for no year filter\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"*Note: Both years must be provided to apply year filter*\")\n",
    "                \n",
    "                scott_numbers = gr.Textbox(\n",
    "                    label=\"Scott Numbers (optional)\", \n",
    "                    placeholder=\"e.g., 1,2,3,4,5 or 68,C15\",\n",
    "                    value=\"\",  # Empty string by default\n",
    "                    info=\"Comma-separated catalog numbers (leave empty for no filter)\"\n",
    "                )\n",
    "                max_results = gr.Slider(\n",
    "                    minimum=20,\n",
    "                    maximum=100,\n",
    "                    value=30,\n",
    "                    step=10,\n",
    "                    label=\"Maximum results per approach\",\n",
    "                )\n",
    "\n",
    "        # Timing display as Textbox for better updates\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                timing_display = gr.Textbox(\n",
    "                    label=\"‚è±Ô∏è Performance Timing & Cost Comparison\",\n",
    "                    lines=22,\n",
    "                    interactive=False,\n",
    "                    value=\"Run a search to see detailed timing and cost comparison between approaches\\n\\nFilters are optional - leave them empty to search all documents\",\n",
    "                    elem_id=\"timing-display\"\n",
    "                )\n",
    "\n",
    "        # Tabbed output with HTML components\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"üîç Basic Hybrid Search\"):\n",
    "                gr.Markdown(\"**Combines vector similarity with keyword matching (35% vector + 65% keyword)**\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## AI Response - Basic Approach\")\n",
    "                        basic_answer_output = gr.HTML(\n",
    "                            value=\"<p><em>Waiting for search...</em></p>\",\n",
    "                            elem_id=\"basic_answer\"\n",
    "                        )\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Documents Found - Basic Search\")\n",
    "                        basic_search_output = gr.HTML(\n",
    "                            value=\"<p><em>No results yet</em></p>\",\n",
    "                            elem_id=\"basic_search\"\n",
    "                        )\n",
    "\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Metadata - Basic Search\")\n",
    "                        basic_metadata_output = gr.HTML(\n",
    "                            value=\"<p><em>No metadata yet</em></p>\",\n",
    "                            elem_id=\"basic_metadata\"\n",
    "                        )\n",
    "\n",
    "            with gr.TabItem(\"üöÄ Advanced Compression Search\"):\n",
    "                gr.Markdown(\"**Multi-query ensemble retrieval with AI-powered document compression**\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## AI Response - Advanced Approach\")\n",
    "                        advanced_answer_output = gr.HTML(\n",
    "                            value=\"<p><em>Waiting for search...</em></p>\",\n",
    "                            elem_id=\"advanced_answer\"\n",
    "                        )\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Documents Found - Advanced Search\")\n",
    "                        advanced_search_output = gr.HTML(\n",
    "                            value=\"<p><em>No results yet</em></p>\",\n",
    "                            elem_id=\"advanced_search\"\n",
    "                        )\n",
    "\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Metadata - Advanced Search\")\n",
    "                        advanced_metadata_output = gr.HTML(\n",
    "                            value=\"<p><em>No metadata yet</em></p>\",\n",
    "                            elem_id=\"advanced_metadata\"\n",
    "                        )\n",
    "\n",
    "        # Wire up events with new parameters\n",
    "        search_btn.click(\n",
    "            fn=gradio_sequential_search,\n",
    "            inputs=[query_input, year_start, year_end, scott_numbers, max_results],\n",
    "            outputs=[\n",
    "                basic_answer_output, basic_search_output, basic_metadata_output,\n",
    "                advanced_answer_output, advanced_search_output, advanced_metadata_output,\n",
    "                timing_display\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        query_input.submit(\n",
    "            fn=gradio_sequential_search,\n",
    "            inputs=[query_input, year_start, year_end, scott_numbers, max_results],\n",
    "            outputs=[\n",
    "                basic_answer_output, basic_search_output, basic_metadata_output,\n",
    "                advanced_answer_output, advanced_search_output, advanced_metadata_output,\n",
    "                timing_display\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Example buttons\n",
    "        btn1.click(fn=set_example_1, outputs=[query_input])\n",
    "        btn2.click(fn=set_example_2, outputs=[query_input])\n",
    "        btn3.click(fn=set_example_3, outputs=[query_input])\n",
    "        btn4.click(fn=set_example_4, outputs=[query_input])\n",
    "        btn5.click(fn=set_example_5, outputs=[query_input])\n",
    "        btn6.click(fn=set_example_6, outputs=[query_input])\n",
    "\n",
    "        # System information with cost details\n",
    "        gr.Markdown(\n",
    "            \"---\\n\"\n",
    "            f\"**System Status:**\\n\"\n",
    "            f\"‚Ä¢ Collection: {collection_name}\\n\"\n",
    "            f\"‚Ä¢ Documents indexed: {total_docs:,}\\n\"\n",
    "            f\"‚Ä¢ Total chunks: {total_chunks:,}\\n\"\n",
    "            f\"‚Ä¢ Status: ‚úÖ Operational\\n\\n\"\n",
    "            f\"**Filter Options:**\\n\"\n",
    "            f\"‚Ä¢ **All filters are OPTIONAL** - leave empty to search entire collection\\n\"\n",
    "            f\"‚Ä¢ **Year filter**: Requires BOTH start and end year to activate\\n\"\n",
    "            f\"‚Ä¢ **Scott numbers**: Can specify one or multiple catalog numbers\\n\"\n",
    "            f\"‚Ä¢ **No filters**: Searches across all documents (recommended for general queries)\\n\\n\"\n",
    "            f\"**Cost Tracking:**\\n\"\n",
    "            f\"‚Ä¢ Model: GPT-5-Nano\\n\"\n",
    "            f\"‚Ä¢ Input: $0.05 per 1M tokens\\n\"\n",
    "            f\"‚Ä¢ Output: $0.40 per 1M tokens\\n\"\n",
    "            f\"‚Ä¢ Cost breakdown shown in metadata\\n\\n\"\n",
    "            f\"**Execution Mode:**\\n\"\n",
    "            f\"‚Ä¢ Sequential execution: Basic search completes first, then Advanced\\n\"\n",
    "            f\"‚Ä¢ Results display progressively as each search completes\\n\"\n",
    "            f\"‚Ä¢ Token usage and costs tracked for each approach\\n\\n\"\n",
    "            f\"**Search Approaches:**\\n\"\n",
    "            f\"‚Ä¢ **Basic Search**: Hybrid semantic search optimized for exact catalog numbers\\n\"\n",
    "            f\"‚Ä¢ **Advanced Search**: Multi-query ensemble with AI compression for complex queries\"\n",
    "        )\n",
    "\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Enhanced launcher ----\n",
    "if rag_system and rag_system.get(\"success\", False):\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"üöÄ LAUNCHING COSTA RICA PHILATELIC RAG INTERFACE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    gradio_app = create_gradio_interface(rag_system)\n",
    "\n",
    "    GRADIO_PORT = int(os.getenv(\"GRADIO_PORT\", 7860))\n",
    "    GRADIO_SHARE = os.getenv(\"GRADIO_SHARE\", \"false\").lower() == \"true\"\n",
    "\n",
    "    print(f\"‚öôÔ∏è Port: {GRADIO_PORT}\")\n",
    "    print(f\"üåç Public URL: {'‚ö†Ô∏è Attempting...' if GRADIO_SHARE else '‚ùå Disabled (more secure)'}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Starting Gradio server...\")\n",
    "        \n",
    "        if GRADIO_SHARE:\n",
    "            print(\"‚è≥ Attempting to create public tunnel...\")\n",
    "            try:\n",
    "                demo = gradio_app.launch(\n",
    "                    server_port=GRADIO_PORT,\n",
    "                    share=True,\n",
    "                    inbrowser=False,\n",
    "                    show_error=True,\n",
    "                    prevent_thread_lock=False,\n",
    "                    quiet=False,\n",
    "                )\n",
    "                \n",
    "                print(\"\\\\nüéâ SUCCESS! Public tunnel created\")\n",
    "                print(f\"üåê AVAILABLE URLS:\")\n",
    "                print(f\"   üì± Local: http://localhost:{GRADIO_PORT}\")\n",
    "                \n",
    "                if hasattr(demo, 'share_url') and demo.share_url:\n",
    "                    print(f\"   üåç Public: {demo.share_url}\")\n",
    "                    print(f\"\\\\nüîó **PUBLIC URL:** {demo.share_url}\")\n",
    "                else:\n",
    "                    print(f\"   üåç Public: Check Gradio output above ‚òùÔ∏è\")\n",
    "                \n",
    "            except Exception as share_error:\n",
    "                print(f\"‚ö†Ô∏è Error creating public tunnel: {share_error}\")\n",
    "                print(\"üîÑ Switching to local mode only...\")\n",
    "                \n",
    "                demo = gradio_app.launch(\n",
    "                    server_port=GRADIO_PORT,\n",
    "                    share=False,\n",
    "                    inbrowser=True,\n",
    "                    show_error=True,\n",
    "                    prevent_thread_lock=False\n",
    "                )\n",
    "                \n",
    "                print(f\"\\\\n‚úÖ LOCAL SERVER OPERATIONAL:\")\n",
    "                print(f\"   üì± Local URL: http://localhost:{GRADIO_PORT}\")\n",
    "                print(f\"   ‚ö†Ô∏è Public URL: Not available (tunnel error)\")\n",
    "                \n",
    "        else:\n",
    "            demo = gradio_app.launch(\n",
    "                server_port=GRADIO_PORT,\n",
    "                share=False,\n",
    "                inbrowser=True,\n",
    "                show_error=True,\n",
    "                prevent_thread_lock=False\n",
    "            )\n",
    "            \n",
    "            print(f\"\\\\n‚úÖ LOCAL SERVER OPERATIONAL:\")\n",
    "            print(f\"   üì± Local URL: http://localhost:{GRADIO_PORT}\")\n",
    "            print(f\"   üí° For public URL, set GRADIO_SHARE=true in .env\")\n",
    "        \n",
    "        print(f\"\\\\nüìã COSTA RICA PHILATELIC FEATURES:\")\n",
    "        print(f\"   ‚Ä¢ Specialized Costa Rica stamp queries\")\n",
    "        print(f\"   ‚Ä¢ Scott catalog number search\")\n",
    "        print(f\"   ‚Ä¢ Variety and error detection\")\n",
    "        print(f\"   ‚Ä¢ Dual search approaches for comprehensive results\")\n",
    "        print(f\"   ‚Ä¢ Performance timing comparison\")\n",
    "        print(f\"   ‚Ä¢ To stop: gr.close_all()\")\n",
    "        \n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"üá®üá∑ COSTA RICA PHILATELIC RAG INTERFACE READY!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Critical error launching Gradio: {e}\")\n",
    "        print(\"\\\\nüîß SUGGESTED SOLUTIONS:\")\n",
    "        print(\"   1. Run: gr.close_all()\")\n",
    "        print(\"   2. Change port: GRADIO_PORT=7861 in .env\")\n",
    "        print(\"   3. Verify no other services on the port\")\n",
    "        print(\"   4. Restart the notebook\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\\\n‚ö†Ô∏è  Cannot create Gradio interface:\")\n",
    "    if not rag_system:\n",
    "        print(\"   ‚Ä¢ RAG system not configured\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ RAG error: {rag_system.get('error', 'Unknown error')}\")\n",
    "    print(\"\\\\nüîß To resolve:\")\n",
    "    print(\"   1. Verify Weaviate is running\")\n",
    "    print(\"   2. Configure OPENAI_API_KEY in .env\") \n",
    "    print(\"   3. Run document indexing\")\n",
    "    print(\"   4. Restart this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
