{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philatelic Gradio App with Weaviate\n",
    "Interactive web interface for CR Philately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Load all the modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from datetime import datetime\n",
    "import weaviate\n",
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_weaviate import WeaviateVectorStore\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.retrievers import MultiQueryRetriever, EnsembleRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "\n",
    "from philatelic_weaviate import *\n",
    "\n",
    "from philatelic_chunk_schema import *\n",
    "\n",
    "print(\"✅ Basic imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "WEAVIATE_URL = os.getenv('WEAVIATE_URL', 'http://localhost:8083')\n",
    "PHILATELIC_JSONS_DIR = os.getenv('PHILATELIC_JSONS_DIR', './results/final_jsons')\n",
    "COLLECTION_NAME = os.getenv('WEAVIATE_COLLECTION_NAME', 'Oxcart')\n",
    "\n",
    "print(f\"🔧 Configuration:\")\n",
    "print(f\"   • Weaviate URL: {WEAVIATE_URL}\")\n",
    "print(f\"   • JSONs Directory: {PHILATELIC_JSONS_DIR}\")\n",
    "print(f\"   • Collection Name: {COLLECTION_NAME}\")\n",
    "print(f\"   • OpenAI API Key: {'✅ Configured' if OPENAI_API_KEY else '❌ Missing configuration'}\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"\\\\n⚠️  IMPORTANT: Configure your OPENAI_API_KEY in the .env file\")\n",
    "    print(\"   Copy .env.example to .env and add your API key\")\n",
    "\n",
    "# Verify that the JSONs directory exists\n",
    "if not os.path.exists(PHILATELIC_JSONS_DIR):\n",
    "    print(f\"\\\\n⚠️  Directory {PHILATELIC_JSONS_DIR} not found\")\n",
    "    print(\"   Make sure you have processed documents with the Dolphin parser\")\n",
    "else:\n",
    "    json_files = glob.glob(os.path.join(PHILATELIC_JSONS_DIR, '*_final.json'))\n",
    "    print(f\"\\\\n📁 Found {len(json_files)} philatelic JSON files\")\n",
    "    if json_files:\n",
    "        print(\"   Examples:\")\n",
    "        for file in json_files[:3]:\n",
    "            print(f\"   • {os.path.basename(file)}\")\n",
    "        if len(json_files) > 3:\n",
    "            print(f\"   • ... and {len(json_files) - 3} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            model=\"gpt-5-nano\", \n",
    "            api_key=OPENAI_API_KEY, \n",
    "            temperature=1,  # obligatorio para gpt-5-nano\n",
    "            timeout=120.0,\n",
    "            #max_completion_tokens=2500,\n",
    "            model_kwargs={\n",
    "                \"verbosity\": \"medium\",\n",
    "                \"reasoning_effort\" : \"low\"\n",
    "            })\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# 📝 RAG PROMPT TEMPLATE - Professional Philatelic Consultation\n",
    "# ========================================================================================\n",
    "\n",
    "philatelic_rag_template = \"\"\"You are a senior philatelic researcher and catalog specialist with expertise in Costa Rican stamps and postal history. Provide comprehensive, well-structured analysis based strictly on the source materials provided.\n",
    "\n",
    "SOURCE MATERIALS:\n",
    "{context}\n",
    "\n",
    "RESEARCH QUERY: {question}\n",
    "\n",
    "RESPONSE REQUIREMENTS:\n",
    "\n",
    "FORMATTING & STRUCTURE:\n",
    "• Use clear hierarchical organization with descriptive headers using markdown\n",
    "• Group related information under logical categories using ## and **bold subheadings**\n",
    "• Use bullet points (•) for individual facts and varieties\n",
    "• Include relevant emojis for major sections (🔍 📮 📚 🎯) to enhance readability\n",
    "• Bold key terms, catalog numbers, and important details\n",
    "• KEEP SECTIONS CONCISE - avoid excessive repetition or overly detailed explanations\n",
    "• Your output is in markdown format\n",
    "\n",
    "CITATION FORMAT:\n",
    "• Every factual statement must include the name of the document (doc_id) and its page numeber like this example: (CRF 100, page 15)\n",
    "• Multiple sources: (doc_id, page number; doc_id, page number) Example: (OXCART 123, page 24 ; OXCART 25, page 15)\n",
    "• Always cite catalog numbers (scott, yvert, michell, etc), varieties, dates, quantities, and technical specifications\n",
    "• When quoting directly, use quotation marks around quoted text\n",
    "\n",
    "CONTENT ORGANIZATION:\n",
    "• Lead with the most direct answer to the query\n",
    "• Organize by catalog numbers, chronological order, or logical categories as appropriate\n",
    "• Include technical specifications: dates, quantities, colors, perforations, varieties\n",
    "• Provide brief historical context and collecting significance\n",
    "• Note relationships between issues, varieties, or catalog entries\n",
    "• Address valuation or rarity when relevant to the query\n",
    "\n",
    "RESPONSE LENGTH:\n",
    "• Aim for clear, informative responses that are thorough but not excessive\n",
    "• Eliminate redundant information and repetitive explanations\n",
    "• Focus on the most relevant information that directly answers the query\n",
    "• If information is extensive, prioritize the most important catalog entries and varieties\n",
    "\n",
    "TECHNICAL STANDARDS:\n",
    "• Use precise philatelic terminology (definitive, commemorative, variety, error, overprint, etc.)\n",
    "• Specify exact catalog numbers with proper formatting (Scott C216, not just C216)\n",
    "• Include denomination and color details when available\n",
    "• Note printing quantities, dates, and technical varieties\n",
    "• Distinguish between verified catalog facts and expert opinions\n",
    "• Flag incomplete or uncertain information clearly\n",
    "\n",
    "RESEARCH COMPLETENESS:\n",
    "• If source materials are insufficient, state: \"The provided documents do not contain sufficient information about...\"\n",
    "• Suggest what additional sources or information would be needed\n",
    "• Note any gaps in catalog coverage or missing details\n",
    "\n",
    "PROFESSIONAL TONE:\n",
    "• Maintain authoritative but accessible language\n",
    "• Present information objectively without unnecessary qualifiers\n",
    "• Use active voice and clear, direct statements\n",
    "• Avoid speculation beyond what sources support\n",
    "\n",
    "RESPONSE:\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "rag_prompt = PromptTemplate(\n",
    "    template=philatelic_rag_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# 📄 OPTIMIZED DOCUMENT FORMATTING - For Academic Citation Style\n",
    "# ========================================================================================\n",
    "\n",
    "def format_docs_for_rag(docs_results: List[Dict]) -> str:\n",
    "    \"\"\"Efficient document formatting optimized for academic citation style (Document Name, p. Page)\"\"\"\n",
    "    \n",
    "    if not docs_results:\n",
    "        return \"\\nNo source documents available.\"\n",
    "    \n",
    "    # Group and sort documents by authority\n",
    "    #doc_groups = {'catalog': [], 'literature': [], 'collection': [], 'reference': []}\n",
    "    docs = []\n",
    "    \n",
    "    for i, doc in enumerate(docs_results, 1):\n",
    "        #category, reliability = classify_document_authority(doc.metadata.get('doc_id', 'Unknown'))\n",
    "        \n",
    "        doc_info = {\n",
    "            'doc_num': i,\n",
    "            'doc_id': doc.metadata.get('doc_id', 'Unknown'),\n",
    "            'page': doc.metadata.get('page_number', 'N/A'),\n",
    "            'content': doc.page_content,\n",
    "        }\n",
    "        #doc_groups[category].append(doc_info)\n",
    "        docs.append(doc_info)\n",
    "    return docs\n",
    "\n",
    "def create_rag_response(retriever_results: List[Dict], query: str) -> Dict:\n",
    "    \"\"\"Streamlined RAG chain execution with academic citation style and token tracking\"\"\"\n",
    "    \n",
    "    if not retriever_results:\n",
    "        return {\n",
    "            \"response\": \"No documents found for this query.\", \n",
    "            \"generation_time\": 0,\n",
    "            \"context_docs_count\": 0,\n",
    "            \"context_length\": 0,\n",
    "            \"token_usage\": {\n",
    "                \"input_tokens\": 0,\n",
    "                \"output_tokens\": 0,\n",
    "                \"total_tokens\": 0\n",
    "            },\n",
    "            \"cost_info\": {\n",
    "                \"estimated_cost_usd\": 0,\n",
    "                \"input_cost\": 0,\n",
    "                \"output_cost\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Format context efficiently for academic citations\n",
    "    context = format_docs_for_rag(retriever_results)\n",
    "    \n",
    "    # Execute RAG chain with OpenAI callback for token tracking\n",
    "    rag_chain = (\n",
    "        {\"context\": lambda x: context, \"question\": RunnablePassthrough()}\n",
    "        | rag_prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use OpenAI callback to track token usage\n",
    "    with get_openai_callback() as cb:\n",
    "        response = rag_chain.invoke(query)\n",
    "        # Get token counts from callback\n",
    "        input_tokens = cb.prompt_tokens\n",
    "        output_tokens = cb.completion_tokens\n",
    "        total_tokens = cb.total_tokens\n",
    "        \n",
    "        # OpenAI callback provides cost directly, but we'll calculate our own\n",
    "        # based on GPT-5-nano pricing\n",
    "    \n",
    "    generation_time = round(time.time() - start_time, 2)\n",
    "    \n",
    "    # Calculate costs for GPT-5-nano\n",
    "    # $0.05 per 1M input tokens, $0.40 per 1M output tokens\n",
    "    cost_per_1m_input = 0.05\n",
    "    cost_per_1m_output = 0.40\n",
    "    \n",
    "    # Convert to cost per token\n",
    "    cost_per_input_token = cost_per_1m_input / 1_000_000\n",
    "    cost_per_output_token = cost_per_1m_output / 1_000_000\n",
    "    \n",
    "    input_cost = input_tokens * cost_per_input_token\n",
    "    output_cost = output_tokens * cost_per_output_token\n",
    "    estimated_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"generation_time\": generation_time,\n",
    "        \"context_docs_count\": len(retriever_results),\n",
    "        \"context_length\": len(context),        \n",
    "        \"token_usage\": {\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens\n",
    "        },\n",
    "        \"cost_info\": {\n",
    "            \"estimated_cost_usd\": round(estimated_cost, 6),\n",
    "            \"input_cost\": round(input_cost, 6),\n",
    "            \"output_cost\": round(output_cost, 6)\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weaviate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Weaviate\n",
    "print(\"🔌 Connecting to Weaviate...\")\n",
    "\n",
    "try:\n",
    "    client = create_weaviate_client(WEAVIATE_URL, OPENAI_API_KEY)\n",
    "    print(\"✅ Connection successful\")\n",
    "    \n",
    "    # Verify that Weaviate is working\n",
    "    meta = client.get_meta()\n",
    "    print(f\"📊 Weaviate version: {meta.get('version', 'unknown')}\")\n",
    "    \n",
    "    # Verify if collection exists\n",
    "    try:\n",
    "        collections = client.collections.list_all()\n",
    "        collection_names = [col.name for col in collections]\n",
    "        \n",
    "        if COLLECTION_NAME in collection_names:\n",
    "            collection = client.collections.get(COLLECTION_NAME)\n",
    "            total_objects = collection.aggregate.over_all(total_count=True).total_count\n",
    "            print(f\"📊 Collection '{COLLECTION_NAME}' exists with {total_objects} documents\")\n",
    "        else:\n",
    "            print(f\"📝 Collection '{COLLECTION_NAME}' does not exist (will be created during indexing)\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not verify collections: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error connecting to Weaviate: {e}\")\n",
    "    print(\"💡 Make sure Weaviate is running:\")\n",
    "    print(\"   docker-compose up -d\")\n",
    "    client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weaviate Search Tests\n",
    "\n",
    "Test the function search_chunks_semantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_chunks_semantic(\n",
    "                client, \n",
    "                \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\", \n",
    "                \"Oxcart\", \n",
    "                limit=50,\n",
    "                filters=[],\n",
    "                mode = \"hybrid\",\n",
    "                alpha= 0.35\n",
    "                \n",
    "            )\n",
    "            \n",
    "print(f\"   📊 Resultados: {len(results)}\")\n",
    "\n",
    "for j, result in enumerate(results, 1):\n",
    "    print(f\"\\n      🏷️ #{j} (Score: {result['score']:.3f})\")\n",
    "    print(f\"         📄 Documento: {result['doc_id']}\")\n",
    "    print(f\"         📋 Tipo: {result['chunk_type']}\")\n",
    "    print(f\"         📄 Página: {result['page_number']}\")\n",
    "    \n",
    "    # Mostrar metadatos relevantes\n",
    "    if result.get('catalog_systems'):\n",
    "        print(f\"         📖 Catálogos: {result['catalog_systems']}\")\n",
    "    if result.get('scott_numbers'):\n",
    "        print(f\"         🔢 Scott: {result['scott_numbers']}\")\n",
    "    if result.get('years'):\n",
    "        print(f\"         📅 Años: {result['years']}\")\n",
    "    if result.get('colors'):\n",
    "        print(f\"         🎨 Colores: {result['colors']}\")\n",
    "    if result.get('variety_classes'):\n",
    "        print(f\"         🔀 Variedades: {result['variety_classes']}\")\n",
    "    \n",
    "    # Texto truncado\n",
    "    text = result.get('text', '')\n",
    "    # if len(text) > 200:\n",
    "    #     text = text[:200] + \"...\"\n",
    "    print(f\"         📝 Texto: {text}\")\n",
    "    print(\"**********************************************************************************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advance Retriever Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "def compress_documents_simple(documents: List[Document], query: str, llm) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Simple document compression using LangChain's native batch processing.\n",
    "    Each document is processed individually with the same prompt.\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return []\n",
    "    \n",
    "    # Simple compression prompt for individual documents\n",
    "    compress_prompt_template = \"\"\"You are a philatelic expert. Extract and compress ONLY the information relevant to this query from the document below.\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "DOCUMENT:\n",
    "{document}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Extract only information directly relevant to the query\n",
    "- Preserve exact catalog numbers, dates, denominations, and technical specifications\n",
    "- Keep Scott numbers, Michel numbers, and other catalog references intact\n",
    "- Maintain variety descriptions and error information\n",
    "- Remove irrelevant content but preserve context for understanding\n",
    "\n",
    "If the document contains no relevant information, respond with exactly: NO_RELEVANT_CONTENT\n",
    "\n",
    "COMPRESSED CONTENT:\"\"\"\n",
    "\n",
    "    # Create individual prompts for each document\n",
    "    prompts = []\n",
    "    for doc in documents:\n",
    "        prompt_text = compress_prompt_template.format(\n",
    "            query=query, \n",
    "            document=doc.page_content\n",
    "        )\n",
    "        prompts.append([(\"user\", prompt_text)])\n",
    "    \n",
    "    # Use LangChain's native batch processing with concurrency control\n",
    "    config = RunnableConfig(max_concurrency=10)  # Process 10 documents concurrently\n",
    "    \n",
    "    try:\n",
    "        responses = llm.batch(prompts, config=config)\n",
    "        \n",
    "        # Filter and create compressed documents\n",
    "        compressed_docs = []\n",
    "        for i, response in enumerate(responses):\n",
    "            content = response.content.strip() if hasattr(response, 'content') else str(response).strip()\n",
    "            \n",
    "            # Only include documents that have relevant content\n",
    "            if content and content != \"NO_RELEVANT_CONTENT\":\n",
    "                compressed_doc = Document(\n",
    "                    page_content=content,\n",
    "                    metadata=documents[i].metadata\n",
    "                )\n",
    "                compressed_docs.append(compressed_doc)\n",
    "        \n",
    "        return compressed_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch compression: {e}\")\n",
    "        # Fallback: return original documents\n",
    "        return documents\n",
    "\n",
    "def search_stamps_with_compression(query, client, embeddings, llm, limit=100, \n",
    "                                 alpha=0.30, diversity_lambda=0.75):\n",
    "    \"\"\"\n",
    "    Optimized philatelic search with simple batch document compression using LangChain's native batch processing.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The stamp query\n",
    "        client: Weaviate client\n",
    "        embeddings: Embedding model\n",
    "        llm: Language model\n",
    "        limit (int): Maximum documents to retrieve\n",
    "        alpha (float): Hybrid search factor (0.30 = 30% vector, 70% keywords)\n",
    "        diversity_lambda (float): MMR diversity factor (0.75 = good diversity)\n",
    "    \n",
    "    Returns:\n",
    "        list: Compressed and optimized documents for philatelic queries\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Create vector store\n",
    "    vector_store = WeaviateVectorStore(\n",
    "        client=client,\n",
    "        index_name=COLLECTION_NAME,\n",
    "        text_key=\"text\",\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Try to create hybrid retriever\n",
    "    hybrid_kwargs = {\"k\": limit // 2}\n",
    "    if alpha is not None:\n",
    "        hybrid_kwargs[\"alpha\"] = alpha\n",
    "    \n",
    "    # 1. Precision hybrid retriever (captures exact numbers + context)\n",
    "    precision_retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs=hybrid_kwargs\n",
    "    )\n",
    "    \n",
    "    # 2. Diversity MMR retriever (avoids duplicate stamps)\n",
    "    diversity_retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": limit // 2, \"lambda_mult\": diversity_lambda}\n",
    "    )\n",
    "    \n",
    "    # 3. Ensemble with dual strategy\n",
    "    base_retriever = EnsembleRetriever(\n",
    "        retrievers=[precision_retriever, diversity_retriever],\n",
    "        weights=[0.7, 0.3]  # 70% precision + 30% diversity\n",
    "    )\n",
    "    \n",
    "    # Specialized prompt for philatelic multi-query generation\n",
    "    query_prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are a specialized philatelic researcher expert in stamp catalogues and varieties.\n",
    "Generate 3 strategically different versions of the question to capture comprehensive stamp information:\n",
    "\n",
    "ORIGINAL: {question}\n",
    "\n",
    "Create variations that target:\n",
    "1. CATALOG PRECISION: Focus on exact catalog numbers, dates, and technical specifications\n",
    "2. CONTEXTUAL SEARCH: Include related series, printings, varieties, and historical context  \n",
    "3. TERMINOLOGY ALTERNATIVES: Use alternative philatelic terms and synonyms\n",
    "\n",
    "Consider these philatelic elements:\n",
    "- Catalog systems: Scott, Michel, Yvert, SG, local catalogs\n",
    "- Technical terms: definitive/commemorative, variety/error, overprint/surcharge\n",
    "- Time references: issue dates, printing dates, first day covers\n",
    "- Denominations: face values, colors, perforations\n",
    "\n",
    "Alternative searches:\n",
    "1.\n",
    "2. \n",
    "3.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # MultiQueryRetriever with specialized prompt\n",
    "    multi_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=base_retriever,\n",
    "        llm=llm,\n",
    "        prompt=query_prompt,\n",
    "        parser_key=\"lines\"\n",
    "    )\n",
    "    \n",
    "    # Execute initial retrieval\n",
    "    initial_results = multi_retriever.invoke(query)\n",
    "       \n",
    "    compression_llm = ChatOpenAI(\n",
    "            model=\"gpt-5-nano\", \n",
    "            api_key=OPENAI_API_KEY, \n",
    "            temperature=1,  # obligatorio para gpt-5-nano\n",
    "            timeout=120.0,\n",
    "            model_kwargs={\n",
    "                \"verbosity\": \"medium\",\n",
    "                \"reasoning_effort\" : \"low\"\n",
    "            })\n",
    "    \n",
    "    # Simple batch compression using LangChain's native batch processing\n",
    "    compressed_results = compress_documents_simple(initial_results, query, compression_llm)\n",
    "    \n",
    "    # Reorder by quality_score if it exists\n",
    "    def get_quality_score(doc):\n",
    "        return getattr(doc, 'metadata', {}).get('quality_score', 0.0)\n",
    "    \n",
    "    sorted_results = sorted(compressed_results, key=get_quality_score, reverse=True)\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the method search_stamps_with_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the optimized search_stamps_with_compression with batch processing\n",
    "# print(\"🧪 Testing optimized batch compression...\")\n",
    "\n",
    "# # Test query focused on specific stamps\n",
    "# test_query = \"Costa Rica 1907 2 colones stamp with original gum Scott 68\"\n",
    "\n",
    "# print(f\"🔍 Query: {test_query}\")\n",
    "# print(\"⏱️ Starting optimized search with batch compression...\")\n",
    "\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# try:\n",
    "#     compressed_docs = search_stamps_with_compression(\n",
    "#         query=test_query,\n",
    "#         client=client, \n",
    "#         embeddings=embeddings, \n",
    "#         limit=30,\n",
    "#         llm=llm,\n",
    "#         alpha=0.30,  # 30% vectorial, 70% keywords for exact numbers\n",
    "#         diversity_lambda=0.75  # 75% relevance, 25% diversity\n",
    "#     )\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     execution_time = end_time - start_time\n",
    "    \n",
    "#     print(f\"✅ Batch compression completed in {execution_time:.2f} seconds\")\n",
    "#     print(f\"📊 Retrieved and compressed {len(compressed_docs)} documents\")\n",
    "    \n",
    "#     # Show sample results\n",
    "#     for i, doc in enumerate(compressed_docs[:3], 1):\n",
    "#         print(f\"\\\\n📄 Document {i}:\")\n",
    "#         print(f\"   Metadata: {getattr(doc, 'metadata', {})}\")\n",
    "#         content = getattr(doc, 'page_content', str(doc))\n",
    "#         preview = content[:200] + \"...\" if len(content) > 200 else content\n",
    "#         print(f\"   Content: {preview}\")\n",
    "        \n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Error during batch compression test: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_info() -> str:\n",
    "    \"\"\"\n",
    "    Get collection information to display in the interface.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return \"❌ No Weaviate connection\"\n",
    "    \n",
    "    try:\n",
    "        stats = get_collection_stats(client, \"Oxcart\")\n",
    "        if stats:\n",
    "            info = f\"📊 **Oxcart Collection Statistics:**\\\\n\\\\n\"\n",
    "            info += f\"📦 **Total chunks:** {stats['total_chunks']:,}\\\\n\"\n",
    "            info += f\"📄 **Documents:** {stats['total_documents']}\\\\n\\\\n\"\n",
    "            \n",
    "            if stats.get('documents'):\n",
    "                info += \"**Indexed documents:**\\\\n\"\n",
    "                for doc_id, count in stats['documents'].items():\n",
    "                    info += f\"• {doc_id}: {count:,} chunks\\\\n\"\n",
    "            \n",
    "            return info\n",
    "        else:\n",
    "            return \"❌ Could not retrieve statistics\"\n",
    "    except Exception as e:\n",
    "        return f\"❌ Error: {e}\"\n",
    "\n",
    "print(\"✅ RAG functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = get_collection_stats(client, \"Oxcart\")\n",
    "stats['total_documents']\n",
    "stats['total_chunks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura que usan tus funciones de búsqueda/respuesta\n",
    "rag_system = {\n",
    "    \"success\": True,\n",
    "    \"client\": client,                    # para que search_and_answer pueda consultar\n",
    "    \"collection_name\": COLLECTION_NAME,  # nombre de la colección\n",
    "    \"weaviate_url\": WEAVIATE_URL,        # info para la UI\n",
    "    \"total_documents\": stats['total_documents'],       # para mostrar estado\n",
    "    \"total_chunks\": stats['total_chunks'],        # opcional en la UI\n",
    "    \"embeddings\":embeddings,\n",
    "    \"llm\":llm,\n",
    "    # puedes añadir más campos que tu search_and_answer necesite\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_answer_basic(\n",
    "    query: str,\n",
    "    rag_system: Dict[str, Any],\n",
    "    year_start: Optional[int] = None,\n",
    "    year_end: Optional[int] = None,\n",
    "    scott_numbers: Optional[List[str]] = None,\n",
    "    max_results: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Basic hybrid search approach with improved philatelic filters.\n",
    "    All filters are OPTIONAL - only applied when provided.\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    if not rag_system or not rag_system.get(\"client\"):\n",
    "        return {\n",
    "            \"answer\": \"❌ Error: No Weaviate connection\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\"error\": \"No Weaviate connection\"}\n",
    "        }\n",
    "    \n",
    "    client_wv = rag_system[\"client\"]\n",
    "    collection_name = rag_system.get(\"collection_name\", \"Oxcart\")\n",
    "    \n",
    "    # Build philatelic filters only if values are provided\n",
    "    filters = {}\n",
    "    \n",
    "    # Year range filter - ONLY if both years are provided and valid\n",
    "    if year_start is not None and year_end is not None:\n",
    "        try:\n",
    "            # Ensure both are integers and valid\n",
    "            start = int(year_start)\n",
    "            end = int(year_end)\n",
    "            # Ensure start <= end\n",
    "            if start > end:\n",
    "                start, end = end, start\n",
    "            filters[\"year_range\"] = (start, end)\n",
    "            print(f\"[DEBUG] Year filter applied: {start}-{end}\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"[WARNING] Invalid year values, skipping year filter: {e}\")\n",
    "    \n",
    "    # Scott numbers ONLY if provided and not empty\n",
    "    if scott_numbers:\n",
    "        print(\"Scott Numbers: \",scott_numbers)\n",
    "        filters[\"catalog_system\"] = \"Scott\"\n",
    "        filters[\"scott_numbers\"] = scott_numbers        \n",
    "    \n",
    "    # Log final filter status\n",
    "    if not filters:\n",
    "        print(\"[DEBUG] No filters applied - searching all documents\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] Filters being used: {filters}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Basic semantic search with philatelic filters\n",
    "        # Pass None if no filters, not empty dict\n",
    "        results = search_chunks_semantic(\n",
    "            client=client_wv,\n",
    "            query=query,\n",
    "            collection_name=collection_name,\n",
    "            limit=int(max_results),\n",
    "            filters=filters if filters else None,  # Pass None if no filters\n",
    "            mode=\"hybrid\",\n",
    "            alpha=0.35\n",
    "        )\n",
    "        \n",
    "        # Convert to LangChain document format for RAG\n",
    "        docs_for_rag = []\n",
    "        for r in results:\n",
    "            doc = type('Document', (), {\n",
    "                'page_content': r.get('text', ''),\n",
    "                'metadata': {\n",
    "                    'doc_id': r.get('doc_id', 'N/A'),\n",
    "                    'page_number': r.get('page_number', 'N/A'),\n",
    "                    'chunk_type': r.get('chunk_type', 'N/A'),\n",
    "                    'score': r.get('score', 0.0),\n",
    "                    'scott_numbers': r.get('scott_numbers', []),\n",
    "                    'years': r.get('years', []),\n",
    "                    'catalog_systems': r.get('catalog_systems', [])\n",
    "                }\n",
    "            })()\n",
    "            docs_for_rag.append(doc)\n",
    "        \n",
    "        # Generate RAG response using LangChain\n",
    "        rag_response = create_rag_response(docs_for_rag, query)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Build metadata with actual filters used\n",
    "        metadata = {\n",
    "            \"approach\": \"Basic Hybrid Search\",\n",
    "            \"query\": query,\n",
    "            \"total_results\": len(results),\n",
    "            \"max_results\": int(max_results),\n",
    "            \"filters_used\": filters if filters else \"None (searching all documents)\",\n",
    "            \"generation_time\": execution_time,\n",
    "            \"context_docs_count\": rag_response.get(\"context_docs_count\", len(docs_for_rag)),\n",
    "            \"context_length\": sum(len(d.page_content) for d in docs_for_rag),\n",
    "            \"token_usage\": rag_response.get(\"token_usage\", {}),\n",
    "            \"cost_info\": rag_response.get(\"cost_info\", {}),\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"answer\": rag_response.get(\"response\", \"No response generated\"),\n",
    "            \"results\": results,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_details = traceback.format_exc()\n",
    "        print(f\"[ERROR] Basic search error: {str(e)}\")\n",
    "        print(f\"[ERROR] Full traceback: {error_details}\")\n",
    "        print(f\"[ERROR] Filters attempted: {filters}\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": f\"❌ Basic search error: {str(e)}\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\n",
    "                \"error\": str(e),\n",
    "                \"generation_time\": 0,\n",
    "                \"filters_attempted\": filters if filters else \"None\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_answer_advanced(\n",
    "    query: str,\n",
    "    rag_system: Dict[str, Any],\n",
    "    max_results: int = 10,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Advanced compression search approach - filters NOT applied (as requested).\n",
    "    \"\"\"\n",
    "    # Validation\n",
    "    if not rag_system or not rag_system.get(\"client\"):\n",
    "        return {\n",
    "            \"answer\": \"❌ Error: No Weaviate connection\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\"error\": \"No Weaviate connection\"}\n",
    "        }\n",
    "\n",
    "    client_wv = rag_system[\"client\"]\n",
    "    embeddings = rag_system.get(\"embeddings\")\n",
    "    llm = rag_system.get(\"llm\")\n",
    "    \n",
    "    # NOTE: Advanced search does not apply filters as requested by user\n",
    "    # This approach uses ensemble retrieval and compression instead\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Advanced search with compression (no filters applied)\n",
    "        compressed_docs = search_stamps_with_compression(\n",
    "            query=query,\n",
    "            client=client_wv,\n",
    "            embeddings=embeddings,\n",
    "            llm=llm,\n",
    "            limit=max_results,\n",
    "            alpha=0.30,\n",
    "            diversity_lambda=0.75\n",
    "        )\n",
    "        \n",
    "        # Generate RAG response using LangChain\n",
    "        rag_response = create_rag_response(compressed_docs, query)\n",
    "        \n",
    "        # Extraer y preservar figuras de los documentos originales\n",
    "        figure_pattern = r'(!\\[([^\\]]*)\\]\\([^)]+\\))'\n",
    "\n",
    "        for doc in compressed_docs:\n",
    "            # Buscar figuras en el contenido original si está disponible\n",
    "            original_content = doc.metadata.get('text_original', doc.page_content)\n",
    "            \n",
    "            # Extraer todas las figuras del contenido original\n",
    "            figures = re.findall(figure_pattern, original_content)\n",
    "            \n",
    "            # Eliminar duplicados manteniendo el orden\n",
    "            seen_figures = set()\n",
    "            unique_figures = []\n",
    "            for fig in figures:\n",
    "                # Usar el path de la imagen como identificador único (ignorando el alt text)\n",
    "                img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                if img_path:\n",
    "                    img_identifier = img_path.group(1)\n",
    "                    if img_identifier not in seen_figures:\n",
    "                        seen_figures.add(img_identifier)\n",
    "                        unique_figures.append(fig)\n",
    "            \n",
    "            # Verificar qué figuras ya están en el contenido comprimido\n",
    "            existing_figures = set()\n",
    "            for fig in unique_figures:\n",
    "                if fig[0] in doc.page_content:\n",
    "                    img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                    if img_path:\n",
    "                        existing_figures.add(img_path.group(1))\n",
    "            \n",
    "            # Agregar solo las figuras que faltan\n",
    "            missing_figures = []\n",
    "            for fig in unique_figures:\n",
    "                img_path = re.search(r'\\]\\(([^)]+)\\)', fig[0])\n",
    "                if img_path and img_path.group(1) not in existing_figures:\n",
    "                    missing_figures.append(fig[0])\n",
    "            \n",
    "            # Si hay figuras faltantes, agregarlas al final\n",
    "            if missing_figures:\n",
    "                figures_text = \"\\n\\n\" + \"\\n\".join(missing_figures)\n",
    "                doc.page_content = doc.page_content + figures_text\n",
    "            \n",
    "            # Guardar las figuras únicas en metadata para acceso rápido\n",
    "            doc.metadata['figures'] = [fig[0] for fig in unique_figures] if unique_figures else []\n",
    "            doc.metadata['has_figures'] = len(unique_figures) > 0\n",
    "        \n",
    "        # Convert compressed docs to results format for display\n",
    "        results = []\n",
    "        for i, doc in enumerate(compressed_docs):\n",
    "            result = {\n",
    "                'doc_id': doc.metadata.get('doc_id', 'N/A'),\n",
    "                'page_number': doc.metadata.get('page_number', 'N/A'),\n",
    "                'chunk_type': doc.metadata.get('chunk_type', 'N/A'),\n",
    "                'text': doc.page_content,\n",
    "                'score': doc.metadata.get('quality_score', 0.0),\n",
    "                'catalog_systems': doc.metadata.get('catalog_systems', []),\n",
    "                'scott_numbers': doc.metadata.get('scott_numbers', []),\n",
    "                'years': doc.metadata.get('years', []),\n",
    "                'colors': doc.metadata.get('colors', []),\n",
    "                'variety_classes': doc.metadata.get('variety_classes', []),\n",
    "                'has_figures': doc.metadata.get('has_figures', False),  \n",
    "                'figures': doc.metadata.get('figures', [])  \n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        metadata = {\n",
    "            \"approach\": \"Advanced Compression Search\",\n",
    "            \"query\": query,\n",
    "            \"total_results\": len(results),\n",
    "            \"compressed_docs\": len(compressed_docs),\n",
    "            \"filters_used\": \"No filters (advanced approach)\",\n",
    "            \"generation_time\": execution_time,\n",
    "            \"context_docs_count\": rag_response[\"context_docs_count\"],\n",
    "            \"docs_with_figures\": sum(1 for r in results if r.get('has_figures', False)),\n",
    "            \"token_usage\": rag_response.get(\"token_usage\", {}),\n",
    "            \"cost_info\": rag_response.get(\"cost_info\", {}),\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            \"answer\": rag_response.get(\"response\", \"No response generated\"),\n",
    "            \"results\": results,\n",
    "            \"metadata\": metadata\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"❌ Advanced search error: {str(e)}\",\n",
    "            \"results\": [],\n",
    "            \"metadata\": {\n",
    "                \"error\": str(e),\n",
    "                \"generation_time\": 0,\n",
    "                \"filters_attempted\": \"None\"\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\"\n",
    "\n",
    "# results = search_and_answer_basic(\n",
    "#     query,\n",
    "#     rag_system,\n",
    "#     None,\n",
    "#     None,\n",
    "#     [\"1\",\"2\"],\n",
    "#     10,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Basic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enhanced test of search_chunks_semantic function\n",
    "# def display_search_results(results, query, filters_used=None):\n",
    "#     \"\"\"\n",
    "#     Enhanced display function for search results\n",
    "#     \"\"\"\n",
    "#     print(f\"🔍 SEARCH RESULTS\")\n",
    "#     print(f\"=\" * 60)\n",
    "#     print(f\"📝 Query: '{query}'\")\n",
    "#     if filters_used:\n",
    "#         print(f\"🔧 Filters applied: {filters_used}\")\n",
    "#     print(f\"📊 Total results: {len(results)}\")\n",
    "#     print(f\"=\" * 60)\n",
    "    \n",
    "#     if not results:\n",
    "#         print(\"❌ No results found\")\n",
    "#         return\n",
    "    \n",
    "#     for j, result in enumerate(results[:5], 1):  # Show top 5 results\n",
    "#         print(f\"\\n🏷️ RESULT #{j} (Score: {result['score']:.4f})\")\n",
    "#         print(f\"   📄 Document: {result['doc_id']}\")\n",
    "#         print(f\"   📋 Chunk Type: {result['chunk_type']}\")\n",
    "#         print(f\"   📄 Page: {result['page_number']}\")\n",
    "        \n",
    "#         # Show metadata if available\n",
    "#         metadata_items = [\n",
    "#             ('📖 Catalog Systems', result.get('catalog_systems', [])),\n",
    "#             ('🔢 Scott Numbers', result.get('scott_numbers', [])),\n",
    "#             ('📅 Years', result.get('years', [])),\n",
    "#             ('🎨 Colors', result.get('colors', [])),\n",
    "#             ('🔀 Variety Classes', result.get('variety_classes', [])),\n",
    "#         ]\n",
    "        \n",
    "#         for label, data in metadata_items:\n",
    "#             if data:\n",
    "#                 display_data = ', '.join(str(item) for item in data) if isinstance(data, list) else str(data)\n",
    "#                 print(f\"   {label}: {display_data}\")\n",
    "        \n",
    "#         # Boolean flags\n",
    "#         if result.get('has_varieties'):\n",
    "#             print(f\"   ✅ Has varieties\")\n",
    "#         if result.get('is_guanacaste'):\n",
    "#             print(f\"   🌎 Guanacaste province\")\n",
    "#         if result.get('has_technical_specs'):\n",
    "#             print(f\"   🔧 Has technical specs\")\n",
    "            \n",
    "#         # Text preview\n",
    "#         text = result.get('text', '')\n",
    "#         preview = text[:300] + \"...\" if len(text) > 300 else text\n",
    "#         print(f\"   📝 Text preview: {preview}\")\n",
    "#         print(f\"   {'─' * 50}\")\n",
    "\n",
    "# # Test 1: Basic search without filters (original test enhanced)\n",
    "# print(\"🧪 TEST 1: Basic Hybrid Search (No Filters)\")\n",
    "# query = \"Costa Rica 1907 2 colones stamp with original gum. Scott 68 issue of 1907\"\n",
    "\n",
    "# results = search_chunks_semantic(\n",
    "#     client=client, \n",
    "#     query=query, \n",
    "#     collection_name=\"Oxcart\", \n",
    "#     limit=20,\n",
    "#     filters=[],  # No filters\n",
    "#     mode=\"hybrid\",\n",
    "#     alpha=0.35\n",
    "# )\n",
    "\n",
    "# display_search_results(results, query)\n",
    "\n",
    "# print(f\"\\n💡 This test shows unfiltered results. Now let's test with specific filters...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test: Combined Filters (Advanced Testing) - UPDATED with Multiple Scott Numbers\n",
    "# print(\"🧪 TEST 6: Combined Filters (Advanced Testing)\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# # Test complex filter combinations for precise searches\n",
    "# combined_tests = [\n",
    "#     {\n",
    "#         \"name\": \"1907 stamps with varieties\",\n",
    "#         \"query\": \"1907 Costa Rica stamps with varieties or errors\",\n",
    "#         \"filters\": {\n",
    "#             \"year_range\": (1907, 1907)\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"1934 Costa Rica stamps\",\n",
    "#         \"query\": \"List all 1934 Costa Rica Stamps\",\n",
    "#         \"filters\": {\n",
    "#             \"year_range\": (1934, 1934)\n",
    "#         }\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"Costa Rica First Issue Scott 1-5 (MULTIPLE SCOTT NUMBERS TEST)\",\n",
    "#         \"query\": \"Costa Rica First Issue Scott 1 2 3 4 5\",\n",
    "#         \"filters\": {\n",
    "#             \"catalog_system\": \"Scott\",\n",
    "#             \"scott_numbers\": [\"1\", \"2\", \"3\", \"4\", \"5\"]  # TEST: Multiple Scott numbers as list\n",
    "#         }\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# for i, test in enumerate(combined_tests, 1):\n",
    "#     print(f\"\\n🔬 COMBINED TEST {i}: {test['name']}\")\n",
    "#     print(f\"{'─' * 60}\")\n",
    "#     print(f\"🎯 Filters: {test['filters']}\")\n",
    "    \n",
    "#     # Special logging for multiple Scott numbers test\n",
    "#     if \"scott_number\" in test['filters'] and isinstance(test['filters']['scott_number'], list):\n",
    "#         print(f\"🔢 TESTING MULTIPLE SCOTT NUMBERS: {test['filters']['scott_number']}\")\n",
    "#         print(f\"📝 Expected: Should find documents with ANY of these Scott numbers (OR logic)\")\n",
    "    \n",
    "#     # Execute search with combined filters\n",
    "#     results = search_chunks_semantic(\n",
    "#         client=client,\n",
    "#         query=test['query'],\n",
    "#         collection_name=\"Oxcart\",\n",
    "#         limit=15,  # Increased limit for multiple Scott test\n",
    "#         filters=test['filters'],\n",
    "#         mode=\"hybrid\",\n",
    "#         alpha=0.35\n",
    "#     )\n",
    "    \n",
    "#     display_search_results(results, test['query'], filters_used=test['filters'])\n",
    "    \n",
    "#     # Detailed validation of filter application\n",
    "#     if results:\n",
    "#         print(f\"\\n   🔍 FILTER VALIDATION:\")\n",
    "#         for filter_key, filter_value in test['filters'].items():\n",
    "#             validation_count = 0\n",
    "            \n",
    "#             # Special handling for multiple Scott numbers\n",
    "#             if filter_key == \"scott_numbers\" and isinstance(filter_value, list):\n",
    "#                 print(f\"      🔢 Checking for ANY Scott number from: {filter_value}\")\n",
    "#                 for result in results:\n",
    "#                     result_scotts = result.get('scott_numbers', [])\n",
    "#                     # Check if ANY of the requested Scott numbers is in the result\n",
    "#                     if any(scott_num in result_scotts for scott_num in filter_value):\n",
    "#                         validation_count += 1\n",
    "#                 print(f\"      ✅ Documents with ANY requested Scott number: {validation_count}/{len(results)}\")\n",
    "                \n",
    "#                 # Show which specific Scott numbers were found\n",
    "#                 found_scotts = set()\n",
    "#                 for result in results:\n",
    "#                     found_scotts.update(result.get('scott_numbers', []))\n",
    "#                 matching_scotts = [s for s in filter_value if s in found_scotts]\n",
    "#                 print(f\"      📋 Requested Scott numbers found: {matching_scotts}\")\n",
    "#                 print(f\"      📋 All Scott numbers in results: {sorted(found_scotts)}\")\n",
    "                \n",
    "#             elif filter_key == \"year_range\":\n",
    "#                 result_years = result.get('years', [])\n",
    "#                 if any(filter_value[0] <= year <= filter_value[1] for year in result_years):\n",
    "#                     validation_count += 1\n",
    "#             elif filter_key == \"catalog_system\":\n",
    "#                 if filter_value in result.get('catalog_systems', []):\n",
    "#                     validation_count += 1\n",
    "#             elif filter_key == \"chunk_type\":\n",
    "#                 if result.get('chunk_type') == filter_value:\n",
    "#                     validation_count += 1\n",
    "#             elif filter_key in [\"has_varieties\", \"is_guanacaste\", \"has_technical_specs\"]:\n",
    "#                 if result.get(filter_key) == filter_value:\n",
    "#                     validation_count += 1\n",
    "            \n",
    "#             # Show validation for non-Scott filters\n",
    "#             if filter_key != \"scott_number\":\n",
    "#                 print(f\"      ✅ {filter_key}: {validation_count}/{len(results)} results match\")\n",
    "    \n",
    "#     print(f\"\\n{'═' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import markdown\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "def create_gradio_interface(rag_system: Dict[str, Any]) -> gr.Blocks:\n",
    "    \"\"\"\n",
    "    Creates the Gradio interface with improved philatelic filters.\n",
    "    Uses HTML for markdown content and Textbox for timing display.\n",
    "    Includes token usage and cost metrics display.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Función para convertir Markdown a HTML con corrección de rutas de imágenes\n",
    "    \n",
    "    def markdown_to_html(text):\n",
    "        \"\"\"Convert markdown text to HTML with lazy base64 loading\"\"\"\n",
    "        if not text:\n",
    "            return \"<p><em>No content</em></p>\"\n",
    "        \n",
    "        import re\n",
    "        import os\n",
    "        import base64\n",
    "        \n",
    "        base_path = r\"C:\\Users\\VM-SERVER\\Desktop\\Oxcart RAG\\results\\markdown\\figures\"\n",
    "        \n",
    "        def image_to_base64_lazy(match):\n",
    "            alt_text = match.group(1)\n",
    "            filename = match.group(2).split('/')[-1].split('\\\\')[-1]\n",
    "            full_path = os.path.join(base_path, filename)\n",
    "            \n",
    "            if os.path.exists(full_path):\n",
    "                try:\n",
    "                    with open(full_path, \"rb\") as img_file:\n",
    "                        b64_string = base64.b64encode(img_file.read()).decode()\n",
    "                        ext = filename.split('.')[-1].lower()\n",
    "                        mime_type = f\"image/{ext}\" if ext != 'jpg' else \"image/jpeg\"\n",
    "                        return f'<img style=\"max-width: 100%; height: auto; display: block; margin: 10px auto; border: 1px solid #ddd; border-radius: 4px;\" alt=\"{alt_text}\" src=\"data:{mime_type};base64,{b64_string}\" />'\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {filename}: {e}\")\n",
    "                    return f'<p>[Image not found: {filename}]</p>'\n",
    "            else:\n",
    "                return f'<p>[Image not found: {filename}]</p>'\n",
    "        \n",
    "        # Primero convertir markdown a HTML\n",
    "        html = markdown.markdown(text, extensions=['tables', 'fenced_code'])\n",
    "        \n",
    "        # Luego reemplazar las imágenes en el HTML\n",
    "        html = re.sub(\n",
    "            r'<img[^>]*alt=\"([^\"]*)\"[^>]*src=\"[^\"]*?([^/\\\\\">]+\\.(?:png|jpg|jpeg|gif))\"[^>]*>',\n",
    "            image_to_base64_lazy,\n",
    "            html\n",
    "        )\n",
    "        \n",
    "        return html\n",
    "\n",
    "    def gradio_sequential_search(query, year_start, year_end, scott_numbers, max_results):\n",
    "        \"\"\"\n",
    "        Sequential search with improved philatelic filters.\n",
    "        Now with OPTIONAL filters - only used if provided by user.\n",
    "        \"\"\"\n",
    "        if not rag_system:\n",
    "            error_msg = \"❌ RAG system not configured\"\n",
    "            error_html = markdown_to_html(error_msg)\n",
    "            yield error_html, \"\", \"\", error_html, \"\", \"\", \"No timing data available\"\n",
    "            return\n",
    "            \n",
    "        if not query or not query.strip():\n",
    "            error_msg = \"❌ Please enter a query\"\n",
    "            error_html = markdown_to_html(error_msg)\n",
    "            yield error_html, \"\", \"\", error_html, \"\", \"\", \"No timing data available\"\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            start_total_time = time.time()\n",
    "            \n",
    "            # Process OPTIONAL filters - convert empty strings to None\n",
    "            # Years: only use if both are provided and valid\n",
    "            processed_year_start = None\n",
    "            processed_year_end = None\n",
    "            \n",
    "            # Check if years are provided as strings and convert\n",
    "            if year_start and year_end:\n",
    "                try:\n",
    "                    # Strip whitespace and check if not empty\n",
    "                    year_start_str = str(year_start).strip()\n",
    "                    year_end_str = str(year_end).strip()\n",
    "                    \n",
    "                    if year_start_str and year_end_str:\n",
    "                        # Try to convert to integers\n",
    "                        year_start_int = int(year_start_str)\n",
    "                        year_end_int = int(year_end_str)\n",
    "                        \n",
    "                        # Validate year range\n",
    "                        if 1800 <= year_start_int <= 2025 and 1800 <= year_end_int <= 2025:\n",
    "                            processed_year_start = year_start_int\n",
    "                            processed_year_end = year_end_int\n",
    "                        else:\n",
    "                            print(f\"[WARNING] Years out of valid range (1800-2025): {year_start_int}-{year_end_int}\")\n",
    "                except (ValueError, TypeError) as e:\n",
    "                    print(f\"[WARNING] Could not parse years: {year_start}, {year_end} - {e}\")\n",
    "                    # Years are invalid, will proceed without year filter\n",
    "            \n",
    "            # Scott numbers: only use if provided and not empty\n",
    "            processed_scott_numbers = None\n",
    "            if scott_numbers and scott_numbers.strip():\n",
    "                # Clean and split the scott numbers\n",
    "                processed_scott_numbers = [s.strip() for s in scott_numbers.split(',') if s.strip()]\n",
    "                # If list is empty after cleaning, set to None\n",
    "                if not processed_scott_numbers:\n",
    "                    processed_scott_numbers = None\n",
    "            \n",
    "            # Log what filters are being used\n",
    "            filters_status = []\n",
    "            if processed_year_start and processed_year_end:\n",
    "                filters_status.append(f\"Years: {processed_year_start}-{processed_year_end}\")\n",
    "            if processed_scott_numbers:\n",
    "                filters_status.append(f\"Scott: {', '.join(processed_scott_numbers)}\")\n",
    "            \n",
    "            filter_msg = \"Filters applied: \" + (\", \".join(filters_status) if filters_status else \"None (searching all documents)\")\n",
    "            print(f\"[DEBUG] {filter_msg}\")\n",
    "            \n",
    "            # ============= STEP 1: Execute Basic Search =============\n",
    "            # Mensajes de estado inicial\n",
    "            loading_basic = markdown_to_html(f\"*🔄 Running Basic Hybrid Search...*\\n\\n{filter_msg}\")\n",
    "            loading_advanced = markdown_to_html(\"*⏳ Waiting for Basic search to complete...*\")\n",
    "            \n",
    "            yield (\n",
    "                loading_basic, \n",
    "                \"\", \n",
    "                \"\", \n",
    "                loading_advanced, \n",
    "                \"\", \n",
    "                \"\", \n",
    "                f\"⏱️ Basic search in progress...\\n{filter_msg}\"\n",
    "            )\n",
    "            \n",
    "            # Call basic search function with optional filters\n",
    "            basic_results_data = search_and_answer_basic(\n",
    "                query=query,\n",
    "                rag_system=rag_system,\n",
    "                year_start=processed_year_start,  # Pass None if not provided\n",
    "                year_end=processed_year_end,      # Pass None if not provided\n",
    "                scott_numbers=processed_scott_numbers,  # Pass None if not provided\n",
    "                max_results=int(max_results),\n",
    "            )\n",
    "            \n",
    "            # Format Basic Results\n",
    "            basic_answer = basic_results_data[\"answer\"]  # Ya viene en Markdown\n",
    "            basic_answer_html = markdown_to_html(basic_answer)\n",
    "            \n",
    "            basic_results = basic_results_data[\"results\"]\n",
    "            basic_metadata = basic_results_data[\"metadata\"]\n",
    "            basic_execution_time = basic_metadata.get(\"generation_time\", 0)\n",
    "            \n",
    "            basic_search_output = format_search_results(basic_results, \"Basic Hybrid Search\")\n",
    "            basic_search_html = markdown_to_html(basic_search_output)\n",
    "            \n",
    "            basic_metadata_output = format_metadata(basic_metadata, basic_execution_time)\n",
    "            basic_metadata_html = markdown_to_html(basic_metadata_output)\n",
    "            \n",
    "            # Timing parcial\n",
    "            timing_partial = f\"\"\"⏱️ EXECUTION TIMING (Partial)\n",
    "            \n",
    "Basic Hybrid Search: ✅ COMPLETED\n",
    "• Time: {basic_execution_time:.2f}s\n",
    "• Results: {len(basic_results)}\n",
    "• {filter_msg}\n",
    "\n",
    "Advanced Search: ⏳ STARTING...\n",
    "\"\"\"\n",
    "            \n",
    "            loading_advanced_2 = markdown_to_html(f\"*🔄 Starting Advanced Compression Search...*\\n\\n{filter_msg}\")\n",
    "            \n",
    "            yield (\n",
    "                basic_answer_html,\n",
    "                basic_search_html,\n",
    "                basic_metadata_html,\n",
    "                loading_advanced_2,\n",
    "                \"\",\n",
    "                \"\",\n",
    "                timing_partial\n",
    "            )\n",
    "            \n",
    "            # ============= STEP 2: Execute Advanced Search =============\n",
    "            advanced_results_data = search_and_answer_advanced(\n",
    "                query=query,\n",
    "                rag_system=rag_system,\n",
    "                max_results=int(max_results),\n",
    "            )\n",
    "            \n",
    "            # Format Advanced Results\n",
    "            advanced_answer = advanced_results_data[\"answer\"]  # Ya viene en Markdown\n",
    "            advanced_answer_html = markdown_to_html(advanced_answer)\n",
    "            \n",
    "            advanced_results = advanced_results_data[\"results\"]\n",
    "            advanced_metadata = advanced_results_data[\"metadata\"]\n",
    "            advanced_execution_time = advanced_metadata.get(\"generation_time\", 0)\n",
    "            \n",
    "            advanced_search_output = format_search_results(advanced_results, \"Advanced Compression Search\")\n",
    "            advanced_search_html = markdown_to_html(advanced_search_output)\n",
    "            \n",
    "            advanced_metadata_output = format_metadata(advanced_metadata, advanced_execution_time)\n",
    "            advanced_metadata_html = markdown_to_html(advanced_metadata_output)\n",
    "            \n",
    "            # Calculate total execution time\n",
    "            total_execution_time = time.time() - start_total_time\n",
    "            \n",
    "            # Final timing information WITH METADATA for costs\n",
    "            timing_final = format_timing_display(\n",
    "                basic_execution_time,\n",
    "                advanced_execution_time,\n",
    "                total_execution_time,\n",
    "                len(basic_results),\n",
    "                len(advanced_results),\n",
    "                filter_msg,\n",
    "                basic_metadata,    # Pass the full metadata\n",
    "                advanced_metadata  # Pass the full metadata\n",
    "            )\n",
    "            \n",
    "            # Yield final complete results\n",
    "            yield (\n",
    "                basic_answer_html,\n",
    "                basic_search_html,\n",
    "                basic_metadata_html,\n",
    "                advanced_answer_html,\n",
    "                advanced_search_html,\n",
    "                advanced_metadata_html,\n",
    "                timing_final\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❌ Error during search: {str(e)}\"\n",
    "            error_html = markdown_to_html(error_msg)\n",
    "            yield error_html, \"\", \"\", error_html, \"\", \"\", f\"❌ Error occurred - no timing data\"\n",
    "\n",
    "    def format_search_results(results, approach_name):\n",
    "        \"\"\"Format search results for display in Markdown with figure handling\"\"\"\n",
    "        if not results:\n",
    "            return f\"*No results found with {approach_name}*\"\n",
    "\n",
    "        lines = []\n",
    "        lines.append(f\"### {approach_name} Results\")\n",
    "        lines.append(f\"**Found {len(results)} documents**\\n\")\n",
    "        lines.append(\"---\")\n",
    "        \n",
    "        for i, r in enumerate(results):\n",
    "            doc_id = r.get(\"doc_id\", \"N/A\")\n",
    "            chunk_type_val = r.get(\"chunk_type\", \"N/A\")\n",
    "            page_number = r.get(\"page_number\", \"N/A\")\n",
    "            score = r.get(\"score\", 0.0)\n",
    "            catalogs = r.get(\"catalog_systems\", [])\n",
    "            scotts = r.get(\"scott_numbers\", [])\n",
    "            years = r.get(\"years\", [])\n",
    "\n",
    "            # Get full text (including figures)\n",
    "            text = r.get(\"text\", \"\")\n",
    "            \n",
    "            # Check if text contains figures\n",
    "            has_figures = \"![Figure]\" in text or \"![\" in text\n",
    "            \n",
    "            # Extract just the text preview (without figures)\n",
    "            import re\n",
    "            text_without_figures = re.sub(r'!\\[([^\\]]*)\\]\\([^)]+\\)', '', text).strip()\n",
    "            preview = (text_without_figures[:300] + \"...\") if len(text_without_figures) > 300 else text_without_figures\n",
    "            \n",
    "            # Extract all figure references\n",
    "            figure_pattern = r'(!\\[([^\\]]*)\\]\\([^)]+\\))'\n",
    "            figures = re.findall(figure_pattern, text)\n",
    "\n",
    "            lines.append(f\"\\n#### 📄 Result {i+1}\")\n",
    "            lines.append(f\"**Score:** `{score:.3f}`\")\n",
    "            \n",
    "            if has_figures:\n",
    "                lines.append(\"🖼️ **This result contains figures**\\n\")\n",
    "            \n",
    "            # Create a table for metadata\n",
    "            lines.append(\"| Field | Value |\")\n",
    "            lines.append(\"|-------|-------|\")\n",
    "            lines.append(f\"| Document | `{doc_id}` |\")\n",
    "            lines.append(f\"| Type | {chunk_type_val} |\")\n",
    "            lines.append(f\"| Page | {page_number} |\")\n",
    "            \n",
    "            if catalogs:\n",
    "                lines.append(f\"| Catalogs | {', '.join(catalogs)} |\")\n",
    "            if scotts:\n",
    "                lines.append(f\"| Scott Numbers | **{', '.join(scotts)}** |\")\n",
    "            if years:\n",
    "                lines.append(f\"| Years | {', '.join(str(y) for y in years)} |\")\n",
    "            \n",
    "            # Always show preview\n",
    "            lines.append(f\"\\n**Preview:**\")\n",
    "            lines.append(f\"> {preview}\")\n",
    "            \n",
    "            # Always show figures if they exist\n",
    "            if has_figures and figures:\n",
    "                lines.append(f\"\\n**Figures in this result:**\\n\")\n",
    "                for figure_match in figures:\n",
    "                    lines.append(figure_match[0])  # Add the complete figure markdown\n",
    "            \n",
    "            lines.append(\"\\n---\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def format_metadata(metadata, execution_time):\n",
    "        \"\"\"Format metadata for display in Markdown including token usage and costs (robust casting).\"\"\"\n",
    "        if not metadata:\n",
    "            return \"*No metadata available*\"\n",
    "\n",
    "        # Helpers\n",
    "        def as_float(x, default=None):\n",
    "            try:\n",
    "                return float(x)\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        def as_int(x, default=None):\n",
    "            try:\n",
    "                return int(float(x))\n",
    "            except (TypeError, ValueError):\n",
    "                return default\n",
    "\n",
    "        out = []\n",
    "        out.append(\"### Search Metadata\\n\")\n",
    "\n",
    "        # Basic info\n",
    "        out.append(\"#### 📊 Search Information\")\n",
    "        out.append(\"| Property | Value |\")\n",
    "        out.append(\"|----------|-------|\")\n",
    "        out.append(f\"| **Approach** | {metadata.get('approach', 'Unknown')} |\")\n",
    "        query_val = str(metadata.get('query', 'N/A')).replace(\"\\n\", \" \")\n",
    "        # (opcional) escapar pipes para no romper la tabla\n",
    "        query_val = query_val.replace(\"|\", \"\\\\|\")\n",
    "        out.append(f\"| **Query** | `{query_val}` |\")\n",
    "\n",
    "        total_results = as_int(metadata.get('total_results'), 0)\n",
    "        out.append(f\"| **Results found** | {total_results} |\")\n",
    "\n",
    "        context_docs_count = as_int(metadata.get('context_docs_count'))\n",
    "        out.append(f\"| **Context docs** | {context_docs_count if context_docs_count is not None else 'N/A'} |\")\n",
    "\n",
    "        context_length = as_int(metadata.get('context_length'))\n",
    "        out.append(\n",
    "            f\"| **Context length** | {context_length:,} chars |\"\n",
    "            if context_length is not None else\n",
    "            \"| **Context length** | N/A |\"\n",
    "        )\n",
    "\n",
    "        if metadata.get('filters_used'):\n",
    "            filters_str = str(metadata['filters_used']).replace('{', '').replace('}', '')\n",
    "            filters_str = filters_str.replace(\"|\", \"\\\\|\")\n",
    "            out.append(f\"| **Filters** | `{filters_str}` |\")\n",
    "\n",
    "        if 'compressed_docs' in metadata:\n",
    "            out.append(f\"| **Compressed docs** | {metadata['compressed_docs']} |\")\n",
    "\n",
    "        # Token usage\n",
    "        token_usage = metadata.get('token_usage') or {}\n",
    "        if token_usage:\n",
    "            in_tok  = as_int(token_usage.get('input_tokens'), 0)\n",
    "            out_tok = as_int(token_usage.get('output_tokens'), 0)\n",
    "            tot_tok = as_int(token_usage.get('total_tokens'), (in_tok or 0) + (out_tok or 0))\n",
    "\n",
    "            out.append(\"\\n#### 🎯 Token Usage\")\n",
    "            out.append(\"| Token Type | Count |\")\n",
    "            out.append(\"|------------|-------|\")\n",
    "            out.append(f\"| **Input tokens** | {in_tok:,} |\")\n",
    "            out.append(f\"| **Output tokens** | {out_tok:,} |\")\n",
    "            out.append(f\"| **Total tokens** | {tot_tok:,} |\")\n",
    "\n",
    "        # Cost info\n",
    "        cost_info = metadata.get('cost_info') or {}\n",
    "        if cost_info:\n",
    "            in_cost  = as_float(cost_info.get('input_cost'), 0.0)\n",
    "            out_cost = as_float(cost_info.get('output_cost'), 0.0)\n",
    "            est_cost = as_float(cost_info.get('estimated_cost_usd'), (in_cost or 0.0) + (out_cost or 0.0))\n",
    "\n",
    "            out.append(\"\\n#### 💰 Cost Analysis\")\n",
    "            out.append(\"| Cost Component | USD |\")\n",
    "            out.append(\"|----------------|-----|\")\n",
    "            out.append(f\"| **Input cost** | ${in_cost:.6f} |\")\n",
    "            out.append(f\"| **Output cost** | ${out_cost:.6f} |\")\n",
    "            out.append(f\"| **Total cost** | **${est_cost:.6f}** |\")\n",
    "\n",
    "        # Performance\n",
    "        out.append(\"\\n#### ⏱️ Performance\")\n",
    "        out.append(\"| Metric | Time |\")\n",
    "        out.append(\"|--------|------|\")\n",
    "\n",
    "        gen_time = as_float(metadata.get('generation_time'))\n",
    "        exec_time = as_float(execution_time)\n",
    "\n",
    "        out.append(\n",
    "            f\"| **Generation time** | {gen_time:.2f} seconds |\"\n",
    "            if gen_time is not None else\n",
    "            \"| **Generation time** | N/A |\"\n",
    "        )\n",
    "        out.append(\n",
    "            f\"| **Total execution** | {exec_time:.2f} seconds |\"\n",
    "            if exec_time is not None else\n",
    "            \"| **Total execution** | N/A |\"\n",
    "        )\n",
    "\n",
    "        if token_usage and gen_time and gen_time > 0:\n",
    "            # usa out_tok ya casteado arriba; si no hay token_usage, se salta este bloque\n",
    "            tps = (as_int(token_usage.get('output_tokens'), 0) or 0) / gen_time\n",
    "            out.append(f\"| **Generation speed** | {tps:.1f} tokens/sec |\")\n",
    "\n",
    "        if metadata.get('error'):\n",
    "            err = str(metadata['error']).replace(\"`\", \"'\")\n",
    "            out.append(f\"\\n⚠️ **Error:** `{err}`\")\n",
    "\n",
    "        return \"\\n\".join(out)\n",
    "\n",
    "\n",
    "    def format_timing_display(\n",
    "        basic_time, advanced_time, total_time,\n",
    "        basic_results, advanced_results,\n",
    "        filter_msg=\"\",\n",
    "        basic_metadata=None, advanced_metadata=None\n",
    "    ):\n",
    "        \"\"\"Enhanced timing display with cost comparison (robust casting)\"\"\"\n",
    "        try:\n",
    "            # --- helpers seguros ---\n",
    "            def as_float(x, default=0.0):\n",
    "                try:\n",
    "                    return float(x)\n",
    "                except (TypeError, ValueError):\n",
    "                    return default\n",
    "\n",
    "            def as_int(x, default=0):\n",
    "                try:\n",
    "                    # evita ints tipo '1_234' si viniera así\n",
    "                    return int(float(x))\n",
    "                except (TypeError, ValueError):\n",
    "                    return default\n",
    "\n",
    "            # tiempos\n",
    "            basic_time = as_float(basic_time)\n",
    "            advanced_time = as_float(advanced_time)\n",
    "            total_time = as_float(total_time)\n",
    "\n",
    "            # costos\n",
    "            basic_cost = 0.0\n",
    "            advanced_cost = 0.0\n",
    "            if basic_metadata and 'cost_info' in basic_metadata:\n",
    "                basic_cost = as_float(basic_metadata['cost_info'].get('estimated_cost_usd', 0))\n",
    "            if advanced_metadata and 'cost_info' in advanced_metadata:\n",
    "                advanced_cost = as_float(advanced_metadata['cost_info'].get('estimated_cost_usd', 0))\n",
    "            total_cost = basic_cost + advanced_cost\n",
    "\n",
    "            # tokens (¡forzar int!)\n",
    "            basic_tokens = 0\n",
    "            advanced_tokens = 0\n",
    "            if basic_metadata and 'token_usage' in basic_metadata:\n",
    "                basic_tokens = as_int(basic_metadata['token_usage'].get('total_tokens', 0))\n",
    "            if advanced_metadata and 'token_usage' in advanced_metadata:\n",
    "                advanced_tokens = as_int(advanced_metadata['token_usage'].get('total_tokens', 0))\n",
    "            total_tokens = basic_tokens + advanced_tokens\n",
    "\n",
    "            # faster\n",
    "            if basic_time > 0 and advanced_time > 0:\n",
    "                if basic_time < advanced_time:\n",
    "                    faster = f\"🏆 Basic search was {advanced_time/basic_time:.1f}x faster\"\n",
    "                elif advanced_time < basic_time:\n",
    "                    faster = f\"🏆 Advanced search was {basic_time/advanced_time:.1f}x faster\"\n",
    "                else:\n",
    "                    faster = \"⚡ Both approaches took similar time\"\n",
    "            else:\n",
    "                faster = \"⏱️ Timing comparison not available\"\n",
    "\n",
    "            # cost effectiveness\n",
    "            cost_comparison = \"\"\n",
    "            if basic_cost > 0 and advanced_cost > 0:\n",
    "                if basic_cost < advanced_cost:\n",
    "                    cost_comparison = f\"💵 Basic search was ${advanced_cost - basic_cost:.6f} cheaper\"\n",
    "                elif advanced_cost < basic_cost:\n",
    "                    cost_comparison = f\"💵 Advanced search was ${basic_cost - advanced_cost:.6f} cheaper\"\n",
    "                else:\n",
    "                    cost_comparison = \"💵 Both approaches had similar costs\"\n",
    "\n",
    "            # speeds\n",
    "            basic_speed = f\"{basic_results/basic_time:.1f}\" if basic_time > 0 else \"N/A\"\n",
    "            advanced_speed = f\"{advanced_results/advanced_time:.1f}\" if advanced_time > 0 else \"N/A\"\n",
    "\n",
    "            timing_display = f\"\"\"⏱️ EXECUTION TIMING & COST COMPARISON\n",
    "                ================================================\n",
    "\n",
    "                📋 SEARCH CONFIGURATION\n",
    "                • {filter_msg}\n",
    "\n",
    "                🔍 BASIC HYBRID SEARCH\n",
    "                • Processing Time: {basic_time:.2f} seconds\n",
    "                • Documents Found: {basic_results}\n",
    "                • Speed: {basic_speed} docs/sec\n",
    "                • Tokens Used: {basic_tokens:,}\n",
    "                • Cost: ${basic_cost:.6f}\n",
    "                • Status: ✅ Complete\n",
    "\n",
    "                🚀 ADVANCED COMPRESSION SEARCH\n",
    "                • Processing Time: {advanced_time:.2f} seconds\n",
    "                • Documents Found: {advanced_results}\n",
    "                • Speed: {advanced_speed} docs/sec\n",
    "                • Tokens Used: {advanced_tokens:,}\n",
    "                • Cost: ${advanced_cost:.6f}\n",
    "                • Status: ✅ Complete\n",
    "\n",
    "                📊 OVERALL PERFORMANCE\n",
    "                • Total Execution: {total_time:.2f} seconds\n",
    "                • Total Tokens: {total_tokens:,}\n",
    "                • Total Cost: ${total_cost:.6f}\n",
    "                • Execution Mode: Sequential (Basic → Advanced)\n",
    "                • {faster}\n",
    "                • {cost_comparison}\n",
    "\n",
    "                💡 PERFORMANCE NOTES:\n",
    "                • Basic search: Fast initial results, lower cost\n",
    "                • Advanced search: Enhanced quality, higher token usage\n",
    "                • Costs shown are for GPT-5-Nano model\n",
    "                • Sequential execution allows progressive viewing\n",
    "                • Filters are optional and only applied when provided\"\"\"\n",
    "            return timing_display\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"❌ Error formatting timing data: {e}\"\n",
    "\n",
    "\n",
    "    # Set example query functions\n",
    "    def set_example_1():\n",
    "        return \"Tell me all about the costa rica 1907 inverted centers?\"\n",
    "    \n",
    "    def set_example_2():\n",
    "        return \"Show me Costa Rica overprinted stamps with varieties or errors\"\n",
    "    \n",
    "    def set_example_3():\n",
    "        return \"1934 airmail definitive issue with catalog values C15-27\"\n",
    "    \n",
    "    def set_example_4():\n",
    "        return \"Tell me all about the first issue crack plate?\"\n",
    "    \n",
    "    def set_example_5():\n",
    "        return \"Costa Rica stamps with perforation errors or printing varieties\"\n",
    "    \n",
    "    def set_example_6():\n",
    "        return \"Research about all the mirror impression stamps of Costa Rica\"\n",
    "\n",
    "    # System information\n",
    "    collection_name = rag_system.get(\"collection_name\", \"Oxcart\")\n",
    "    total_docs = rag_system.get(\"total_documents\", 0)\n",
    "    total_chunks = rag_system.get(\"total_chunks\", 0)\n",
    "\n",
    "    # --- UI with improved philatelic filters ---\n",
    "    with gr.Blocks(\n",
    "        title=\"OXCART RAG - Costa Rica Philatelic System\",\n",
    "        css=\"\"\"\n",
    "        .markdown-text {\n",
    "            font-family: 'Inter', sans-serif;\n",
    "        }\n",
    "        table {\n",
    "            border-collapse: collapse;\n",
    "            width: 100%;\n",
    "        }\n",
    "        th, td {\n",
    "            border: 1px solid #ddd;\n",
    "            padding: 8px;\n",
    "            text-align: left;\n",
    "        }\n",
    "        \"\"\"\n",
    "    ) as interface:\n",
    "        gr.Markdown(\n",
    "            \"# 🇨🇷 OXCART RAG - Costa Rica Philatelic System\\n\\n\"\n",
    "            \"Advanced search for Costa Rican stamps and postal history with sequential dual AI approaches.\"\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Your Costa Rica philatelic query\",\n",
    "                    placeholder=\"e.g., What Costa Rica stamps from 1907 have Scott number 68?\",\n",
    "                    lines=2,\n",
    "                )\n",
    "\n",
    "                search_btn = gr.Button(\"🔍 Search with Both Approaches (Sequential)\", variant=\"primary\")\n",
    "\n",
    "                gr.Markdown(\"**Example Queries:**\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    btn1 = gr.Button(\"📮 1907 inverted centers\", variant=\"secondary\")\n",
    "                    btn2 = gr.Button(\"📮 Overprinted varieties\", variant=\"secondary\")\n",
    "                    btn3 = gr.Button(\"📮 1934 airmail stamps\", variant=\"secondary\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    btn4 = gr.Button(\"📮 First issue crack plate\", variant=\"secondary\")\n",
    "                    btn5 = gr.Button(\"📮 Perforation errors\", variant=\"secondary\")\n",
    "                    btn6 = gr.Button(\"📮 Mirror impression stamps\", variant=\"secondary\")\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"**🔧 Optional Philatelic Filters**\")\n",
    "                gr.Markdown(\"*Leave empty to search all documents*\")\n",
    "\n",
    "                year_start = gr.Textbox(\n",
    "                    label=\"Start Year (optional)\", \n",
    "                    value=\"\", \n",
    "                    placeholder=\"e.g., 1907\",\n",
    "                    info=\"Leave empty for no year filter\"\n",
    "                )\n",
    "                year_end = gr.Textbox(\n",
    "                    label=\"End Year (optional)\", \n",
    "                    value=\"\", \n",
    "                    placeholder=\"e.g., 1910\",\n",
    "                    info=\"Leave empty for no year filter\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"*Note: Both years must be provided to apply year filter*\")\n",
    "                \n",
    "                scott_numbers = gr.Textbox(\n",
    "                    label=\"Scott Numbers (optional)\", \n",
    "                    placeholder=\"e.g., 1,2,3,4,5 or 68,C15\",\n",
    "                    value=\"\",  # Empty string by default\n",
    "                    info=\"Comma-separated catalog numbers (leave empty for no filter)\"\n",
    "                )\n",
    "                max_results = gr.Slider(\n",
    "                    minimum=20,\n",
    "                    maximum=100,\n",
    "                    value=30,\n",
    "                    step=10,\n",
    "                    label=\"Maximum results per approach\",\n",
    "                )\n",
    "\n",
    "        # Timing display as Textbox for better updates\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                timing_display = gr.Textbox(\n",
    "                    label=\"⏱️ Performance Timing & Cost Comparison\",\n",
    "                    lines=22,\n",
    "                    interactive=False,\n",
    "                    value=\"Run a search to see detailed timing and cost comparison between approaches\\n\\nFilters are optional - leave them empty to search all documents\",\n",
    "                    elem_id=\"timing-display\"\n",
    "                )\n",
    "\n",
    "        # Tabbed output with HTML components\n",
    "        with gr.Tabs():\n",
    "            with gr.TabItem(\"🔍 Basic Hybrid Search\"):\n",
    "                gr.Markdown(\"**Combines vector similarity with keyword matching (35% vector + 65% keyword)**\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## AI Response - Basic Approach\")\n",
    "                        basic_answer_output = gr.HTML(\n",
    "                            value=\"<p><em>Waiting for search...</em></p>\",\n",
    "                            elem_id=\"basic_answer\"\n",
    "                        )\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Documents Found - Basic Search\")\n",
    "                        basic_search_output = gr.HTML(\n",
    "                            value=\"<p><em>No results yet</em></p>\",\n",
    "                            elem_id=\"basic_search\"\n",
    "                        )\n",
    "\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Metadata - Basic Search\")\n",
    "                        basic_metadata_output = gr.HTML(\n",
    "                            value=\"<p><em>No metadata yet</em></p>\",\n",
    "                            elem_id=\"basic_metadata\"\n",
    "                        )\n",
    "\n",
    "            with gr.TabItem(\"🚀 Advanced Compression Search\"):\n",
    "                gr.Markdown(\"**Multi-query ensemble retrieval with AI-powered document compression**\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## AI Response - Advanced Approach\")\n",
    "                        advanced_answer_output = gr.HTML(\n",
    "                            value=\"<p><em>Waiting for search...</em></p>\",\n",
    "                            elem_id=\"advanced_answer\"\n",
    "                        )\n",
    "\n",
    "                with gr.Row():\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Documents Found - Advanced Search\")\n",
    "                        advanced_search_output = gr.HTML(\n",
    "                            value=\"<p><em>No results yet</em></p>\",\n",
    "                            elem_id=\"advanced_search\"\n",
    "                        )\n",
    "\n",
    "                    with gr.Column():\n",
    "                        gr.Markdown(\"## Metadata - Advanced Search\")\n",
    "                        advanced_metadata_output = gr.HTML(\n",
    "                            value=\"<p><em>No metadata yet</em></p>\",\n",
    "                            elem_id=\"advanced_metadata\"\n",
    "                        )\n",
    "\n",
    "        # Wire up events with new parameters\n",
    "        search_btn.click(\n",
    "            fn=gradio_sequential_search,\n",
    "            inputs=[query_input, year_start, year_end, scott_numbers, max_results],\n",
    "            outputs=[\n",
    "                basic_answer_output, basic_search_output, basic_metadata_output,\n",
    "                advanced_answer_output, advanced_search_output, advanced_metadata_output,\n",
    "                timing_display\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        query_input.submit(\n",
    "            fn=gradio_sequential_search,\n",
    "            inputs=[query_input, year_start, year_end, scott_numbers, max_results],\n",
    "            outputs=[\n",
    "                basic_answer_output, basic_search_output, basic_metadata_output,\n",
    "                advanced_answer_output, advanced_search_output, advanced_metadata_output,\n",
    "                timing_display\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Example buttons\n",
    "        btn1.click(fn=set_example_1, outputs=[query_input])\n",
    "        btn2.click(fn=set_example_2, outputs=[query_input])\n",
    "        btn3.click(fn=set_example_3, outputs=[query_input])\n",
    "        btn4.click(fn=set_example_4, outputs=[query_input])\n",
    "        btn5.click(fn=set_example_5, outputs=[query_input])\n",
    "        btn6.click(fn=set_example_6, outputs=[query_input])\n",
    "\n",
    "        # System information with cost details\n",
    "        gr.Markdown(\n",
    "            \"---\\n\"\n",
    "            f\"**System Status:**\\n\"\n",
    "            f\"• Collection: {collection_name}\\n\"\n",
    "            f\"• Documents indexed: {total_docs:,}\\n\"\n",
    "            f\"• Total chunks: {total_chunks:,}\\n\"\n",
    "            f\"• Status: ✅ Operational\\n\\n\"\n",
    "            f\"**Filter Options:**\\n\"\n",
    "            f\"• **All filters are OPTIONAL** - leave empty to search entire collection\\n\"\n",
    "            f\"• **Year filter**: Requires BOTH start and end year to activate\\n\"\n",
    "            f\"• **Scott numbers**: Can specify one or multiple catalog numbers\\n\"\n",
    "            f\"• **No filters**: Searches across all documents (recommended for general queries)\\n\\n\"\n",
    "            f\"**Cost Tracking:**\\n\"\n",
    "            f\"• Model: GPT-5-Nano\\n\"\n",
    "            f\"• Input: $0.05 per 1M tokens\\n\"\n",
    "            f\"• Output: $0.40 per 1M tokens\\n\"\n",
    "            f\"• Cost breakdown shown in metadata\\n\\n\"\n",
    "            f\"**Execution Mode:**\\n\"\n",
    "            f\"• Sequential execution: Basic search completes first, then Advanced\\n\"\n",
    "            f\"• Results display progressively as each search completes\\n\"\n",
    "            f\"• Token usage and costs tracked for each approach\\n\\n\"\n",
    "            f\"**Search Approaches:**\\n\"\n",
    "            f\"• **Basic Search**: Hybrid semantic search optimized for exact catalog numbers\\n\"\n",
    "            f\"• **Advanced Search**: Multi-query ensemble with AI compression for complex queries\"\n",
    "        )\n",
    "\n",
    "    return interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Enhanced launcher ----\n",
    "if rag_system and rag_system.get(\"success\", False):\n",
    "    print(\"\\\\n\" + \"=\" * 60)\n",
    "    print(\"🚀 LAUNCHING COSTA RICA PHILATELIC RAG INTERFACE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    gradio_app = create_gradio_interface(rag_system)\n",
    "\n",
    "    GRADIO_PORT = int(os.getenv(\"GRADIO_PORT\", 7860))\n",
    "    GRADIO_SHARE = os.getenv(\"GRADIO_SHARE\", \"false\").lower() == \"true\"\n",
    "\n",
    "    print(f\"⚙️ Port: {GRADIO_PORT}\")\n",
    "    print(f\"🌍 Public URL: {'⚠️ Attempting...' if GRADIO_SHARE else '❌ Disabled (more secure)'}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"🔄 Starting Gradio server...\")\n",
    "        \n",
    "        if GRADIO_SHARE:\n",
    "            print(\"⏳ Attempting to create public tunnel...\")\n",
    "            try:\n",
    "                demo = gradio_app.launch(\n",
    "                    server_port=GRADIO_PORT,\n",
    "                    share=True,\n",
    "                    inbrowser=False,\n",
    "                    show_error=True,\n",
    "                    prevent_thread_lock=False,\n",
    "                    quiet=False,\n",
    "                )\n",
    "                \n",
    "                print(\"\\\\n🎉 SUCCESS! Public tunnel created\")\n",
    "                print(f\"🌐 AVAILABLE URLS:\")\n",
    "                print(f\"   📱 Local: http://localhost:{GRADIO_PORT}\")\n",
    "                \n",
    "                if hasattr(demo, 'share_url') and demo.share_url:\n",
    "                    print(f\"   🌍 Public: {demo.share_url}\")\n",
    "                    print(f\"\\\\n🔗 **PUBLIC URL:** {demo.share_url}\")\n",
    "                else:\n",
    "                    print(f\"   🌍 Public: Check Gradio output above ☝️\")\n",
    "                \n",
    "            except Exception as share_error:\n",
    "                print(f\"⚠️ Error creating public tunnel: {share_error}\")\n",
    "                print(\"🔄 Switching to local mode only...\")\n",
    "                \n",
    "                demo = gradio_app.launch(\n",
    "                    server_port=GRADIO_PORT,\n",
    "                    share=False,\n",
    "                    inbrowser=True,\n",
    "                    show_error=True,\n",
    "                    prevent_thread_lock=False\n",
    "                )\n",
    "                \n",
    "                print(f\"\\\\n✅ LOCAL SERVER OPERATIONAL:\")\n",
    "                print(f\"   📱 Local URL: http://localhost:{GRADIO_PORT}\")\n",
    "                print(f\"   ⚠️ Public URL: Not available (tunnel error)\")\n",
    "                \n",
    "        else:\n",
    "            demo = gradio_app.launch(\n",
    "                server_port=GRADIO_PORT,\n",
    "                share=False,\n",
    "                inbrowser=True,\n",
    "                show_error=True,\n",
    "                prevent_thread_lock=False\n",
    "            )\n",
    "            \n",
    "            print(f\"\\\\n✅ LOCAL SERVER OPERATIONAL:\")\n",
    "            print(f\"   📱 Local URL: http://localhost:{GRADIO_PORT}\")\n",
    "            print(f\"   💡 For public URL, set GRADIO_SHARE=true in .env\")\n",
    "        \n",
    "        print(f\"\\\\n📋 COSTA RICA PHILATELIC FEATURES:\")\n",
    "        print(f\"   • Specialized Costa Rica stamp queries\")\n",
    "        print(f\"   • Scott catalog number search\")\n",
    "        print(f\"   • Variety and error detection\")\n",
    "        print(f\"   • Dual search approaches for comprehensive results\")\n",
    "        print(f\"   • Performance timing comparison\")\n",
    "        print(f\"   • To stop: gr.close_all()\")\n",
    "        \n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"🇨🇷 COSTA RICA PHILATELIC RAG INTERFACE READY!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Critical error launching Gradio: {e}\")\n",
    "        print(\"\\\\n🔧 SUGGESTED SOLUTIONS:\")\n",
    "        print(\"   1. Run: gr.close_all()\")\n",
    "        print(\"   2. Change port: GRADIO_PORT=7861 in .env\")\n",
    "        print(\"   3. Verify no other services on the port\")\n",
    "        print(\"   4. Restart the notebook\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\\\n⚠️  Cannot create Gradio interface:\")\n",
    "    if not rag_system:\n",
    "        print(\"   • RAG system not configured\")\n",
    "    else:\n",
    "        print(f\"   • RAG error: {rag_system.get('error', 'Unknown error')}\")\n",
    "    print(\"\\\\n🔧 To resolve:\")\n",
    "    print(\"   1. Verify Weaviate is running\")\n",
    "    print(\"   2. Configure OPENAI_API_KEY in .env\") \n",
    "    print(\"   3. Run document indexing\")\n",
    "    print(\"   4. Restart this notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-clean (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
